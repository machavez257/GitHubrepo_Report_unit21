,url,repository_url,labels_url,comments_url,events_url,html_url,id,node_id,number,title,labels,state,locked,assignee,assignees,milestone,comments,created_at,updated_at,closed_at,author_association,active_lock_reason,draft,body,timeline_url,performed_via_github_app,repo_name,owner,user.login,user.id,user.node_id,user.avatar_url,user.gravatar_id,user.url,user.html_url,user.followers_url,user.following_url,user.gists_url,user.starred_url,user.subscriptions_url,user.organizations_url,user.repos_url,user.events_url,user.received_events_url,user.type,user.site_admin,pull_request.url,pull_request.html_url,pull_request.diff_url,pull_request.patch_url,pull_request.merged_at,reactions.url,reactions.total_count,reactions.+1,reactions.-1,reactions.laugh,reactions.hooray,reactions.confused,reactions.heart,reactions.rocket,reactions.eyes,assignee.login,assignee.id,assignee.node_id,assignee.avatar_url,assignee.gravatar_id,assignee.url,assignee.html_url,assignee.followers_url,assignee.following_url,assignee.gists_url,assignee.starred_url,assignee.subscriptions_url,assignee.organizations_url,assignee.repos_url,assignee.events_url,assignee.received_events_url,assignee.type,assignee.site_admin
0,https://api.github.com/repos/apache/spark/issues/35380,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35380/labels{/name},https://api.github.com/repos/apache/spark/issues/35380/comments,https://api.github.com/repos/apache/spark/issues/35380/events,https://github.com/apache/spark/pull/35380,1120824813,PR_kwDOAQXtWs4x6PcH,35380,[WIP][SPARK-37417][PYTHON][ML] Inline type hints for pyspark.ml.linalg.__init__.py,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-02-01T15:18:18Z,2022-02-01T18:35:07Z,,MEMBER,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Migration of type type annotations for `pyspark.ml.linalg.__init__.py` from stub file to inline hints.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

As part of ongoing type hint migrations.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/35380/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35380,https://github.com/apache/spark/pull/35380,https://github.com/apache/spark/pull/35380.diff,https://github.com/apache/spark/pull/35380.patch,,https://api.github.com/repos/apache/spark/issues/35380/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
1,https://api.github.com/repos/apache/spark/issues/35379,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35379/labels{/name},https://api.github.com/repos/apache/spark/issues/35379/comments,https://api.github.com/repos/apache/spark/issues/35379/events,https://github.com/apache/spark/pull/35379,1120322524,PR_kwDOAQXtWs4x4kXv,35379,fix bugs in AvroSerializer,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1988040187, 'node_id': 'MDU6TGFiZWwxOTg4MDQwMTg3', 'url': 'https://api.github.com/repos/apache/spark/labels/AVRO', 'name': 'AVRO', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-02-01T07:30:21Z,2022-02-01T11:38:22Z,,NONE,,False,"**What's the problem to fix?**

`AvroSerializer`'s implementation, at least in `newConverter`, was not 100% based on the `InternalRow` and `SpecializedGetters` interface. It assumes many implementation details of the interface. 

For example, in 

```scala
      case (TimestampType, LONG) => avroType.getLogicalType match {
          // For backward compatibility, if the Avro type is Long and it is not logical type
          // (the `null` case), output the timestamp value as with millisecond precision.
          case null | _: TimestampMillis => (getter, ordinal) =>
            DateTimeUtils.microsToMillis(timestampRebaseFunc(getter.getLong(ordinal)))
          case _: TimestampMicros => (getter, ordinal) =>
            timestampRebaseFunc(getter.getLong(ordinal))
          case other => throw new IncompatibleSchemaException(errorPrefix +
            s""SQL type ${TimestampType.sql} cannot be converted to Avro logical type $other"")
        }
```

it assumes the `InternalRow` instance encodes `TimestampType` as `java.lang.Long`. That's true for `Unsaferow` but not for `GenericInternalRow`. 

Hence the above code will end up with runtime exceptions when used on an instance of `GenericInternalRow`, which is most of the case for Python.

This PR may not be complete as I don't have much free time to work on it.
But it should be a good improvement for now.",https://api.github.com/repos/apache/spark/issues/35379/timeline,,spark,apache,Zhen-hao,10957195,MDQ6VXNlcjEwOTU3MTk1,https://avatars.githubusercontent.com/u/10957195?v=4,,https://api.github.com/users/Zhen-hao,https://github.com/Zhen-hao,https://api.github.com/users/Zhen-hao/followers,https://api.github.com/users/Zhen-hao/following{/other_user},https://api.github.com/users/Zhen-hao/gists{/gist_id},https://api.github.com/users/Zhen-hao/starred{/owner}{/repo},https://api.github.com/users/Zhen-hao/subscriptions,https://api.github.com/users/Zhen-hao/orgs,https://api.github.com/users/Zhen-hao/repos,https://api.github.com/users/Zhen-hao/events{/privacy},https://api.github.com/users/Zhen-hao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35379,https://github.com/apache/spark/pull/35379,https://github.com/apache/spark/pull/35379.diff,https://github.com/apache/spark/pull/35379.patch,,https://api.github.com/repos/apache/spark/issues/35379/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
2,https://api.github.com/repos/apache/spark/issues/35378,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35378/labels{/name},https://api.github.com/repos/apache/spark/issues/35378/comments,https://api.github.com/repos/apache/spark/issues/35378/events,https://github.com/apache/spark/pull/35378,1120321179,PR_kwDOAQXtWs4x4kF4,35378,[SPARK-38077][SQL] Fix binary compatibility issue with isDeterministic flag,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-02-01T07:28:14Z,2022-02-01T18:35:08Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This fixes a binary compatibility issue caused by SPARK-37957 with the introduction of the additional `isDeterministic` which defaults to true.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Adding method parameters with default value will break binary compatibility (see [here](https://github.com/jatcwang/binary-compatibility-guide#dont-adding-parameters-with-default-values-to-methods)). Even though Spark doesn't strictly guarantee it, it is still better to avoid. In this case, the compatibility of [frameless](https://github.com/typelevel/frameless) is broken when it wants to work with multiple Spark versions (e.g., 3.2.0 and 3.2.1).

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Now it requires users to call `setDeterministic` after initializing `Invoke` and `StaticInvoke` if they want to mark the methods as non-deterministic.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/35378/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35378,https://github.com/apache/spark/pull/35378,https://github.com/apache/spark/pull/35378.diff,https://github.com/apache/spark/pull/35378.patch,,https://api.github.com/repos/apache/spark/issues/35378/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
3,https://api.github.com/repos/apache/spark/issues/35377,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35377/labels{/name},https://api.github.com/repos/apache/spark/issues/35377/comments,https://api.github.com/repos/apache/spark/issues/35377/events,https://github.com/apache/spark/pull/35377,1120254557,PR_kwDOAQXtWs4x4WJN,35377,[SPARK-37958][DOCS] Update spark.files.overwrite description,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-02-01T05:33:06Z,2022-02-01T11:37:48Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Update the description about `spark.files.overwrite`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The description is misleading so users might misunderstand that they can use `SparkContext.addFile` to update the files that the added by the same API before.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
` SKIP_API=1 bundle exec jekyll build`

The screenshot of document is below.
<img width=""898"" alt=""Screen Shot 2022-02-01 at 11 14 59"" src=""https://user-images.githubusercontent.com/14937752/151917587-3e0f1ca8-c87e-4245-a12b-4b3ab77aa196.png"">

",https://api.github.com/repos/apache/spark/issues/35377/timeline,,spark,apache,yoda-mon,14937752,MDQ6VXNlcjE0OTM3NzUy,https://avatars.githubusercontent.com/u/14937752?v=4,,https://api.github.com/users/yoda-mon,https://github.com/yoda-mon,https://api.github.com/users/yoda-mon/followers,https://api.github.com/users/yoda-mon/following{/other_user},https://api.github.com/users/yoda-mon/gists{/gist_id},https://api.github.com/users/yoda-mon/starred{/owner}{/repo},https://api.github.com/users/yoda-mon/subscriptions,https://api.github.com/users/yoda-mon/orgs,https://api.github.com/users/yoda-mon/repos,https://api.github.com/users/yoda-mon/events{/privacy},https://api.github.com/users/yoda-mon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35377,https://github.com/apache/spark/pull/35377,https://github.com/apache/spark/pull/35377.diff,https://github.com/apache/spark/pull/35377.patch,,https://api.github.com/repos/apache/spark/issues/35377/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
4,https://api.github.com/repos/apache/spark/issues/35374,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35374/labels{/name},https://api.github.com/repos/apache/spark/issues/35374/comments,https://api.github.com/repos/apache/spark/issues/35374/events,https://github.com/apache/spark/pull/35374,1120010737,PR_kwDOAQXtWs4x3iUb,35374,[SPARK-34183][SS] DataSource V2: Required distribution and ordering in micro-batch execution,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-31T22:40:45Z,2022-01-31T22:47:46Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adjusts existing logical plans for micro-batch writes to support required distribution and ordering. This change implements what was discussed in PR #31700. In particular, the consensus was to adapt existing streaming plans to support write requirements instead of introducing new logical plans for Structured Streaming. That's a separate item and must be addressed independently.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

These changes are needed so that data sources can request a specific distribution and ordering not only for batch but also for micro-batch writes.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

This PR extends existing tests to cover micro-batch cases.",https://api.github.com/repos/apache/spark/issues/35374/timeline,,spark,apache,aokolnychyi,6235869,MDQ6VXNlcjYyMzU4Njk=,https://avatars.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35374,https://github.com/apache/spark/pull/35374,https://github.com/apache/spark/pull/35374.diff,https://github.com/apache/spark/pull/35374.patch,,https://api.github.com/repos/apache/spark/issues/35374/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
5,https://api.github.com/repos/apache/spark/issues/35372,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35372/labels{/name},https://api.github.com/repos/apache/spark/issues/35372/comments,https://api.github.com/repos/apache/spark/issues/35372/events,https://github.com/apache/spark/pull/35372,1119849088,PR_kwDOAQXtWs4x3AQf,35372,[SPARK-38080][TESTS][SS]Flaky test: StreamingQueryManagerSuite: 'awaitAnyTermination with timeout and resetTerminated',"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-31T19:31:53Z,2022-01-31T19:34:38Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Fix a flaky test.

### Why are the changes needed?

`StreamingQueryManagerSuite: 'awaitAnyTermination with timeout and resetTerminated'` is a flaky test.

### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

- The flaky test can be reproduced by adding a `Thread.sleep(100)` in https://github.com/apache/spark/blob/v3.2.1/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L346
- Using the above reproduction to verify the PR. ",https://api.github.com/repos/apache/spark/issues/35372/timeline,,spark,apache,zsxwing,1000778,MDQ6VXNlcjEwMDA3Nzg=,https://avatars.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35372,https://github.com/apache/spark/pull/35372,https://github.com/apache/spark/pull/35372.diff,https://github.com/apache/spark/pull/35372.patch,,https://api.github.com/repos/apache/spark/issues/35372/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
6,https://api.github.com/repos/apache/spark/issues/35371,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35371/labels{/name},https://api.github.com/repos/apache/spark/issues/35371/comments,https://api.github.com/repos/apache/spark/issues/35371/events,https://github.com/apache/spark/pull/35371,1119219669,PR_kwDOAQXtWs4x06-T,35371,[WIP][SPARK-37946][SQL] Use error classes in the execution errors related to partitions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-31T10:06:18Z,2022-01-31T11:05:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Use error classes defined in `error-classes.json` in the execution errors related to partitions.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To use the new error framework.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
TBD",https://api.github.com/repos/apache/spark/issues/35371/timeline,,spark,apache,yutoacts,87687356,MDQ6VXNlcjg3Njg3MzU2,https://avatars.githubusercontent.com/u/87687356?v=4,,https://api.github.com/users/yutoacts,https://github.com/yutoacts,https://api.github.com/users/yutoacts/followers,https://api.github.com/users/yutoacts/following{/other_user},https://api.github.com/users/yutoacts/gists{/gist_id},https://api.github.com/users/yutoacts/starred{/owner}{/repo},https://api.github.com/users/yutoacts/subscriptions,https://api.github.com/users/yutoacts/orgs,https://api.github.com/users/yutoacts/repos,https://api.github.com/users/yutoacts/events{/privacy},https://api.github.com/users/yutoacts/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35371,https://github.com/apache/spark/pull/35371,https://github.com/apache/spark/pull/35371.diff,https://github.com/apache/spark/pull/35371.patch,,https://api.github.com/repos/apache/spark/issues/35371/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
7,https://api.github.com/repos/apache/spark/issues/35370,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35370/labels{/name},https://api.github.com/repos/apache/spark/issues/35370/comments,https://api.github.com/repos/apache/spark/issues/35370/events,https://github.com/apache/spark/pull/35370,1118983843,PR_kwDOAQXtWs4x0IvE,35370,[SPARK-38042][SQL] Ensure that ScalaReflection.dataTypeFor works on aliased array types,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-31T05:13:46Z,2022-02-01T02:21:24Z,,NONE,,False,"An aliased array type in a product, in a Dataset or Dataframe, causes an exception:

```
type Data = Array[Long]
val xs:List[(Data,Int)] = List((Array(1),1), (Array(2),2))
sc.parallelize(xs).toDF(""a"", ""b"")
```

Causing

```
scala.MatchError: Data (of class scala.reflect.internal.Types$AliasNoArgsTypeRef) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$dataTypeFor$1(ScalaReflection.scala:104) 
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.dataTypeFor(ScalaReflection.scala:88) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$6(ScalaReflection.scala:573) 
 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) 
 at scala.collection.immutable.List.foreach(List.scala:392) 
 at scala.collection.TraversableLike.map(TraversableLike.scala:238) 
 at scala.collection.TraversableLike.map$(TraversableLike.scala:231) 
 at scala.collection.immutable.List.map(List.scala:298) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:562) 
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:432) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerForType$1(ScalaReflection.scala:421) 
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.serializerForType(ScalaReflection.scala:413) 
 at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:55) 
 at org.apache.spark.sql.Encoders$.product(Encoders.scala:285) 
 at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder(SQLImplicits.scala:251) 
 at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder$(SQLImplicits.scala:251) 
 at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:32) 
 ... 48 elided
```

It seems that this can be fixed by changing, in ScalaReflection.dataTypeFor:

```
val TypeRef(_, _, Seq(elementType)) = tpe
```

to

```
val TypeRef(_, _, Seq(elementType)) = tpe.dealias
```

### Why are the changes needed?

Without this change, any attempt to create datasets or dataframes using such types throws the exception above.


### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Added a test to DatasetSuite
",https://api.github.com/repos/apache/spark/issues/35370/timeline,,spark,apache,jtnystrom,710364,MDQ6VXNlcjcxMDM2NA==,https://avatars.githubusercontent.com/u/710364?v=4,,https://api.github.com/users/jtnystrom,https://github.com/jtnystrom,https://api.github.com/users/jtnystrom/followers,https://api.github.com/users/jtnystrom/following{/other_user},https://api.github.com/users/jtnystrom/gists{/gist_id},https://api.github.com/users/jtnystrom/starred{/owner}{/repo},https://api.github.com/users/jtnystrom/subscriptions,https://api.github.com/users/jtnystrom/orgs,https://api.github.com/users/jtnystrom/repos,https://api.github.com/users/jtnystrom/events{/privacy},https://api.github.com/users/jtnystrom/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35370,https://github.com/apache/spark/pull/35370,https://github.com/apache/spark/pull/35370.diff,https://github.com/apache/spark/pull/35370.patch,,https://api.github.com/repos/apache/spark/issues/35370/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
8,https://api.github.com/repos/apache/spark/issues/35369,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35369/labels{/name},https://api.github.com/repos/apache/spark/issues/35369/comments,https://api.github.com/repos/apache/spark/issues/35369/events,https://github.com/apache/spark/pull/35369,1118983417,PR_kwDOAQXtWs4x0Iou,35369,[SPARK-38076][CORE] Remove redundant null-check is covered by further condition,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-31T05:13:13Z,2022-01-31T05:15:10Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
There are many code pattern in Spark Java code as follows:
```java
obj != null && obj instanceof SomeClass 
```
the null-check is redundant as `instanceof` operator implies non-nullity, so this pr remove the redundant `null-check`. 


### Why are the changes needed?
Code simplification




### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Pass GA",https://api.github.com/repos/apache/spark/issues/35369/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35369,https://github.com/apache/spark/pull/35369,https://github.com/apache/spark/pull/35369.diff,https://github.com/apache/spark/pull/35369.patch,,https://api.github.com/repos/apache/spark/issues/35369/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
9,https://api.github.com/repos/apache/spark/issues/35367,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35367/labels{/name},https://api.github.com/repos/apache/spark/issues/35367/comments,https://api.github.com/repos/apache/spark/issues/35367/events,https://github.com/apache/spark/pull/35367,1118732203,PR_kwDOAQXtWs4xzSYI,35367,[WIP][SPARK-37415][PYTHON][ML] Inline type hints for pyspark.ml.util,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-30T21:56:37Z,2022-01-30T21:58:20Z,,MEMBER,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR migrates type `pyspark.ml.util` annotations from stub file to inline type hints.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Part of ongoing migration of type hints.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests.",https://api.github.com/repos/apache/spark/issues/35367/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35367,https://github.com/apache/spark/pull/35367,https://github.com/apache/spark/pull/35367.diff,https://github.com/apache/spark/pull/35367.patch,,https://api.github.com/repos/apache/spark/issues/35367/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
10,https://api.github.com/repos/apache/spark/issues/35366,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35366/labels{/name},https://api.github.com/repos/apache/spark/issues/35366/comments,https://api.github.com/repos/apache/spark/issues/35366/events,https://github.com/apache/spark/pull/35366,1118639867,PR_kwDOAQXtWs4xy_XU,35366,[SPARK-37941][SQL] Use error classes in the compilation errors of casting,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-30T18:14:22Z,2022-01-31T09:28:16Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Migrate the following errors in QueryCompilationErrors onto use error classes:
upCastFailureError => CANNOT_UP_CAST_DATATYPE
unsupportedAbstractDataTypeForUpCastError => UNSUPPORTED_FEATURE
cannotUpCastAsAttributeError => removed as no longer used.

### Why are the changes needed?
Porting casting errors to new error framework.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
New UT added.",https://api.github.com/repos/apache/spark/issues/35366/timeline,,spark,apache,ivoson,15122230,MDQ6VXNlcjE1MTIyMjMw,https://avatars.githubusercontent.com/u/15122230?v=4,,https://api.github.com/users/ivoson,https://github.com/ivoson,https://api.github.com/users/ivoson/followers,https://api.github.com/users/ivoson/following{/other_user},https://api.github.com/users/ivoson/gists{/gist_id},https://api.github.com/users/ivoson/starred{/owner}{/repo},https://api.github.com/users/ivoson/subscriptions,https://api.github.com/users/ivoson/orgs,https://api.github.com/users/ivoson/repos,https://api.github.com/users/ivoson/events{/privacy},https://api.github.com/users/ivoson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35366,https://github.com/apache/spark/pull/35366,https://github.com/apache/spark/pull/35366.diff,https://github.com/apache/spark/pull/35366.patch,,https://api.github.com/repos/apache/spark/issues/35366/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
11,https://api.github.com/repos/apache/spark/issues/35363,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35363/labels{/name},https://api.github.com/repos/apache/spark/issues/35363/comments,https://api.github.com/repos/apache/spark/issues/35363/events,https://github.com/apache/spark/pull/35363,1118389175,PR_kwDOAQXtWs4xyQZl,35363,[SPARK-23445][FOLLOWUP] evaluateEquality should ignore attribute without min/max ColumnStat,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-30T02:48:25Z,2022-01-30T15:00:05Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
`FilterEstimation.scala#estimate`---> `calculateFilterSelectivity()`---> `calculateSingleCondition()`---> `evaluateEquality()`
At the beginning of the `evaluateEquality()` we need to determine if the colStatsMap of the attribute contains the min/max properties.

### Why are the changes needed?
When cbo is turned on, if the min/max property of the attribute does not exist, we will get 0 instead of 1 when calculating the filterSelectivity, which is not as expected。


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
No
",https://api.github.com/repos/apache/spark/issues/35363/timeline,,spark,apache,Stove-hust,52523908,MDQ6VXNlcjUyNTIzOTA4,https://avatars.githubusercontent.com/u/52523908?v=4,,https://api.github.com/users/Stove-hust,https://github.com/Stove-hust,https://api.github.com/users/Stove-hust/followers,https://api.github.com/users/Stove-hust/following{/other_user},https://api.github.com/users/Stove-hust/gists{/gist_id},https://api.github.com/users/Stove-hust/starred{/owner}{/repo},https://api.github.com/users/Stove-hust/subscriptions,https://api.github.com/users/Stove-hust/orgs,https://api.github.com/users/Stove-hust/repos,https://api.github.com/users/Stove-hust/events{/privacy},https://api.github.com/users/Stove-hust/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35363,https://github.com/apache/spark/pull/35363,https://github.com/apache/spark/pull/35363.diff,https://github.com/apache/spark/pull/35363.patch,,https://api.github.com/repos/apache/spark/issues/35363/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
12,https://api.github.com/repos/apache/spark/issues/35362,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35362/labels{/name},https://api.github.com/repos/apache/spark/issues/35362/comments,https://api.github.com/repos/apache/spark/issues/35362/events,https://github.com/apache/spark/pull/35362,1118353939,PR_kwDOAQXtWs4xyJy2,35362,[SPARK-38069][SQL] Improve structured streaming window of calculated,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-29T23:37:35Z,2022-01-31T07:10:55Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Remove the CaseWhen，Modified the calculation method of the obtained window

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Structed Streaming computes window by intermediate result windowId, and windowId computes window by CaseWhen.

We can use Flink's method of calculating window to write it, which is more easy to understand, simple and efficient

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
NO

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing test as this is just refactoring.",https://api.github.com/repos/apache/spark/issues/35362/timeline,,spark,apache,nyingping,10743150,MDQ6VXNlcjEwNzQzMTUw,https://avatars.githubusercontent.com/u/10743150?v=4,,https://api.github.com/users/nyingping,https://github.com/nyingping,https://api.github.com/users/nyingping/followers,https://api.github.com/users/nyingping/following{/other_user},https://api.github.com/users/nyingping/gists{/gist_id},https://api.github.com/users/nyingping/starred{/owner}{/repo},https://api.github.com/users/nyingping/subscriptions,https://api.github.com/users/nyingping/orgs,https://api.github.com/users/nyingping/repos,https://api.github.com/users/nyingping/events{/privacy},https://api.github.com/users/nyingping/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35362,https://github.com/apache/spark/pull/35362,https://github.com/apache/spark/pull/35362.diff,https://github.com/apache/spark/pull/35362.patch,,https://api.github.com/repos/apache/spark/issues/35362/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
13,https://api.github.com/repos/apache/spark/issues/35361,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35361/labels{/name},https://api.github.com/repos/apache/spark/issues/35361/comments,https://api.github.com/repos/apache/spark/issues/35361/events,https://github.com/apache/spark/pull/35361,1118033915,PR_kwDOAQXtWs4xxKtT,35361,[SPARK-37932][SQL] Fix view bug when using join in duplicate views,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-29T05:14:12Z,2022-01-29T12:19:20Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When the join with duplicate view like
```SELECT l1.idFROM v1 l1
 INNER JOIN (
   SELECT id FROM v1
   GROUP BY id  HAVING COUNT(DISTINCT name) > 1
 ) l2
 ON l1.id = l2.id
 GROUP BY l1.name, l1.id;
```
The error stack is:
```
Resolved attribute(s) name#26 missing from id#31,name#32 in operator !Aggregate [id#31], [id#31, count(distinct name#26) AS count(distinct name#26)#33L]. Attribute(s) with the same name appear in the operation: name. Please check if the right attribute(s) are used.;
Aggregate [name#26, id#25], [id#25]
+- Join Inner, (id#25 = id#31)
   :- SubqueryAlias l1
   :  +- SubqueryAlias spark_catalog.default.v1
   :     +- View (`default`.`v1`, [id#25,name#26])
   :        +- Project [cast(id#20 as int) AS id#25, cast(name#21 as string) AS name#26]
   :           +- Project [id#20, name#21]
   :              +- SubqueryAlias spark_catalog.default.t
   :                 +- Relation default.t[id#20,name#21] parquet
   +- SubqueryAlias l2
      +- Project [id#31]
         +- Filter (count(distinct name#26)#33L > cast(1 as bigint))
            +- !Aggregate [id#31], [id#31, count(distinct name#26) AS count(distinct name#26)#33L]
               +- SubqueryAlias spark_catalog.default.v1
                  +- View (`default`.`v1`, [id#31,name#32])
                     +- Project [cast(id#27 as int) AS id#31, cast(name#28 as string) AS name#32]
                        +- Project [id#27, name#28]
                           +- SubqueryAlias spark_catalog.default.t
                              +- Relation default.t[id#27,name#28] parquet
```
Spark will consider the two views to be duplicates, which will cause the query to fail. We can add new aliases to attributes in the view to avoid two views from being considered duplicates.

Optimization plan after this change：
```
Aggregate [name#26, id#25], [id#25]
+- Join Inner, (id#25 = id#27)
   :- SubqueryAlias l1
   :  +- SubqueryAlias spark_catalog.default.v1
   :     +- View (`default`.`v1`, [id#25,name#26])
   :        +- Project [cast(id#20 as int) AS id#25, cast(name#21 as string) AS name#26]
   :           +- Project [id#20, name#21]
   :              +- SubqueryAlias spark_catalog.default.t
   :                 +- Relation default.t[id#20,name#21] parquet
   +- SubqueryAlias l2
      +- Project [id#27]
         +- Filter (count(distinct name#28)#33L > cast(1 as bigint))
            +- Aggregate [id#27], [id#27, count(distinct name#28) AS count(distinct name#28)#33L]
               +- SubqueryAlias spark_catalog.default.v1
                  +- View (`default`.`v1`, [id#27,name#28])
                     +- Project [cast(id#29 as int) AS id#27, cast(name#30 as string) AS name#28]
                        +- Project [id#29, name#30]
                           +- SubqueryAlias spark_catalog.default.t
                              +- Relation default.t[id#29,name#30] parquet
```
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Fix bug when using join in duplicate views.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. When we join with duplicate view, the query would be successful.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Add new UT",https://api.github.com/repos/apache/spark/issues/35361/timeline,,spark,apache,chenzhx,18550937,MDQ6VXNlcjE4NTUwOTM3,https://avatars.githubusercontent.com/u/18550937?v=4,,https://api.github.com/users/chenzhx,https://github.com/chenzhx,https://api.github.com/users/chenzhx/followers,https://api.github.com/users/chenzhx/following{/other_user},https://api.github.com/users/chenzhx/gists{/gist_id},https://api.github.com/users/chenzhx/starred{/owner}{/repo},https://api.github.com/users/chenzhx/subscriptions,https://api.github.com/users/chenzhx/orgs,https://api.github.com/users/chenzhx/repos,https://api.github.com/users/chenzhx/events{/privacy},https://api.github.com/users/chenzhx/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35361,https://github.com/apache/spark/pull/35361,https://github.com/apache/spark/pull/35361.diff,https://github.com/apache/spark/pull/35361.patch,,https://api.github.com/repos/apache/spark/issues/35361/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
14,https://api.github.com/repos/apache/spark/issues/35360,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35360/labels{/name},https://api.github.com/repos/apache/spark/issues/35360/comments,https://api.github.com/repos/apache/spark/issues/35360/events,https://github.com/apache/spark/pull/35360,1117999188,PR_kwDOAQXtWs4xxENr,35360,[SPARK-38065][SQL] Improve the performance of DS V2 aggregate push-down,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-29T02:08:06Z,2022-01-30T03:46:55Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, the build-in scan builder that implement `SupportsPushDownAggregates` has itself's config to control aggregate push-down.
We can advance the judgement of the config which controls aggregate push-down.


### Why are the changes needed?
Improve the performance of DS V2 aggregate push-down.


### Does this PR introduce _any_ user-facing change?
'Yes'. This PR add a API `enablePushAggregation` in `SupportsPushDownAggregates`.


### How was this patch tested?
Exists tests.
",https://api.github.com/repos/apache/spark/issues/35360/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35360,https://github.com/apache/spark/pull/35360,https://github.com/apache/spark/pull/35360.diff,https://github.com/apache/spark/pull/35360.patch,,https://api.github.com/repos/apache/spark/issues/35360/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
15,https://api.github.com/repos/apache/spark/issues/35358,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35358/labels{/name},https://api.github.com/repos/apache/spark/issues/35358/comments,https://api.github.com/repos/apache/spark/issues/35358/events,https://github.com/apache/spark/pull/35358,1117860669,PR_kwDOAQXtWs4xwnmd,35358,[SPARK-38062][CORE] Avoid resolving placeholder hostname for FallbackStorage in BlockManagerDecommissioner,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-28T21:25:18Z,2022-01-28T21:34:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This updates `BlockManagerDecommissioner` to avoid treating ""remote"", the placeholder hostname used by `FALLBACK_BLOCK_MANAGER_ID`, as a valid hostname and attempting to perform a network transfer to it. If the `peer` it encounters matches the fallback block manager ID, it now goes directly to accessing `fallbackStorage`, instead of first attempting to treat it like a valid block manager ID.

In addition, this reverts the changes from SPARK-37318, which should no longer be necessary now that the underlying issue is resolved.

### Why are the changes needed?
See SPARK-38062 for a much more detailed explanation. The gist of it is that:
- Attempting to resolve ""remote"" can behave unexpectedly in some DNS environments. This can cause failures of the `FallbackStorageSuite` tests, but also could potentially cause issues in a production deployment.
- SPARK-37318 ""fixes"" the tests by skipping them if such a DNS environment is detected, but this has the obvious drawback of disabling the tests, and doesn't address the problem for production environments.
- Even if resolving ""remote"" does quickly fail, as the current code expects, it is semantically wrong -- we should not treat this placeholder as a valid hostname.

### Does this PR introduce _any_ user-facing change?
`FallbackStorage` may be resolved slightly quicker, as it removes an unnecessary lookup step, but it should be negligible in most environments. No other user-facing changes.

### How was this patch tested?
The DNS environment in which unit tests are run in an automated fashion at my company means that we experience an issue very similar to what is described in SPARK-37318. Without this patch, tests in `FallbackStorageSuite` consistently fail, exceeding their timeouts. With this patch, the tests consistently (and quickly!) succeed.",https://api.github.com/repos/apache/spark/issues/35358/timeline,,spark,apache,xkrogen,6570401,MDQ6VXNlcjY1NzA0MDE=,https://avatars.githubusercontent.com/u/6570401?v=4,,https://api.github.com/users/xkrogen,https://github.com/xkrogen,https://api.github.com/users/xkrogen/followers,https://api.github.com/users/xkrogen/following{/other_user},https://api.github.com/users/xkrogen/gists{/gist_id},https://api.github.com/users/xkrogen/starred{/owner}{/repo},https://api.github.com/users/xkrogen/subscriptions,https://api.github.com/users/xkrogen/orgs,https://api.github.com/users/xkrogen/repos,https://api.github.com/users/xkrogen/events{/privacy},https://api.github.com/users/xkrogen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35358,https://github.com/apache/spark/pull/35358,https://github.com/apache/spark/pull/35358.diff,https://github.com/apache/spark/pull/35358.patch,,https://api.github.com/repos/apache/spark/issues/35358/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
16,https://api.github.com/repos/apache/spark/issues/35357,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35357/labels{/name},https://api.github.com/repos/apache/spark/issues/35357/comments,https://api.github.com/repos/apache/spark/issues/35357/events,https://github.com/apache/spark/pull/35357,1117625063,PR_kwDOAQXtWs4xv4ue,35357,[SPARK-21195][CORE] MetricSystem should pick up dynamically registered metrics in sources,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-28T16:50:23Z,2022-01-29T15:49:41Z,,NONE,,False,"What changes were proposed in this pull request?

MetricSystem picks up new metrics from sources that are added throughout execution. If you do measurements via dynamic proxies you might not want to redeclare all metrics that the proxies will create and you'd prefer them to get populated as they're being produced. Right now all sources are processed only onceat startup and metrics are picked up only if they have been registered statically at compile time. Behaviour I am proposing lets you not have to declare metrics in two places.

This had been previously suggested in #18406, #29980 and #31267.

Why are the changes needed?

Currently there's no way to access MetricRegistry that MetricsSystem uses to hold its state and as such it's not possible to reprocess a source. MetricsSystem throws if any metric had already been registered previously.

n.b. the MetricRegistry is added as a constructor argument to make testing easier but could as well be accessed via reflection as a private variable.
Does this PR introduce any user-facing change?

No
How was this patch tested?

Added tests",https://api.github.com/repos/apache/spark/issues/35357/timeline,,spark,apache,robert3005,512084,MDQ6VXNlcjUxMjA4NA==,https://avatars.githubusercontent.com/u/512084?v=4,,https://api.github.com/users/robert3005,https://github.com/robert3005,https://api.github.com/users/robert3005/followers,https://api.github.com/users/robert3005/following{/other_user},https://api.github.com/users/robert3005/gists{/gist_id},https://api.github.com/users/robert3005/starred{/owner}{/repo},https://api.github.com/users/robert3005/subscriptions,https://api.github.com/users/robert3005/orgs,https://api.github.com/users/robert3005/repos,https://api.github.com/users/robert3005/events{/privacy},https://api.github.com/users/robert3005/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35357,https://github.com/apache/spark/pull/35357,https://github.com/apache/spark/pull/35357.diff,https://github.com/apache/spark/pull/35357.patch,,https://api.github.com/repos/apache/spark/issues/35357/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
17,https://api.github.com/repos/apache/spark/issues/35356,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35356/labels{/name},https://api.github.com/repos/apache/spark/issues/35356/comments,https://api.github.com/repos/apache/spark/issues/35356/events,https://github.com/apache/spark/pull/35356,1117414108,PR_kwDOAQXtWs4xvMkx,35356,[SPARK-38056][Web UI] Fix issue of Structured streaming not working in history server when using LevelDB,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-28T13:35:59Z,2022-01-30T01:18:10Z,,NONE,,False,"### What changes were proposed in this pull request?

Change type of `org.apache.spark.sql.streaming.ui.StreamingQueryData.runId` from `UUID` to `String`.

### Why are the changes needed?

In [SPARK-31953](https://github.com/apache/spark/commit/4f9667035886a67e6c9a4e8fad2efa390e87ca68), structured streaming support is added in history server. However this does not work when history server is using LevelDB instead of in-memory KV store.

- Level DB does not support `UUID` as key.
- If `spark.history.store.path` is set in history server to use Level DB, when writing info to the store during replaying events, error will occur.
- `StreamingQueryStatusListener` will throw exceptions when writing info, saying `java.lang.IllegalArgumentException: Type java.util.UUID not allowed as key.`.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Added `StreamingQueryDataSuite` to test whether `StreamingQueryData` can be written to both in-memory and LevelDB based store.
",https://api.github.com/repos/apache/spark/issues/35356/timeline,,spark,apache,kuwii,10705175,MDQ6VXNlcjEwNzA1MTc1,https://avatars.githubusercontent.com/u/10705175?v=4,,https://api.github.com/users/kuwii,https://github.com/kuwii,https://api.github.com/users/kuwii/followers,https://api.github.com/users/kuwii/following{/other_user},https://api.github.com/users/kuwii/gists{/gist_id},https://api.github.com/users/kuwii/starred{/owner}{/repo},https://api.github.com/users/kuwii/subscriptions,https://api.github.com/users/kuwii/orgs,https://api.github.com/users/kuwii/repos,https://api.github.com/users/kuwii/events{/privacy},https://api.github.com/users/kuwii/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35356,https://github.com/apache/spark/pull/35356,https://github.com/apache/spark/pull/35356.diff,https://github.com/apache/spark/pull/35356.patch,,https://api.github.com/repos/apache/spark/issues/35356/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
18,https://api.github.com/repos/apache/spark/issues/35355,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35355/labels{/name},https://api.github.com/repos/apache/spark/issues/35355/comments,https://api.github.com/repos/apache/spark/issues/35355/events,https://github.com/apache/spark/pull/35355,1117292886,PR_kwDOAQXtWs4xuy3x,35355,[SPARK-38054][SQL] DS V2 supports list namespaces of MySQL dialect,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-28T11:23:01Z,2022-01-29T01:25:56Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, `JDBCTableCatalog.scala` query namespaces show below.
```
      val schemaBuilder = ArrayBuilder.make[Array[String]]
      val rs = conn.getMetaData.getSchemas()
      while (rs.next()) {
        schemaBuilder += Array(rs.getString(1))
      }
      schemaBuilder.result
```

But the code cannot get any information when using MySQL JDBC driver.
This PR uses `SHOW SCHEMAS` to query namespaces of MySQL.
This PR also fix other issues below:

- Release the docker tests in `MySQLNamespaceSuite.scala`.
- Because MySQL doesn't support create comment of schema, let's throws `SQLFeatureNotSupportedException`.
- Because MySQL doesn't support `DROP SCHEMA` in `RESTRICT` mode, let's throws `SQLFeatureNotSupportedException`.
- Reactor `JdbcUtils.executeQuery` to avoid `java.sql.SQLException: Operation not allowed after ResultSet closed`.


### Why are the changes needed?
MySQL dialect supports query namespaces.


### Does this PR introduce _any_ user-facing change?
'Yes'.
Some API changed.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/35355/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35355,https://github.com/apache/spark/pull/35355,https://github.com/apache/spark/pull/35355.diff,https://github.com/apache/spark/pull/35355.patch,,https://api.github.com/repos/apache/spark/issues/35355/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
19,https://api.github.com/repos/apache/spark/issues/35354,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35354/labels{/name},https://api.github.com/repos/apache/spark/issues/35354/comments,https://api.github.com/repos/apache/spark/issues/35354/events,https://github.com/apache/spark/pull/35354,1117169511,PR_kwDOAQXtWs4xuY9y,35354,[WIP][SPARK-38053][SQL] Report driver side metric from datasource V2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-28T09:14:26Z,2022-01-29T17:01:54Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
WIP


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
WIP


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
WIP

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
WIP",https://api.github.com/repos/apache/spark/issues/35354/timeline,,spark,apache,Yikf,51110188,MDQ6VXNlcjUxMTEwMTg4,https://avatars.githubusercontent.com/u/51110188?v=4,,https://api.github.com/users/Yikf,https://github.com/Yikf,https://api.github.com/users/Yikf/followers,https://api.github.com/users/Yikf/following{/other_user},https://api.github.com/users/Yikf/gists{/gist_id},https://api.github.com/users/Yikf/starred{/owner}{/repo},https://api.github.com/users/Yikf/subscriptions,https://api.github.com/users/Yikf/orgs,https://api.github.com/users/Yikf/repos,https://api.github.com/users/Yikf/events{/privacy},https://api.github.com/users/Yikf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35354,https://github.com/apache/spark/pull/35354,https://github.com/apache/spark/pull/35354.diff,https://github.com/apache/spark/pull/35354.patch,,https://api.github.com/repos/apache/spark/issues/35354/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
20,https://api.github.com/repos/apache/spark/issues/35353,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35353/labels{/name},https://api.github.com/repos/apache/spark/issues/35353/comments,https://api.github.com/repos/apache/spark/issues/35353/events,https://github.com/apache/spark/pull/35353,1117070142,PR_kwDOAQXtWs4xuEUk,35353,[SPARK-38013][SQL][TEST] AQE can change bhj to smj if no extra shuffle introduce,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-28T07:06:34Z,2022-01-28T09:08:37Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add a test case in `AdaptiveQueryExecSuite`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
AQE can change bhj to smj, and it requires two conditions:
- no extra shuffle introduce, otherwise the built-in cost evaluator will ban it
- AQE does not think the join can be planned as broadcast join. That says the cost statistics in normal planner is not accurate.

It's counterintuitive, but it's an expected behavior as AQE designed.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Pass CI",https://api.github.com/repos/apache/spark/issues/35353/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35353,https://github.com/apache/spark/pull/35353,https://github.com/apache/spark/pull/35353.diff,https://github.com/apache/spark/pull/35353.patch,,https://api.github.com/repos/apache/spark/issues/35353/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
21,https://api.github.com/repos/apache/spark/issues/35352,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35352/labels{/name},https://api.github.com/repos/apache/spark/issues/35352/comments,https://api.github.com/repos/apache/spark/issues/35352/events,https://github.com/apache/spark/pull/35352,1117061970,PR_kwDOAQXtWs4xuCof,35352,[SPARK-38063][SQL] Support split_part Function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2022-01-28T06:52:58Z,2022-02-01T19:44:04Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
`split_part()` is a commonly supported function by other systems such as Postgres and some other systems.

The Spark equivalent is `element_at(split(arg, delim), part)`


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Adding new SQL function.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes. This PR adds a new function so there is no previous behavior. The following demonstrates more about the new function:

syntax: `split_part(str, delimiter, partNum)`

This function splits `str` by `delimiter` and return requested part of the split (1-based). If any input parameter is NULL, return NULL.

`str` and `delimiter` are the same type as `string`. `partNum` is `integer` type

Examples:
```
      > SELECT _FUNC_('11.12.13', '.', 3);
       13
      > SELECT _FUNC_(NULL, '.', 3);
      NULL
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Unit Test
",https://api.github.com/repos/apache/spark/issues/35352/timeline,,spark,apache,amaliujia,1938382,MDQ6VXNlcjE5MzgzODI=,https://avatars.githubusercontent.com/u/1938382?v=4,,https://api.github.com/users/amaliujia,https://github.com/amaliujia,https://api.github.com/users/amaliujia/followers,https://api.github.com/users/amaliujia/following{/other_user},https://api.github.com/users/amaliujia/gists{/gist_id},https://api.github.com/users/amaliujia/starred{/owner}{/repo},https://api.github.com/users/amaliujia/subscriptions,https://api.github.com/users/amaliujia/orgs,https://api.github.com/users/amaliujia/repos,https://api.github.com/users/amaliujia/events{/privacy},https://api.github.com/users/amaliujia/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35352,https://github.com/apache/spark/pull/35352,https://github.com/apache/spark/pull/35352.diff,https://github.com/apache/spark/pull/35352.patch,,https://api.github.com/repos/apache/spark/issues/35352/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
22,https://api.github.com/repos/apache/spark/issues/35350,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35350/labels{/name},https://api.github.com/repos/apache/spark/issues/35350/comments,https://api.github.com/repos/apache/spark/issues/35350/events,https://github.com/apache/spark/pull/35350,1117031074,PR_kwDOAQXtWs4xt8N4,35350,[SPARK-38052][SQL] Refactor UnsafeRow#isFixedLength and UnsafeRow#isMutable use looping,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2022-01-28T05:57:42Z,2022-01-31T10:53:18Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Methods UnsafeRow#isFixedLength and UnsafeRow#isMutable use tail recursion now , this pr can refactor with looping, which will be considerably faster.



### Why are the changes needed?
Replace tail recursion with looping.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Pass GA",https://api.github.com/repos/apache/spark/issues/35350/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35350,https://github.com/apache/spark/pull/35350,https://github.com/apache/spark/pull/35350.diff,https://github.com/apache/spark/pull/35350.patch,,https://api.github.com/repos/apache/spark/issues/35350/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
23,https://api.github.com/repos/apache/spark/issues/35345,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35345/labels{/name},https://api.github.com/repos/apache/spark/issues/35345/comments,https://api.github.com/repos/apache/spark/issues/35345/events,https://github.com/apache/spark/pull/35345,1116968041,PR_kwDOAQXtWs4xtvR9,35345,[SPARK-37145][K8S] Add KubernetesCustom[Driver/Executor]FeatureConfigStep developer api,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-28T03:40:12Z,2022-02-01T16:12:41Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This patch adds the support for extending user feature steps with configuration by adding 2 developer api:
- KubernetesDriverCustomFeatureConfigStep: to help user extend custom feature step in executor side
- KubernetesExecutorCustomFeatureConfigStep: to help user extend custom feature step in driver side

Before this patch user can only add feature step like:
- `class TestStep extends KubernetesFeatureConfigStep`: without any kubernetes conf 

After this patch user can add feature step with configuration like:
- `class TestStepWithDriverConf extends KubernetesDriverCustomFeatureConfigStep`: only driver
- `class TestStepWithExecConf extends KubernetesExecutorCustomFeatureConfigStep`: only executor
- `class TestStepWithK8SConf extends KubernetesDriverCustomFeatureConfigStep with KubernetesExecutorCustomFeatureConfigStep`: both driver and executor

### Why are the changes needed?
In https://github.com/apache/spark/pull/30206 , a developer API for custom feature steps has been added, but it didn't support initialize user feature step with kubernetes conf (like `KubernetesConf`/`KubernetesDriverConf`/`KubernetesExecutorConf`).

In most of scenarios, users want to make corresponding changes in their feature steps according to the configuration. Such as, the customized scheduler scenario, user wants to configure pod according to passed job configuration.

### Does this PR introduce _any_ user-facing change?
Improve the developer API for for custom feature steps.


### How was this patch tested?
- Added UT
- Runing k8s integration test manaully: `build/sbt -Pkubernetes -Pkubernetes-integration-tests -Dtest.exclude.tags=minikube,r ""kubernetes-integration-tests/test`

Closes: https://github.com/apache/spark/pull/34924",https://api.github.com/repos/apache/spark/issues/35345/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35345,https://github.com/apache/spark/pull/35345,https://github.com/apache/spark/pull/35345.diff,https://github.com/apache/spark/pull/35345.patch,,https://api.github.com/repos/apache/spark/issues/35345/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
24,https://api.github.com/repos/apache/spark/issues/35343,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35343/labels{/name},https://api.github.com/repos/apache/spark/issues/35343/comments,https://api.github.com/repos/apache/spark/issues/35343/events,https://github.com/apache/spark/pull/35343,1116712415,PR_kwDOAQXtWs4xs6vv,35343,[SPARK-38046][SS][TEST] Fix KafkaSource/KafkaMicroBatch flaky test due to non-deterministic timing,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2022-01-27T20:34:40Z,2022-01-29T19:29:19Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Fix a flaky test in KafkaMicroBatchSourceSuite

### Why are the changes needed?

There is a test call ""compositeReadLimit""

https://github.com/apache/spark/blob/master/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala#L460

that is flaky.  The problem is because the Kakfa connector is always getting the actual system time and not advancing it manually, thus leaving room for non-deterministic behaviors especially since the source determines if ""maxTriggerDelayMs"" is satisfied by comparing the last trigger time with the current system time.  One can simply ""sleep"" at points in the test to generate different outcomes.

### Does this PR introduce _any_ user-facing change?

no

### How was this patch tested?

",https://api.github.com/repos/apache/spark/issues/35343/timeline,,spark,apache,jerrypeng,3613359,MDQ6VXNlcjM2MTMzNTk=,https://avatars.githubusercontent.com/u/3613359?v=4,,https://api.github.com/users/jerrypeng,https://github.com/jerrypeng,https://api.github.com/users/jerrypeng/followers,https://api.github.com/users/jerrypeng/following{/other_user},https://api.github.com/users/jerrypeng/gists{/gist_id},https://api.github.com/users/jerrypeng/starred{/owner}{/repo},https://api.github.com/users/jerrypeng/subscriptions,https://api.github.com/users/jerrypeng/orgs,https://api.github.com/users/jerrypeng/repos,https://api.github.com/users/jerrypeng/events{/privacy},https://api.github.com/users/jerrypeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35343,https://github.com/apache/spark/pull/35343,https://github.com/apache/spark/pull/35343.diff,https://github.com/apache/spark/pull/35343.patch,,https://api.github.com/repos/apache/spark/issues/35343/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
25,https://api.github.com/repos/apache/spark/issues/35342,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35342/labels{/name},https://api.github.com/repos/apache/spark/issues/35342/comments,https://api.github.com/repos/apache/spark/issues/35342/events,https://github.com/apache/spark/pull/35342,1116213359,PR_kwDOAQXtWs4xrPzT,35342,[SPARK-38043][SQL] Refactor FileBasedDataSourceSuite and add DataSourceSuite for each data source,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-27T13:26:42Z,2022-01-30T01:16:46Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Current test framework for file based data source is complex and mess, 
this pr we aim to convert FileBasedDataSourceSuite to a base suite, and implement it for each datasources.

Later I will review some existing unit tests to integrate into this new testing framework

### Why are the changes needed?
Refactor　code


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existed UT
",https://api.github.com/repos/apache/spark/issues/35342/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35342,https://github.com/apache/spark/pull/35342,https://github.com/apache/spark/pull/35342.diff,https://github.com/apache/spark/pull/35342.patch,,https://api.github.com/repos/apache/spark/issues/35342/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
26,https://api.github.com/repos/apache/spark/issues/35337,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35337/labels{/name},https://api.github.com/repos/apache/spark/issues/35337/comments,https://api.github.com/repos/apache/spark/issues/35337/events,https://github.com/apache/spark/pull/35337,1115375871,PR_kwDOAQXtWs4xoj8f,35337,[SPARK-37840][SQL] Dynamic Update of UDF,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2022-01-26T18:59:25Z,2022-01-28T11:42:48Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
If we need to change definitions of our UDFs using same jarname and classname, we need to restart thriftserver.  

`CREATE FUNCTION func AS class1 USING JAR jar1`
followed by
`CREATE OR REPLACE FUNCTION func AS class1 USING JAR jar1`
won't work. This is due to the fact that we are not able to update our classloader with latest version of jar and rpc file server is also not updated along with SparkContext.addedJars.
In production environment, it is not always feasible to restart session.


In this PR, i propose to update our UDFs dynamically.
To achieve this, user can update their UDFs using `CREATE OR REPLACE` command
For e.g:
1. Create a new function
`CREATE TEMPORARY FUNCTION func AS c1 USING JAR jar1;`

2. Once associated jar files are updated, enable dynamic update config and fire following command
`CREATE OR REPLACE TEMPORARY FUNCTION func AS c1 USING JAR jar1;`

Above query will update the classpaths and users will be able to use updated jars.
We can update both permanent and temporary functions.
A new configuration is added for this PR : `spark.sql.function.updateUdfResources`
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
We need to start our thriftserver whenever we want to update our UDF definition. This is not always possible in production environment. These changes will allow users to update them dynamically.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
Yes,
Now users can update their UDFs without restarting thriftserver using CREATE OR REPLACE FUNCTION command, if dynamic update is turned on.
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Unit Tests added.
More tests will be added for testing other scenarios.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35337/timeline,,spark,apache,iRakson,15366835,MDQ6VXNlcjE1MzY2ODM1,https://avatars.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35337,https://github.com/apache/spark/pull/35337,https://github.com/apache/spark/pull/35337.diff,https://github.com/apache/spark/pull/35337.patch,,https://api.github.com/repos/apache/spark/issues/35337/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
27,https://api.github.com/repos/apache/spark/issues/35336,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35336/labels{/name},https://api.github.com/repos/apache/spark/issues/35336/comments,https://api.github.com/repos/apache/spark/issues/35336/events,https://github.com/apache/spark/pull/35336,1115295112,PR_kwDOAQXtWs4xoUXC,35336,[SPARK-37936][SQL] Use error classes in the parsing errors of intervals,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2022-01-26T17:33:56Z,2022-01-28T16:20:49Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->
In the PR, We propose to throw ParseException from below methodswith the error classes:

moreThanOneFromToUnitInIntervalLiteralError
invalidIntervalLiteralError
invalidIntervalFormError
invalidFromToUnitValueError
fromToIntervalUnsupportedError
mixedIntervalUnitsError
MORE_THAN_ONE_FROM_TO_UNIT_IN_INTERVAL_LITERAL - moreThanOneFromToUnitInIntervalLiteralError
INVALID_INTERVAL_LITERAL - invalidIntervalLiteralError
INVALID_INTERVAL_FORM - invalidIntervalFormError
INVALID_FROM_TO_UNIT_VALUE - invalidFromToUnitValueError
UNSUPPORTED_FROM_TO_INTERVAL - fromToIntervalUnsupportedError
MIXED_INTERVAL_UNITS - mixedIntervalUnitsError

New error classes are added to error-classes.json.

New test suite QueryParsingErrorsSuite for checking the errors has been created.
### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Please refer SPARK-37935 - Migrate onto error classes

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Yes

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
By running new test suite:
New Test suite created",https://api.github.com/repos/apache/spark/issues/35336/timeline,,spark,apache,senthh,9917543,MDQ6VXNlcjk5MTc1NDM=,https://avatars.githubusercontent.com/u/9917543?v=4,,https://api.github.com/users/senthh,https://github.com/senthh,https://api.github.com/users/senthh/followers,https://api.github.com/users/senthh/following{/other_user},https://api.github.com/users/senthh/gists{/gist_id},https://api.github.com/users/senthh/starred{/owner}{/repo},https://api.github.com/users/senthh/subscriptions,https://api.github.com/users/senthh/orgs,https://api.github.com/users/senthh/repos,https://api.github.com/users/senthh/events{/privacy},https://api.github.com/users/senthh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35336,https://github.com/apache/spark/pull/35336,https://github.com/apache/spark/pull/35336.diff,https://github.com/apache/spark/pull/35336.patch,,https://api.github.com/repos/apache/spark/issues/35336/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
28,https://api.github.com/repos/apache/spark/issues/35335,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35335/labels{/name},https://api.github.com/repos/apache/spark/issues/35335/comments,https://api.github.com/repos/apache/spark/issues/35335/events,https://github.com/apache/spark/pull/35335,1114954080,PR_kwDOAQXtWs4xnOqR,35335,[SPARK-38036][SQL][TESTS] Refactor `VersionsSuite` to `HiveClientSuite` and make it a subclass of `HiveVersionSuite`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-26T12:15:18Z,2022-01-27T03:12:05Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
There is a TODO in `VersionsSuite`:

- TODO: Refactor this to `HiveClientSuite` and make it a subclass of `HiveVersionSuite`

this pr completed this TODO, the main change as follows:

- copy all test cases in `versions.foreach` scope of `VersionsSuite` to `HiveClientSuite`
- override `nestedSuites` function in `HiveClientSuites` to use each hive version to test the cases in `HiveClientSuite` similar as `HiveClientUserNameSuites` and `HivePartitionFilteringSuites`
- move other cases to `HiveClientSuites`

### Why are the changes needed?
Make `VersionsSuite` as a subclass of `HiveVersionSuite`  to unify the test mode of multi version hive



### Does this PR introduce _any_ user-facing change?
No.


### How was this patch tested?
- Pass GA
- Manual test:

**Before**

```
mvn clean install -DskipTests -pl sql/hive -am 
mvn test -pl sql/hive -Dtest=none -DwildcardSuites=org.apache.spark.sql.hive.client.HiveClientSuites

Run completed in 13 minutes, 10 seconds.
Total number of tests run: 867
Suites: completed 2, aborted 0
Tests: succeeded 867, failed 0, canceled 0, ignored 1, pending 0
All tests passed.
```

**After**

```
mvn clean install -DskipTests -pl sql/hive -am 
mvn test -pl sql/hive -Dtest=none -DwildcardSuites=org.apache.spark.sql.hive.client.HiveClientSuites

Run completed in 3 minutes, 8 seconds.
Total number of tests run: 867
Suites: completed 14, aborted 0
Tests: succeeded 867, failed 0, canceled 0, ignored 1, pending 0
All tests passed
```

The number of test cases is the same, and Suites changed from 2 to 14
",https://api.github.com/repos/apache/spark/issues/35335/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35335,https://github.com/apache/spark/pull/35335,https://github.com/apache/spark/pull/35335.diff,https://github.com/apache/spark/pull/35335.patch,,https://api.github.com/repos/apache/spark/issues/35335/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
29,https://api.github.com/repos/apache/spark/issues/35334,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35334/labels{/name},https://api.github.com/repos/apache/spark/issues/35334/comments,https://api.github.com/repos/apache/spark/issues/35334/events,https://github.com/apache/spark/pull/35334,1114923525,PR_kwDOAQXtWs4xnILz,35334,[SPARK-38034][SQL] Optimize TransposeWindow rule,[],open,False,,[],,2,2022-01-26T11:39:05Z,2022-01-30T04:01:47Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Optimize the TransposeWindow rule to extend applicable cases and optimize time complexity.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
We could apply the rule for more cases, which could improve the execution performance by eliminate unnecessary shuffle, and by reducing the time complexity from O(n!) to O(n2), the performance for the rule itself could improve

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
UT",https://api.github.com/repos/apache/spark/issues/35334/timeline,,spark,apache,constzhou,5299740,MDQ6VXNlcjUyOTk3NDA=,https://avatars.githubusercontent.com/u/5299740?v=4,,https://api.github.com/users/constzhou,https://github.com/constzhou,https://api.github.com/users/constzhou/followers,https://api.github.com/users/constzhou/following{/other_user},https://api.github.com/users/constzhou/gists{/gist_id},https://api.github.com/users/constzhou/starred{/owner}{/repo},https://api.github.com/users/constzhou/subscriptions,https://api.github.com/users/constzhou/orgs,https://api.github.com/users/constzhou/repos,https://api.github.com/users/constzhou/events{/privacy},https://api.github.com/users/constzhou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35334,https://github.com/apache/spark/pull/35334,https://github.com/apache/spark/pull/35334.diff,https://github.com/apache/spark/pull/35334.patch,,https://api.github.com/repos/apache/spark/issues/35334/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
30,https://api.github.com/repos/apache/spark/issues/35332,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35332/labels{/name},https://api.github.com/repos/apache/spark/issues/35332/comments,https://api.github.com/repos/apache/spark/issues/35332/events,https://github.com/apache/spark/pull/35332,1114647223,PR_kwDOAQXtWs4xmPJm,35332,[SPARK-38030][SQL] Canonicalization should not remove nullability of AttributeReference dataType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-26T05:32:57Z,2022-01-31T17:58:59Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Canonicalization of AttributeReference should not remove nullability information of its dataType.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
SPARK-38030 lists an issue where canonicalization of cast resulted in an unresolved expression, thus causing query failure. The issue was that the child AttributeReference's dataType was converted to nullable during canonicalization and hence the Cast's `checkInputDataTypes` fails. Although the exact repro listed in SPARK-38030 no longer works in master due to an unrelated change (details in the JIRA), some other codepaths which depend on canonicalized representations can trigger the same issue.

<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Added unit test to ensure that canonicalization preserves nullability of AttributeReference and does not result in an unresolved cast
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35332/timeline,,spark,apache,shardulm94,6961317,MDQ6VXNlcjY5NjEzMTc=,https://avatars.githubusercontent.com/u/6961317?v=4,,https://api.github.com/users/shardulm94,https://github.com/shardulm94,https://api.github.com/users/shardulm94/followers,https://api.github.com/users/shardulm94/following{/other_user},https://api.github.com/users/shardulm94/gists{/gist_id},https://api.github.com/users/shardulm94/starred{/owner}{/repo},https://api.github.com/users/shardulm94/subscriptions,https://api.github.com/users/shardulm94/orgs,https://api.github.com/users/shardulm94/repos,https://api.github.com/users/shardulm94/events{/privacy},https://api.github.com/users/shardulm94/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35332,https://github.com/apache/spark/pull/35332,https://github.com/apache/spark/pull/35332.diff,https://github.com/apache/spark/pull/35332.patch,,https://api.github.com/repos/apache/spark/issues/35332/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
31,https://api.github.com/repos/apache/spark/issues/35329,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35329/labels{/name},https://api.github.com/repos/apache/spark/issues/35329/comments,https://api.github.com/repos/apache/spark/issues/35329/events,https://github.com/apache/spark/pull/35329,1114592709,PR_kwDOAQXtWs4xmD7c,35329,[SPARK-33326][SQL] PartitionStatistic add numFiles,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-26T03:34:35Z,2022-01-27T04:35:13Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, in analyze partition command, we only update statistic about `Partition Statistics`, but won't update parameters in `Partition Parameters`,  the information such as `numFiles` is very useful for user, for compatibility with hive, we should update these two.


### Why are the changes needed?
Keep compatibility with hive

### Does this PR introduce _any_ user-facing change?
After running analyze partition command, user can got latest statistic in partition parameters.


### How was this patch tested?
Added UT",https://api.github.com/repos/apache/spark/issues/35329/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35329,https://github.com/apache/spark/pull/35329,https://github.com/apache/spark/pull/35329.diff,https://github.com/apache/spark/pull/35329.patch,,https://api.github.com/repos/apache/spark/issues/35329/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
32,https://api.github.com/repos/apache/spark/issues/35328,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35328/labels{/name},https://api.github.com/repos/apache/spark/issues/35328/comments,https://api.github.com/repos/apache/spark/issues/35328/events,https://github.com/apache/spark/pull/35328,1114577668,PR_kwDOAQXtWs4xmA2c,35328,[SPARK-37937][SQL] Use error classes in the parsing errors of lateral join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-26T03:04:03Z,2022-01-31T03:16:53Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
In the PR, I propose to use the following error classes for the parsing errors of lateral joins:
`INVALID_LATERAL_JOIN_RELATION`, `LATERAL_JOIN_WITH_NATURAL_JOIN_UNSUPPORTED`, `LATERAL_JOIN_WITH_USING_JOIN_UNSUPPORTED`, and `UNSUPPORTED_LATERAL_JOIN_TYPE`

These new error classes are added to `error-classes.json`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Porting the parsing errors for lateral join to the new error framework should improve user experience with Spark SQL.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added new test suite",https://api.github.com/repos/apache/spark/issues/35328/timeline,,spark,apache,imback82,12103644,MDQ6VXNlcjEyMTAzNjQ0,https://avatars.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35328,https://github.com/apache/spark/pull/35328,https://github.com/apache/spark/pull/35328.diff,https://github.com/apache/spark/pull/35328.patch,,https://api.github.com/repos/apache/spark/issues/35328/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
33,https://api.github.com/repos/apache/spark/issues/35323,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35323/labels{/name},https://api.github.com/repos/apache/spark/issues/35323/comments,https://api.github.com/repos/apache/spark/issues/35323/events,https://github.com/apache/spark/pull/35323,1114164987,PR_kwDOAQXtWs4xkrPA,35323,[SPARK-38025][SQL][TEST] Improve test suite ExternalCatalogSuite,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-25T17:22:54Z,2022-01-26T12:26:36Z,,NONE,,False,"### What changes were proposed in this pull request?
This PR improves test suite ExternalCatalogSuite.scala by removing repetitive code and replacing them with already available utility function with some minor changes. This will reduce redundant code, simplify the suite and improve readability.


### Why are the changes needed?
Reduce code, simplify the suite and improve readability.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Locally using `build/sbt -v -d ""test:testOnly *CatalogSuite""` and in the local workflow actions
",https://api.github.com/repos/apache/spark/issues/35323/timeline,,spark,apache,khalidmammadov,11574708,MDQ6VXNlcjExNTc0NzA4,https://avatars.githubusercontent.com/u/11574708?v=4,,https://api.github.com/users/khalidmammadov,https://github.com/khalidmammadov,https://api.github.com/users/khalidmammadov/followers,https://api.github.com/users/khalidmammadov/following{/other_user},https://api.github.com/users/khalidmammadov/gists{/gist_id},https://api.github.com/users/khalidmammadov/starred{/owner}{/repo},https://api.github.com/users/khalidmammadov/subscriptions,https://api.github.com/users/khalidmammadov/orgs,https://api.github.com/users/khalidmammadov/repos,https://api.github.com/users/khalidmammadov/events{/privacy},https://api.github.com/users/khalidmammadov/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35323,https://github.com/apache/spark/pull/35323,https://github.com/apache/spark/pull/35323.diff,https://github.com/apache/spark/pull/35323.patch,,https://api.github.com/repos/apache/spark/issues/35323/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
34,https://api.github.com/repos/apache/spark/issues/35320,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35320/labels{/name},https://api.github.com/repos/apache/spark/issues/35320/comments,https://api.github.com/repos/apache/spark/issues/35320/events,https://github.com/apache/spark/pull/35320,1113820413,PR_kwDOAQXtWs4xjiY7,35320,[SPARK-37839][SQL][FOLLOWUP] Check overflow when DS V2 partial aggregate push-down `AVG`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-25T12:19:16Z,2022-01-26T06:24:36Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
https://github.com/apache/spark/pull/35130 supports partial aggregate push-down `AVG` for DS V2.
The behavior doesn't consistent with `Average` if occurs overflow in ansi mode.
This PR closely follows the implement of `Average` to respect overflow in ansi mode.


### Why are the changes needed?
Make the behavior consistent with `Average` if occurs overflow in ansi mode.


### Does this PR introduce _any_ user-facing change?
'Yes'.
Users could see the exception about overflow throws in ansi mode.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/35320/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35320,https://github.com/apache/spark/pull/35320,https://github.com/apache/spark/pull/35320.diff,https://github.com/apache/spark/pull/35320.patch,,https://api.github.com/repos/apache/spark/issues/35320/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
35,https://api.github.com/repos/apache/spark/issues/35319,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35319/labels{/name},https://api.github.com/repos/apache/spark/issues/35319/comments,https://api.github.com/repos/apache/spark/issues/35319/events,https://github.com/apache/spark/pull/35319,1113774246,PR_kwDOAQXtWs4xjYua,35319,[SPARK-36571][SQL] Add new SQLPathHadoopMapReduceCommitProtocol resolve conflict when write into partition table's different partition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-25T11:30:09Z,2022-01-26T08:49:40Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
For current data source insert sql commit protocol, it have below problems:
**case a**: both job A and job B write data into partitioned table TBL with different statistic partition, it will have conflict since they use same temp location ${table_location}/_temporary/0/....,
when job A has finished and then it will clean this temp location, then cause job B’s temp data is cleared. Then it will cause  job B to fail to write data.
**case b**: for current dynamic partition insert, if we kill a job writing data, will remain data under table location in the staging dir under table path.
**case c**: If we use a dynamic partition insert to insert a new table with a huge number of partitions, we need to move partition path one by one, for this case, we can just rename stagingdir path to table path to make it more quicker. But if we want to do this, we need to make staging dir can be customized and should not use the staging path under table location.
 
In this approach, we plan to do two thing:

1. Make staging dir can be customized like hive 

    - When we terminate a job doing dynamic partition insert, it will remain staging dir under table location . If  we make the staging dir can be customized like hive(such as define staging dir as /tmp/spark/.stagingdir ) can avoid remaining such staging dir and data under table path.

    - If we define staging dir using a tmp location, not under table location, when we use dynamic partition insert to write data to a new table, we can just rename staging dir to target table location to avoid moving partition dir one by one. It’s more quickly.

    - If we can customize staging dir,  we can implement a new commit protocol to enable use of staging dir to write non-partitioned table and static partition insert use staging dir and won’t increase FS operation.

2. New SQL Commit protocol supports staging dir and won’t increase FS operation. 

For current static partition insert in v1, it have step:

    1. Task attempts firstly write files under the intermediate path, e.g. /path/to/outputPath/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/{part_spec_path}/xxx.parquet.

    2. Then task commit file to /path/to/outputPath/_temporary/{appId}/_temporary/{taskId}/{part_spec_path}/xxx.parquet.

    3. Job commit move file to /path/to/outputPath/{part_spec_path}/xxx.parquet.

For current dynamic partition insert in v1, it have step:

    1. Task attempts firstly write files under the intermediate path, e.g. /path/to/outputPath/.spark-staging-{jobId}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/a=1/b=1/xxx.parquet.
    2. Then task commit file to /path/to/outputPath/.spark-staging-{jobId}/_temporary/{appId}/_temporary/{taskId}/a=1/b=1/xxx.parquet.
   3. During Job commit move file to /path/to/outputPath/a=1/b=1/xxx.parquet.

In this new sql commit protocol **SQLPathHadoopMapReduceCommitProtocol** , 

for non-partition insert:

    1. Task attempts firstly write files under the intermediate path, e.g. {staging_dir}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/xxx.parquet.
    2. Then task commit file to {staging_dir}/_temporary/{appId}/_temporary/{taskId}/xxx.parquet.
    3. When job commit, moving file to /path/to/outputPath/xxx.parquet.
 
for all static partition insert:

    1. Task attempts firstly write files under the intermediate path, e.g. {staging_dir}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/{part_spec_path}/xxx.parquet.
    2. Then task commit file to {staging_dir}/_temporary/{appId}/_temporary/{taskId}/{part_spec_path}/xxx.parquet.
    3. When job commit, moving file to /path/to/outputPath/{part_spec_path}/xxx.parquet.

for dynamic partition insert,  we implement it as:

    1. Task attempts firstly write files under the intermediate path, e.g. {staging_dir}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/{part_spec_path}/xxx.parquet.
    2. Then task commit file to {staging_dir}/_temporary/{appId}/_temporary/{taskId}/{part_spec_path}/xxx.parquet.
    3. When job commit, move file to /{staging_dir}/{part_spec_path}/xxx.parquet.  Then, if we write to a empty table path, we directly rename   /{staging_dir} to /{table_path}, if not, we move partition dir one by one from  /{staging_dir}/{part_spec_path} to  /{table_location}/{part_spec_path}

The new sql commit protocal’s benefit is:
    - Can support Insert into non-partitioned table form it self
    - Can support Insert into partition table's statistic partition and read data from target partition.
    - Can support Insert into different partition using statistic partition concurrently 

These are all  normal problems when we use data source API insert data


### Why are the changes needed?
Provide a more flexible commit protocol and won't impact perf


### Does this PR introduce _any_ user-facing change?
User can set sql commit protocol to `SQLPathHadoopMapReduceCommitProtocol` to use a commit protocol with staging dir


### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/35319/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35319,https://github.com/apache/spark/pull/35319,https://github.com/apache/spark/pull/35319.diff,https://github.com/apache/spark/pull/35319.patch,,https://api.github.com/repos/apache/spark/issues/35319/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
36,https://api.github.com/repos/apache/spark/issues/35310,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35310/labels{/name},https://api.github.com/repos/apache/spark/issues/35310/comments,https://api.github.com/repos/apache/spark/issues/35310/events,https://github.com/apache/spark/pull/35310,1113466936,PR_kwDOAQXtWs4xiaTg,35310,[SPARK-38014][SQL][TESTS] Add Parquet Data Page V2 write scenario for `BuiltInDataSourceWriteBenchmark`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-25T06:09:05Z,2022-01-26T02:34:43Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`BuiltInDataSourceWriteBenchmark` only test data page V1  for parquet now, this pr add parquet data page V2 write scenario and update relevant benchmark result files.



### Why are the changes needed?
Add micro benchmark scene for parquet




### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Pass GA
",https://api.github.com/repos/apache/spark/issues/35310/timeline,,spark,apache,LuciferYang,1475305,MDQ6VXNlcjE0NzUzMDU=,https://avatars.githubusercontent.com/u/1475305?v=4,,https://api.github.com/users/LuciferYang,https://github.com/LuciferYang,https://api.github.com/users/LuciferYang/followers,https://api.github.com/users/LuciferYang/following{/other_user},https://api.github.com/users/LuciferYang/gists{/gist_id},https://api.github.com/users/LuciferYang/starred{/owner}{/repo},https://api.github.com/users/LuciferYang/subscriptions,https://api.github.com/users/LuciferYang/orgs,https://api.github.com/users/LuciferYang/repos,https://api.github.com/users/LuciferYang/events{/privacy},https://api.github.com/users/LuciferYang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35310,https://github.com/apache/spark/pull/35310,https://github.com/apache/spark/pull/35310.diff,https://github.com/apache/spark/pull/35310.patch,,https://api.github.com/repos/apache/spark/issues/35310/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
37,https://api.github.com/repos/apache/spark/issues/35307,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35307/labels{/name},https://api.github.com/repos/apache/spark/issues/35307/comments,https://api.github.com/repos/apache/spark/issues/35307/events,https://github.com/apache/spark/pull/35307,1113344369,PR_kwDOAQXtWs4xiBEn,35307,[SPARK-38008][CORE] Fix the method description of refill,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-25T02:15:45Z,2022-01-26T13:29:53Z,,CONTRIBUTOR,,False,"

### What changes were proposed in this pull request?

Fix the method description of refill.

### Why are the changes needed?
Easy to understand.
### Does this PR introduce _any_ user-facing change?
No.


### How was this patch tested?

Existing tests.",https://api.github.com/repos/apache/spark/issues/35307/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35307,https://github.com/apache/spark/pull/35307,https://github.com/apache/spark/pull/35307.diff,https://github.com/apache/spark/pull/35307.patch,,https://api.github.com/repos/apache/spark/issues/35307/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
38,https://api.github.com/repos/apache/spark/issues/35297,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35297/labels{/name},https://api.github.com/repos/apache/spark/issues/35297/comments,https://api.github.com/repos/apache/spark/issues/35297/events,https://github.com/apache/spark/pull/35297,1112079030,PR_kwDOAQXtWs4xd21l,35297,[SPARK-37993][SHUFFLE] Avoid multiple calls to configuration parameter values.,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-24T02:02:23Z,2022-01-24T02:29:22Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Avoid multiple calls to configuration parameter values.

### Why are the changes needed?
Improve performance.

### Does this PR introduce _any_ user-facing change?
No.


### How was this patch tested?
Existed UT.
",https://api.github.com/repos/apache/spark/issues/35297/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35297,https://github.com/apache/spark/pull/35297,https://github.com/apache/spark/pull/35297.diff,https://github.com/apache/spark/pull/35297.patch,,https://api.github.com/repos/apache/spark/issues/35297/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
39,https://api.github.com/repos/apache/spark/issues/35293,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35293/labels{/name},https://api.github.com/repos/apache/spark/issues/35293/comments,https://api.github.com/repos/apache/spark/issues/35293/events,https://github.com/apache/spark/pull/35293,1111926481,PR_kwDOAQXtWs4xdZqY,35293,[SPARK-37991][SQL][TESTS] Improve test case logic by replacing it with a test utility function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-23T16:58:09Z,2022-01-24T02:29:31Z,,NONE,,False,"### What changes were proposed in this pull request?

This PR improves test case ""**basic create and list partitions**"" inside `ExternalCatalogSuite.scala` by replacing set up code with a test utility function from the same suite: `newBasicCatalog()`

### Why are the changes needed?
Code reduction and simplification

### Does this PR introduce any user-facing change?
No

### How was this patch tested?

`build/sbt -v -d ""test:testOnly *CatalogSuite""`",https://api.github.com/repos/apache/spark/issues/35293/timeline,,spark,apache,khalidmammadov,11574708,MDQ6VXNlcjExNTc0NzA4,https://avatars.githubusercontent.com/u/11574708?v=4,,https://api.github.com/users/khalidmammadov,https://github.com/khalidmammadov,https://api.github.com/users/khalidmammadov/followers,https://api.github.com/users/khalidmammadov/following{/other_user},https://api.github.com/users/khalidmammadov/gists{/gist_id},https://api.github.com/users/khalidmammadov/starred{/owner}{/repo},https://api.github.com/users/khalidmammadov/subscriptions,https://api.github.com/users/khalidmammadov/orgs,https://api.github.com/users/khalidmammadov/repos,https://api.github.com/users/khalidmammadov/events{/privacy},https://api.github.com/users/khalidmammadov/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35293,https://github.com/apache/spark/pull/35293,https://github.com/apache/spark/pull/35293.diff,https://github.com/apache/spark/pull/35293.patch,,https://api.github.com/repos/apache/spark/issues/35293/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
40,https://api.github.com/repos/apache/spark/issues/35290,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35290/labels{/name},https://api.github.com/repos/apache/spark/issues/35290/comments,https://api.github.com/repos/apache/spark/issues/35290/events,https://github.com/apache/spark/pull/35290,1111880534,PR_kwDOAQXtWs4xdRJC,35290,[SPARK-37865][SQL][3.0]Fix union bug when the first child of union has duplicate columns,[],open,False,,[],,5,2022-01-23T14:04:41Z,2022-01-29T03:45:10Z,,NONE,,False,"This is the backport PR replated to #35168.

### What changes were proposed in this pull request?

When the first child of Union has duplicate columns like select a, a from t1 union select a, b from t2, spark only use the first column to aggregate the results, which would make the results incorrect, and this behavior is inconsistent with other engines like PostgreSQL, MySQL. **We could alias the attribute of the first child of union to resolve this, or you could argue that this is the feature of Spark SQL**.

sample query:
select
a,
a
from values (1, 1), (1, 2) as t1(a, b)
UNION ALL
SELECT
c,
d
from values (2, 3), (2, 3) as t2(c, d)

result is   (1, 1), (1, 1), (3, 3), (3, 3) 
expected (1, 1), (1, 1), (2, 3), (2, 3)

---

select
a,
a
from values (1, 1), (1, 2) as t1(a, b)
UNION
SELECT
c,
d
from values (2, 3), (2, 3) as t2(c, d)

result is  (1, 1), (2, 2)
expected (1, 1), (2, 3)


### Why are the changes needed?

It is possibly a bug.


### Does this PR introduce _any_ user-facing change?
 
Yes. When we union with the first child has duplicate columns, the result would be different.


### How was this patch tested?

Add new UT.
",https://api.github.com/repos/apache/spark/issues/35290/timeline,,spark,apache,chasingegg,18375889,MDQ6VXNlcjE4Mzc1ODg5,https://avatars.githubusercontent.com/u/18375889?v=4,,https://api.github.com/users/chasingegg,https://github.com/chasingegg,https://api.github.com/users/chasingegg/followers,https://api.github.com/users/chasingegg/following{/other_user},https://api.github.com/users/chasingegg/gists{/gist_id},https://api.github.com/users/chasingegg/starred{/owner}{/repo},https://api.github.com/users/chasingegg/subscriptions,https://api.github.com/users/chasingegg/orgs,https://api.github.com/users/chasingegg/repos,https://api.github.com/users/chasingegg/events{/privacy},https://api.github.com/users/chasingegg/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35290,https://github.com/apache/spark/pull/35290,https://github.com/apache/spark/pull/35290.diff,https://github.com/apache/spark/pull/35290.patch,,https://api.github.com/repos/apache/spark/issues/35290/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
41,https://api.github.com/repos/apache/spark/issues/35289,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35289/labels{/name},https://api.github.com/repos/apache/spark/issues/35289/comments,https://api.github.com/repos/apache/spark/issues/35289/events,https://github.com/apache/spark/pull/35289,1111863429,PR_kwDOAQXtWs4xdN_h,35289,[SPARK-37397][PYTHON] Inline annotations for pyspark.ml.base,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-23T12:54:57Z,2022-01-30T12:06:07Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Migration of type annotation for `pyspark.ml.base` from stub file to inline hints.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

As a part of ongoing type hints migration.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests + new data tests.",https://api.github.com/repos/apache/spark/issues/35289/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35289,https://github.com/apache/spark/pull/35289,https://github.com/apache/spark/pull/35289.diff,https://github.com/apache/spark/pull/35289.patch,,https://api.github.com/repos/apache/spark/issues/35289/reactions,1,0,0,0,1,0,0,0,0,,,,,,,,,,,,,,,,,,
42,https://api.github.com/repos/apache/spark/issues/35287,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35287/labels{/name},https://api.github.com/repos/apache/spark/issues/35287/comments,https://api.github.com/repos/apache/spark/issues/35287/events,https://github.com/apache/spark/pull/35287,1111784064,PR_kwDOAQXtWs4xc_Ss,35287,[SPARK-37989][SQL] Push down limit through Aggregate if it is group only,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-23T05:51:57Z,2022-01-26T07:06:31Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr add a new logical plan(`PartialDistinct`) to support partially deduplicate input rows and use it push down limit through Aggregate if it is group only.

For example:
```scala
spark.range(10000L).write.saveAsTable(""t1"")
spark.sql(""SELECT DISTINCT * FROM t1 LIMIT 5"").explain(true)
```

Before this pr:
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- Aggregate [id#3L], [id#3L]
      +- Relation default.t1[id#3L] parquet
```

After this pr:
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- Aggregate [id#3L], [id#3L]
      +- LocalLimit 5
         +- PartialDistinct
            +- Relation default.t1[id#3L] parquet
```

`PartialDistinct` has other use cases.

Use cases | Example query | Benefited queries
-- | -- | --
Partially deduplicate the right side of left semi/anti join | `SELECT  * FROM a WHERE id IN (SELECT id FROM b)` | q10, q14a, q14b, q16, q23a, q23b, q35, q69, q94 , q95
Push down partially deduplicate through join | `SELECT DISTINCT * FROM a JOIN b ON a.id = b.id` | q37, q38, q82, q87


### Why are the changes needed?

Improve query performance by reduce shuffle data.
Before this PR | After this PR
--- | ---
![image](https://user-images.githubusercontent.com/5399861/151117530-40a50695-14d1-405f-8604-4ed067382c6b.png) | ![image](https://user-images.githubusercontent.com/5399861/151117473-d5534ee2-c581-42a1-b863-4c73be62d25f.png)





### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s""CREATE TABLE t1 using parquet AS SELECT id AS a, id % ($numRows div 100) AS b, id % ($numRows div 100000) AS c, id % ($numRows div 100000000) AS d, 1 AS e FROM range(1, ${numRows}L, 1, 5)"")

Seq(""a"", ""b"", ""c"", ""d"", ""e"").foreach { col =>
  val benchmark = new Benchmark(s""Push down limit through Aggregate for column $col"", numRows, minNumIters = 1)
  Seq(10, 500, 5000).foreach { limitNumber =>
    Seq(LimitPushDown.ruleName, """").foreach { execludedRules =>
      benchmark.addCase(s""Query with limit $limitNumber and push down ${if (execludedRules.length > 0) ""disabled"" else ""enabled""}"") { _ =>
        withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
          spark.sql(s""SELECT distinct $col FROM t1 LIMIT $limitNumber"").write.format(""noop"").mode(""Overwrite"").save()
        }
      }
    }
  }
  benchmark.run()
}
```

```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate for column a:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------------
Query with limit 10 and push down disabled             660955         660955           0          1.6         630.3       1.0X
Query with limit 10 and push down enabled              226576         226576           0          4.6         216.1       2.9X
Query with limit 500 and push down disabled            607825         607825           0          1.7         579.7       1.1X
Query with limit 500 and push down enabled             226541         226541           0          4.6         216.0       2.9X
Query with limit 5000 and push down disabled           656318         656318           0          1.6         625.9       1.0X
Query with limit 5000 and push down enabled            209951         209951           0          5.0         200.2       3.1X

Push down limit through Aggregate for column b:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------------
Query with limit 10 and push down disabled             488007         488007           0          2.1         465.4       1.0X
Query with limit 10 and push down enabled              185569         185569           0          5.7         177.0       2.6X
Query with limit 500 and push down disabled            448877         448877           0          2.3         428.1       1.1X
Query with limit 500 and push down enabled             176152         176152           0          6.0         168.0       2.8X
Query with limit 5000 and push down disabled           488315         488315           0          2.1         465.7       1.0X
Query with limit 5000 and push down enabled            203667         203667           0          5.1         194.2       2.4X

Push down limit through Aggregate for column c:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------------
Query with limit 10 and push down disabled              29690          29690           0         35.3          28.3       1.0X
Query with limit 10 and push down enabled               28573          28573           0         36.7          27.2       1.0X
Query with limit 500 and push down disabled             32982          32982           0         31.8          31.5       0.9X
Query with limit 500 and push down enabled              27068          27068           0         38.7          25.8       1.1X
Query with limit 5000 and push down disabled            28202          28202           0         37.2          26.9       1.1X
Query with limit 5000 and push down enabled             27972          27972           0         37.5          26.7       1.1X

Push down limit through Aggregate for column d:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------------
Query with limit 10 and push down disabled              17116          17116           0         61.3          16.3       1.0X
Query with limit 10 and push down enabled               18308          18308           0         57.3          17.5       0.9X
Query with limit 500 and push down disabled             16998          16998           0         61.7          16.2       1.0X
Query with limit 500 and push down enabled              18278          18278           0         57.4          17.4       0.9X
Query with limit 5000 and push down disabled            17302          17302           0         60.6          16.5       1.0X
Query with limit 5000 and push down enabled             17342          17342           0         60.5          16.5       1.0X

Push down limit through Aggregate for column e:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------------
Query with limit 10 and push down disabled              16508          16508           0         63.5          15.7       1.0X
Query with limit 10 and push down enabled               16567          16567           0         63.3          15.8       1.0X
Query with limit 500 and push down disabled             16888          16888           0         62.1          16.1       1.0X
Query with limit 500 and push down enabled              16802          16802           0         62.4          16.0       1.0X
Query with limit 5000 and push down disabled            17139          17139           0         61.2          16.3       1.0X
Query with limit 5000 and push down enabled             16988          16988           0         61.7          16.2       1.0X
```",https://api.github.com/repos/apache/spark/issues/35287/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35287,https://github.com/apache/spark/pull/35287,https://github.com/apache/spark/pull/35287.diff,https://github.com/apache/spark/pull/35287.patch,,https://api.github.com/repos/apache/spark/issues/35287/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
43,https://api.github.com/repos/apache/spark/issues/35278,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35278/labels{/name},https://api.github.com/repos/apache/spark/issues/35278/comments,https://api.github.com/repos/apache/spark/issues/35278/events,https://github.com/apache/spark/pull/35278,1111356772,PR_kwDOAQXtWs4xblaW,35278,[SPARK-37677][CORE] Use the shell command to decompress the ZIP file,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2022-01-22T08:50:52Z,2022-01-29T05:09:46Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When we set parameter ""--archives hdfs:///user/zjx/python3.6.9.zip"" to submit the Spark job, driver will unzip it, but it will lost executable permission after unzipping. So in this PR, I keep the original file permissions.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
When we submit a py job and want to use our own version of Python，we may add ""--archives hdfs:///user/zjx/python3.6.9.zip"" ,after driver unzip this file, and want to execute the program using python3, it will report permission denied. So we should add executable permission to those script.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
After the fix, the zip package submitted by the user for Python will run normally.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
This scheme is already being deployed in production.",https://api.github.com/repos/apache/spark/issues/35278/timeline,,spark,apache,zhongjingxiong,84573424,MDQ6VXNlcjg0NTczNDI0,https://avatars.githubusercontent.com/u/84573424?v=4,,https://api.github.com/users/zhongjingxiong,https://github.com/zhongjingxiong,https://api.github.com/users/zhongjingxiong/followers,https://api.github.com/users/zhongjingxiong/following{/other_user},https://api.github.com/users/zhongjingxiong/gists{/gist_id},https://api.github.com/users/zhongjingxiong/starred{/owner}{/repo},https://api.github.com/users/zhongjingxiong/subscriptions,https://api.github.com/users/zhongjingxiong/orgs,https://api.github.com/users/zhongjingxiong/repos,https://api.github.com/users/zhongjingxiong/events{/privacy},https://api.github.com/users/zhongjingxiong/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35278,https://github.com/apache/spark/pull/35278,https://github.com/apache/spark/pull/35278.diff,https://github.com/apache/spark/pull/35278.patch,,https://api.github.com/repos/apache/spark/issues/35278/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
44,https://api.github.com/repos/apache/spark/issues/35274,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35274/labels{/name},https://api.github.com/repos/apache/spark/issues/35274/comments,https://api.github.com/repos/apache/spark/issues/35274/events,https://github.com/apache/spark/pull/35274,1111143131,PR_kwDOAQXtWs4xa2g_,35274,[SPARK-37982] Use error classes in the execution errors related to unsupported input type,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-22T00:53:26Z,2022-01-26T05:58:35Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Use error classes in the execution errors related to unsupported input type

### Why are the changes needed?
Migrate onto error classes


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing Tests",https://api.github.com/repos/apache/spark/issues/35274/timeline,,spark,apache,leesf,10128888,MDQ6VXNlcjEwMTI4ODg4,https://avatars.githubusercontent.com/u/10128888?v=4,,https://api.github.com/users/leesf,https://github.com/leesf,https://api.github.com/users/leesf/followers,https://api.github.com/users/leesf/following{/other_user},https://api.github.com/users/leesf/gists{/gist_id},https://api.github.com/users/leesf/starred{/owner}{/repo},https://api.github.com/users/leesf/subscriptions,https://api.github.com/users/leesf/orgs,https://api.github.com/users/leesf/repos,https://api.github.com/users/leesf/events{/privacy},https://api.github.com/users/leesf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35274,https://github.com/apache/spark/pull/35274,https://github.com/apache/spark/pull/35274.diff,https://github.com/apache/spark/pull/35274.patch,,https://api.github.com/repos/apache/spark/issues/35274/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
45,https://api.github.com/repos/apache/spark/issues/35270,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35270/labels{/name},https://api.github.com/repos/apache/spark/issues/35270/comments,https://api.github.com/repos/apache/spark/issues/35270/events,https://github.com/apache/spark/pull/35270,1110323152,PR_kwDOAQXtWs4xYF7l,35270,[SPARK-34805][SQL] Propagate metadata from nested columns in Alias,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-21T10:44:19Z,2022-02-01T09:03:22Z,,NONE,,False,"### What changes were proposed in this pull request?
The metadata of a `GetStructField` expression is propagated in the `Alias` expression.


### Why are the changes needed?
Currently, in a dataframe with nested structs, when selecting an inner struct, the metadata of that inner struct is lost. For example, suppose
`df.schema.head.dataType.head.metadata`
returns a non-empty Metadata object, then
`df.select('Field0.SubField0').schema.head.metadata`
returns an empty Metadata object

The following snippet demonstrates the issue
```
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{LongType, MetadataBuilder, StructField, StructType}
val metadataAbc = new MetadataBuilder().putString(""my-metadata"", ""abc"").build()
val metadataXyz = new MetadataBuilder().putString(""my-metadata"", ""xyz"").build()
val schema = StructType(Seq(
  StructField(""abc"",
    StructType(Seq(
      StructField(""xyz"", LongType, nullable = true, metadataXyz)
    )), metadata = metadataAbc)))
import scala.collection.JavaConverters._
val data = Seq(Row(Row(1L))).asJava
val df = spark.createDataFrame(data, schema)

println(df.select(""abc"").schema.head.metadata) // OK, metadata is {""my-metadata"":""abc""}
println(df.select(""abc.xyz"").schema.head.metadata) // NOT OK, metadata is {}, expected {""my-metadata"",""xyz""}
```

The issue can be reproduced in versions 3.2.0, 3.1.2 and 2.4.8

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added a new test
",https://api.github.com/repos/apache/spark/issues/35270/timeline,,spark,apache,kevinwallimann,54136196,MDQ6VXNlcjU0MTM2MTk2,https://avatars.githubusercontent.com/u/54136196?v=4,,https://api.github.com/users/kevinwallimann,https://github.com/kevinwallimann,https://api.github.com/users/kevinwallimann/followers,https://api.github.com/users/kevinwallimann/following{/other_user},https://api.github.com/users/kevinwallimann/gists{/gist_id},https://api.github.com/users/kevinwallimann/starred{/owner}{/repo},https://api.github.com/users/kevinwallimann/subscriptions,https://api.github.com/users/kevinwallimann/orgs,https://api.github.com/users/kevinwallimann/repos,https://api.github.com/users/kevinwallimann/events{/privacy},https://api.github.com/users/kevinwallimann/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35270,https://github.com/apache/spark/pull/35270,https://github.com/apache/spark/pull/35270.diff,https://github.com/apache/spark/pull/35270.patch,,https://api.github.com/repos/apache/spark/issues/35270/reactions,3,3,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
46,https://api.github.com/repos/apache/spark/issues/35269,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35269/labels{/name},https://api.github.com/repos/apache/spark/issues/35269/comments,https://api.github.com/repos/apache/spark/issues/35269/events,https://github.com/apache/spark/pull/35269,1110201775,PR_kwDOAQXtWs4xXssU,35269,[SPARK-28516][SQL] Data Type Formatting Functions: `to_char`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-21T08:43:08Z,2022-01-24T12:19:12Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Many database support the function `to_char` to convert a number to string and `to_number` to convert a string to number. 
https://github.com/apache/spark/pull/35060 supported `to_number`.
The implement of `to_char` has many different between `Postgresql` ,`Oracle` and `Phoenix`.
So, this PR follows the implement of `to_char` in `Oracle` that give a strict parameter verification.
So, this PR follows the implement of `to_char` in `Phoenix` that uses `DecimalFormat`.

This PR support the patterns for numeric formatting as follows:

|Symbol|Meaning|Examples|
|------|-------|--------|
|**9**|Position for a digit; When formatting, it adds a leading blank space or trailing 0.|9999|
|**0**|Position for a digit; When formatting, it adds leading/trailing 0.|0000|
|**.**|Decimal point (only allowed once)|99.99|
|**D**|Decimal point, same as **.** (only allowed once)|99D99|
|**,**|Group (thousands) separator|9,999|
|**G**|Group (thousands) separator, same as **,**|9G999|
|**-**|Sign anchored to number (only allowed once)|-9999|
|**S**|Sign anchored to number, same as **-** (only allowed once)|S9999|
|**$**|Returns value with a leading/trailing dollar sign|$9999|

**PostgreSQL:**
https://www.postgresql.org/docs/12/functions-formatting.html

**Oracle:**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/TO_CHAR-number.html#GUID-00DA076D-2468-41AB-A3AC-CC78DBA0D9CB

**Vertica**
https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Formatting/TO_CHAR.htm?tocpath=SQL%20Reference%20Manual%7CSQL%20Functions%7CFormatting%20Functions%7C_____2

**Redshift**
https://docs.aws.amazon.com/redshift/latest/dg/r_TO_CHAR.html

**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/4xbLyOA_385QLYctkj~hjw

**Snowflake:**
https://docs.snowflake.com/en/sql-reference/functions/to_char.html

**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/to_char%20(number).htm

**Phoenix**
http://phoenix.incubator.apache.org/language/functions.html#to_char

**Intersystems**
https://docs.intersystems.com/latest/csp/docbook/DocBook.UI.Page.cls?KEY=RSQL_tochar

The syntax like:), 
> select to_char(12454367, '00,000,000');
12,454,367


### Why are the changes needed?
`to_char` is very useful for formatted currency to number conversion.


### Does this PR introduce any user-facing change?
Yes. New feature.


### How was this patch tested?
New tests
",https://api.github.com/repos/apache/spark/issues/35269/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35269,https://github.com/apache/spark/pull/35269,https://github.com/apache/spark/pull/35269.diff,https://github.com/apache/spark/pull/35269.patch,,https://api.github.com/repos/apache/spark/issues/35269/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
47,https://api.github.com/repos/apache/spark/issues/35265,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35265/labels{/name},https://api.github.com/repos/apache/spark/issues/35265/comments,https://api.github.com/repos/apache/spark/issues/35265/events,https://github.com/apache/spark/pull/35265,1110033571,PR_kwDOAQXtWs4xXKnW,35265,[WIP][SPARK-37888][SQL][TESTS] Unify v1 and v2 DESCRIBE TABLE tests,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-21T04:13:59Z,2022-01-26T05:33:26Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

1. Move `DESCRIBE TABLE` parsing tests to `DescribeRelationParserSuite`.
2. Put common `DESCRIBE TABLE` tests into one trait `org.apache.spark.sql.execution.command.DescribeTableSuiteBase`, and put datasource specific tests to the `v1.DescribeTableSuite` and `v2.DescribeTableSuite`.

The changes follow the approach of #30287.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
1. The unification will allow to run common `DESCRIBE TABLE` tests for both DSv1/Hive DSv1 and DSv2
2. We can detect missing features and differences between DSv1 and DSv2 implementations.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing unit tests and new tests.",https://api.github.com/repos/apache/spark/issues/35265/timeline,,spark,apache,imback82,12103644,MDQ6VXNlcjEyMTAzNjQ0,https://avatars.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35265,https://github.com/apache/spark/pull/35265,https://github.com/apache/spark/pull/35265.diff,https://github.com/apache/spark/pull/35265.patch,,https://api.github.com/repos/apache/spark/issues/35265/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
48,https://api.github.com/repos/apache/spark/issues/35262,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35262/labels{/name},https://api.github.com/repos/apache/spark/issues/35262/comments,https://api.github.com/repos/apache/spark/issues/35262/events,https://github.com/apache/spark/pull/35262,1109651893,PR_kwDOAQXtWs4xV6y9,35262,[SPARK-37974][SQL] Implement vectorized DELTA_BYTE_ARRAY and DELTA_LENGTH_BYTE_ARRAY encodings for Parquet V2 support,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2022-01-20T18:50:42Z,2022-01-31T21:30:51Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR provides a vectorized implementation of the DELTA_BYTE_ARRAY encoding of Parquet V2. The PR also implements the DELTA_LENGTH_BYTE_ARRAY encoding which is needed by the former.

### Why are the changes needed?
The current support for Parquet V2 in the vectorized reader uses a non-vectorized version of the above encoding and needs to be vectorized.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Reproduces all the tests for the encodings from the Parquet implementation. Also adds more cases to the Parquet Encoding test suite.",https://api.github.com/repos/apache/spark/issues/35262/timeline,,spark,apache,parthchandra,6529136,MDQ6VXNlcjY1MjkxMzY=,https://avatars.githubusercontent.com/u/6529136?v=4,,https://api.github.com/users/parthchandra,https://github.com/parthchandra,https://api.github.com/users/parthchandra/followers,https://api.github.com/users/parthchandra/following{/other_user},https://api.github.com/users/parthchandra/gists{/gist_id},https://api.github.com/users/parthchandra/starred{/owner}{/repo},https://api.github.com/users/parthchandra/subscriptions,https://api.github.com/users/parthchandra/orgs,https://api.github.com/users/parthchandra/repos,https://api.github.com/users/parthchandra/events{/privacy},https://api.github.com/users/parthchandra/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35262,https://github.com/apache/spark/pull/35262,https://github.com/apache/spark/pull/35262.diff,https://github.com/apache/spark/pull/35262.patch,,https://api.github.com/repos/apache/spark/issues/35262/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
49,https://api.github.com/repos/apache/spark/issues/35259,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35259/labels{/name},https://api.github.com/repos/apache/spark/issues/35259/comments,https://api.github.com/repos/apache/spark/issues/35259/events,https://github.com/apache/spark/pull/35259,1109256995,PR_kwDOAQXtWs4xUo0l,35259,[SPARK-37970][SS] Introduce AcceptsLatestSeenOffset to indicate latest seen offset to streaming source,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-20T12:43:31Z,2022-01-20T12:49:33Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR introduces a new interface on streaming data source `AcceptsLatestSeenOffset`, which notifies Spark to provide latest seen offset to the sources implementing the interface at every restart of the query. Spark will provide the latest seen offset before fetching the offset or data.

Worth noting that the interface only support DSv2 streaming sources; the usage of DSv1 streaming source is limited to internal and it has different method call flow, so we would like to focus on DSv2. Spark will throw error if the DSv1 streaming source implements the interface.

### Why are the changes needed?

This could be useful for the source if source needs to prepare based on the latest seen offset before fetching anything. More specifically, we found this very useful and handy for the data source which needs to track the offset by itself, since the external storage does not provide the offset for the latest available data.

### Does this PR introduce _any_ user-facing change?

No, the change is limited to the data source developers.

### How was this patch tested?

New unit tests.",https://api.github.com/repos/apache/spark/issues/35259/timeline,,spark,apache,HeartSaVioR,1317309,MDQ6VXNlcjEzMTczMDk=,https://avatars.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35259,https://github.com/apache/spark/pull/35259,https://github.com/apache/spark/pull/35259.diff,https://github.com/apache/spark/pull/35259.patch,,https://api.github.com/repos/apache/spark/issues/35259/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
50,https://api.github.com/repos/apache/spark/issues/35258,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35258/labels{/name},https://api.github.com/repos/apache/spark/issues/35258/comments,https://api.github.com/repos/apache/spark/issues/35258/events,https://github.com/apache/spark/pull/35258,1109114185,PR_kwDOAQXtWs4xUK9k,35258,[SPARK-37969][SQL] HiveFileFormat should check field name,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-20T10:17:59Z,2022-01-25T06:31:30Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
When write ORC, Spark side check passed, but when initial OutputWriter, it failed.
```
[info]   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (10.12.188.15 executor driver): java.lang.IllegalArgumentException: Error: : expected at the position 19 of 'struct<ID:bigint,IF(ID=1,ID,0):bigint,B:bigint>' but '(' is found.
[info] 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:384)
[info] 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:355)
[info] 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:507)
[info] 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:329)
[info] 	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(TypeInfoUtils.java:814)
[info] 	at org.apache.hadoop.hive.ql.io.orc.OrcSerde.initialize(OrcSerde.java:112)
[info] 	at org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:122)
[info] 	at org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:105)
[info] 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[info] 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[info] 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:313)
[info] 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:252)
[info] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[info] 	at org.apache.spark.scheduler.Task.run(Task.scala:136)
[info] 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)
[info] 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1475)
[info] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)
[info] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info] 	at java.lang.Thread.run(Thread.java:748)
[info]
```

Error for parquet 
```


[info]   Cause: java.lang.IllegalArgumentException: field ended by ';': expected ';' but got 'IF' at line 2:   optional int32 (IF
[info]   at org.apache.parquet.schema.MessageTypeParser.check(MessageTypeParser.java:239)
[info]   at org.apache.parquet.schema.MessageTypeParser.addPrimitiveType(MessageTypeParser.java:208)
[info]   at org.apache.parquet.schema.MessageTypeParser.addType(MessageTypeParser.java:113)
[info]   at org.apache.parquet.schema.MessageTypeParser.addGroupTypeFields(MessageTypeParser.java:101)
[info]   at org.apache.parquet.schema.MessageTypeParser.parse(MessageTypeParser.java:94)
[info]   at org.apache.parquet.schema.MessageTypeParser.parseMessageType(MessageTypeParser.java:84)
[info]   at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.getSchema(DataWritableWriteSupport.java:43)
[info]   at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.init(DataWritableWriteSupport.java:48)
[info]   at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:476)
[info]   at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:430)
[info]   at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:425)
[info]   at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.<init>(ParquetRecordWriterWrapper.java:70)
[info]   at org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getParquerRecordWriterWrapper(MapredParquetOutputFormat.java:137)
[info]   at org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat.getHiveRecordWriter(MapredParquetOutputFormat.java:126)
[info]   at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:286)
[info]   at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:271)
[info]   at org.apache.spark.sql.hive.execution.HiveOutputWriter.<init>(HiveFileFormat.scala:132)
[info]   at org.apache.spark.sql.hive.execution.HiveFileFormat$$anon$1.newInstance(HiveFileFormat.scala:105)
[info]   at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
[info]   at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
[info]   at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:313)
[info]   at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:252)
[info]   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[info]   at org.apache.spark.scheduler.Task.run(Task.scala:136)
[info]   at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)
[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1475)
[info]   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
```

We should make this type error failed earlier.

### Why are the changes needed?
Failed earlier, avoid wast computation.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
WIP",https://api.github.com/repos/apache/spark/issues/35258/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35258,https://github.com/apache/spark/pull/35258,https://github.com/apache/spark/pull/35258.diff,https://github.com/apache/spark/pull/35258.patch,,https://api.github.com/repos/apache/spark/issues/35258/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
51,https://api.github.com/repos/apache/spark/issues/35256,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35256/labels{/name},https://api.github.com/repos/apache/spark/issues/35256/comments,https://api.github.com/repos/apache/spark/issues/35256/events,https://github.com/apache/spark/pull/35256,1108987102,PR_kwDOAQXtWs4xTwmJ,35256,[SPARK-37933][SQL] Limit push down for parquet vectorized reader,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2022-01-20T08:09:56Z,2022-01-24T11:54:14Z,,CONTRIBUTOR,,False,"### Why are the changes needed?
Based on [34291](https://github.com/apache/spark/pull/34291), we can support limit push down to parquet datasource v2 reader, which can stop scanning parquet early, and reduce network and disk IO.
Currently, only vectorized reader is supported in this pr. Row based reader with limit pushdown needs to be supported in parquet-hadoop first, thus it will be supported in the next pr.

Limit parse status for parquet
before
```
== Physical Plan ==
CollectLimit 10
+- *(1) ColumnarToRow
   +- BatchScan[a#0, b#1] ParquetScan DataFilters: [], Format: parquet, Location: InMemoryFileIndex(1 paths)[file:/datasources.db/test_push_down/par..., PartitionFilters: [], PushedAggregation: [], PushedFilters: [], PushedGroupBy: [], ReadSchema: struct<a:int,b:int>, PushedFilters: [], PushedAggregation: [], PushedGroupBy: [] RuntimeFilters: [] 
```
After
```
== Physical Plan ==
CollectLimit 10
+- *(1) ColumnarToRow
   +- BatchScan[a#0, b#1] ParquetScan DataFilters: [], Format: parquet, Location: InMemoryFileIndex(1 paths)[file:/datasources.db/test_push_down/par..., PartitionFilters: [], PushedAggregation: [], PushedFilters: [], PushedGroupBy: [], ReadSchema: struct<a:int,b:int>, PushedFilters: [], PushedAggregation: [], PushedGroupBy: [], PushedLimit: Some(10) RuntimeFilters: [] 
```

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
origin tests and new tests",https://api.github.com/repos/apache/spark/issues/35256/timeline,,spark,apache,stczwd,10897625,MDQ6VXNlcjEwODk3NjI1,https://avatars.githubusercontent.com/u/10897625?v=4,,https://api.github.com/users/stczwd,https://github.com/stczwd,https://api.github.com/users/stczwd/followers,https://api.github.com/users/stczwd/following{/other_user},https://api.github.com/users/stczwd/gists{/gist_id},https://api.github.com/users/stczwd/starred{/owner}{/repo},https://api.github.com/users/stczwd/subscriptions,https://api.github.com/users/stczwd/orgs,https://api.github.com/users/stczwd/repos,https://api.github.com/users/stczwd/events{/privacy},https://api.github.com/users/stczwd/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35256,https://github.com/apache/spark/pull/35256,https://github.com/apache/spark/pull/35256.diff,https://github.com/apache/spark/pull/35256.patch,,https://api.github.com/repos/apache/spark/issues/35256/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
52,https://api.github.com/repos/apache/spark/issues/35254,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35254/labels{/name},https://api.github.com/repos/apache/spark/issues/35254/comments,https://api.github.com/repos/apache/spark/issues/35254/events,https://github.com/apache/spark/pull/35254,1108807717,PR_kwDOAQXtWs4xTLsu,35254,[SPARK-37966][SQL] Static partition insert should write _SUCCESS under partition location,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-20T03:29:16Z,2022-01-22T06:48:00Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, when we use static partition insert to write a partition with data source API, the `_SUCCESS` flag file is write under table location, it should be write under partition's location.


### Why are the changes needed?
leave `_SUCCESS` file under table path may cause auth problem. Also cause some jobs using `_SUCCESS` to check write partition finished  hard to work.


### Does this PR introduce _any_ user-facing change?
Static partition insert write `_SUCCESS` under partition location

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/35254/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35254,https://github.com/apache/spark/pull/35254,https://github.com/apache/spark/pull/35254.diff,https://github.com/apache/spark/pull/35254.patch,,https://api.github.com/repos/apache/spark/issues/35254/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
53,https://api.github.com/repos/apache/spark/issues/35252,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35252/labels{/name},https://api.github.com/repos/apache/spark/issues/35252/comments,https://api.github.com/repos/apache/spark/issues/35252/events,https://github.com/apache/spark/pull/35252,1108518801,PR_kwDOAQXtWs4xSPwU,35252,[SPARK-37154][PYTHON] Inline hints for pyspark.rdd,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-19T20:20:11Z,2022-01-28T12:32:49Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR proposes migration of type hints for `pyspark.rdd` from stub file to inline annotation.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

As a part of ongoing process of migration of stubs to inline hints.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests + new data tests.",https://api.github.com/repos/apache/spark/issues/35252/timeline,,spark,apache,zero323,1554276,MDQ6VXNlcjE1NTQyNzY=,https://avatars.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35252,https://github.com/apache/spark/pull/35252,https://github.com/apache/spark/pull/35252.diff,https://github.com/apache/spark/pull/35252.patch,,https://api.github.com/repos/apache/spark/issues/35252/reactions,1,0,0,0,1,0,0,0,0,,,,,,,,,,,,,,,,,,
54,https://api.github.com/repos/apache/spark/issues/35250,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35250/labels{/name},https://api.github.com/repos/apache/spark/issues/35250/comments,https://api.github.com/repos/apache/spark/issues/35250/events,https://github.com/apache/spark/pull/35250,1107833123,PR_kwDOAQXtWs4xP_Um,35250,[SPARK-37961][SQL] Override maxRows/maxRowsPerPartition for some logical operators,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-19T09:04:58Z,2022-01-20T00:17:20Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?


1, override `maxRowsPerPartition` in `Sort`,`Expand`,`Sample`,`CollectMetrics`;
2, override `maxRows` in `Expand`,`CollectMetrics`;

### Why are the changes needed?

1, provide an accurate value if possible
2, `SampleExec` with replace may generate more rows than child node;

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
added testsuites",https://api.github.com/repos/apache/spark/issues/35250/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35250,https://github.com/apache/spark/pull/35250,https://github.com/apache/spark/pull/35250.diff,https://github.com/apache/spark/pull/35250.patch,,https://api.github.com/repos/apache/spark/issues/35250/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
55,https://api.github.com/repos/apache/spark/issues/35248,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35248/labels{/name},https://api.github.com/repos/apache/spark/issues/35248/comments,https://api.github.com/repos/apache/spark/issues/35248/events,https://github.com/apache/spark/pull/35248,1107828490,PR_kwDOAQXtWs4xP-XQ,35248,[SPARK-37960][SQL] Support aggregate push down with `CASE ... WHEN ... ELSE ... END`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-19T09:00:16Z,2022-01-30T01:54:31Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark supports aggregate push down (e.g. `SUM(column)`) into JDBC data source.
`SUM(CASE ... WHEN ... ELSE ... END)` is very useful for users.
case when could help users to finish many works.


### Why are the changes needed?
Support aggregate push down with `CASE ... WHEN ... ELSE ... END`.


### Does this PR introduce _any_ user-facing change?
Yes. Users could use `CASE ... WHEN ... ELSE ... END` with aggregate push down.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/35248/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35248,https://github.com/apache/spark/pull/35248,https://github.com/apache/spark/pull/35248.diff,https://github.com/apache/spark/pull/35248.patch,,https://api.github.com/repos/apache/spark/issues/35248/reactions,1,0,0,0,1,0,0,0,0,,,,,,,,,,,,,,,,,,
56,https://api.github.com/repos/apache/spark/issues/35244,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35244/labels{/name},https://api.github.com/repos/apache/spark/issues/35244/comments,https://api.github.com/repos/apache/spark/issues/35244/events,https://github.com/apache/spark/pull/35244,1107459174,PR_kwDOAQXtWs4xOyV_,35244,[SPARK-37956][DOCS] Add Python and Java examples of Parquet encryption in Spark SQL to documentation ,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-18T23:20:15Z,2022-01-19T16:12:50Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add Java and Python examples based on the Scala example for the use of Parquet encryption in Spark.


### Why are the changes needed?
To provide information on how to use Parquet column encryption in Spark SQL in additional languages: in Python and Java, based on the Scala example. 


### Does this PR introduce _any_ user-facing change?
Yes, it adds Parquet encryption usage examples in Python and Java to the documentation, which currently provides only a scala example.


### How was this patch tested?
SKIP_API=1 bundle exec jekyll build

Python tab:
![image](https://user-images.githubusercontent.com/63074550/150082994-23649e12-73e8-46c3-bf40-05c1d8bb9831.png)


Java tab:
![image](https://user-images.githubusercontent.com/63074550/150082824-63b657dd-9cfa-4cb8-9368-23ee11f5ec2d.png)

",https://api.github.com/repos/apache/spark/issues/35244/timeline,,spark,apache,andersonm-ibm,63074550,MDQ6VXNlcjYzMDc0NTUw,https://avatars.githubusercontent.com/u/63074550?v=4,,https://api.github.com/users/andersonm-ibm,https://github.com/andersonm-ibm,https://api.github.com/users/andersonm-ibm/followers,https://api.github.com/users/andersonm-ibm/following{/other_user},https://api.github.com/users/andersonm-ibm/gists{/gist_id},https://api.github.com/users/andersonm-ibm/starred{/owner}{/repo},https://api.github.com/users/andersonm-ibm/subscriptions,https://api.github.com/users/andersonm-ibm/orgs,https://api.github.com/users/andersonm-ibm/repos,https://api.github.com/users/andersonm-ibm/events{/privacy},https://api.github.com/users/andersonm-ibm/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35244,https://github.com/apache/spark/pull/35244,https://github.com/apache/spark/pull/35244.diff,https://github.com/apache/spark/pull/35244.patch,,https://api.github.com/repos/apache/spark/issues/35244/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
57,https://api.github.com/repos/apache/spark/issues/35241,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35241/labels{/name},https://api.github.com/repos/apache/spark/issues/35241/comments,https://api.github.com/repos/apache/spark/issues/35241/events,https://github.com/apache/spark/pull/35241,1106704969,PR_kwDOAQXtWs4xMUMD,35241,[SPARK-37953][SQL] Improve the implement of `UnevaluableAggregate`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-18T10:04:06Z,2022-01-20T06:07:28Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark provides `UnevaluableAggregate` to replace function with another function which must be exists in build-in functions, so `UnevaluableAggregate` make Spark could reuse the implement of build-in function.
However, the sub classes of `UnevaluableAggregate` looks cumbersome.


### Why are the changes needed?
Improve the implement of `UnevaluableAggregate`.


### Does this PR introduce _any_ user-facing change?
Yes. Users will see the.change of `UnevaluableAggregate`.


### How was this patch tested?
Exists test.
",https://api.github.com/repos/apache/spark/issues/35241/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35241,https://github.com/apache/spark/pull/35241,https://github.com/apache/spark/pull/35241.diff,https://github.com/apache/spark/pull/35241.patch,,https://api.github.com/repos/apache/spark/issues/35241/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
58,https://api.github.com/repos/apache/spark/issues/35240,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35240/labels{/name},https://api.github.com/repos/apache/spark/issues/35240/comments,https://api.github.com/repos/apache/spark/issues/35240/events,https://github.com/apache/spark/pull/35240,1106667207,PR_kwDOAQXtWs4xMMT8,35240,[SPARK-37930][PYTHON] Fix DataFrame select subset with duplicated columns,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2022-01-18T09:26:54Z,2022-01-21T06:02:11Z,,CONTRIBUTOR,,True,"### What changes were proposed in this pull request?
Fix select subset with duplicated columns of ps.DataFrame

### Why are the changes needed?
Currently, when select subset with duplicated columns of ps.DataFrame we will face an exception 
```python-traceback
ValueError: Length mismatch: Expected axis has 4 elements, new values have 2 elements
```
We should fix it and follow the pandas's behavior
### Does this PR introduce _any_ user-facing change?
Yes,

Before this PR
```python
>>> psdf
   a
0  1
1  2
2  3
3  4
>>> psdf[['a', 'a']]
Traceback (most recent call last):
  ...
  File ""/u02/venv3.9-2/lib/python3.9/site-packages/pandas/core/internals/base.py"", line 57, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4 elements, new values have 2 elements

>>> psdf.loc[:, ['a', 'a']]
Traceback (most recent call last):
  ...
  File ""/u02/venv3.9-2/lib/python3.9/site-packages/pandas/core/internals/base.py"", line 57, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4 elements, new values have 2 elements 
```

After this PR
```python
>>> psdf[['a', 'a']]
   a  a
0  1  1
1  2  2
2  3  3
3  4  4

>>> psdf.loc[:, ['a', 'a']]
   a  a
0  1  1
1  2  2
2  3  3
3  4  4

```

### How was this patch tested?
unit test
",https://api.github.com/repos/apache/spark/issues/35240/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35240,https://github.com/apache/spark/pull/35240,https://github.com/apache/spark/pull/35240.diff,https://github.com/apache/spark/pull/35240.patch,,https://api.github.com/repos/apache/spark/issues/35240/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
59,https://api.github.com/repos/apache/spark/issues/35239,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35239/labels{/name},https://api.github.com/repos/apache/spark/issues/35239/comments,https://api.github.com/repos/apache/spark/issues/35239/events,https://github.com/apache/spark/pull/35239,1106631383,PR_kwDOAQXtWs4xME5V,35239,[SPARK-37952][DOCS] Add missing statements to ALTER TABLE document,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2022-01-18T08:50:58Z,2022-01-31T08:01:26Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add some missing statements to the ALTER TABLE document (which are mainly supported with v2 table).

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To let users know those statements and how to use them.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, docs changed.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.

If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
`SKIP_API=1 bundle exec jekyll build`
![Screenshot from 2022-01-18 17-10-11](https://user-images.githubusercontent.com/87687356/149903242-357b2377-edd9-40c0-9ac4-feec8b25cbb3.png)
![Screenshot from 2022-01-18 17-10-39](https://user-images.githubusercontent.com/87687356/149903304-89ff0dc4-4abc-4f4e-b659-65d8eac32cc7.png)
![Screenshot from 2022-01-18 17-11-09](https://user-images.githubusercontent.com/87687356/149903324-f5998189-0647-4937-89f9-1c8bc69048d4.png)
",https://api.github.com/repos/apache/spark/issues/35239/timeline,,spark,apache,yutoacts,87687356,MDQ6VXNlcjg3Njg3MzU2,https://avatars.githubusercontent.com/u/87687356?v=4,,https://api.github.com/users/yutoacts,https://github.com/yutoacts,https://api.github.com/users/yutoacts/followers,https://api.github.com/users/yutoacts/following{/other_user},https://api.github.com/users/yutoacts/gists{/gist_id},https://api.github.com/users/yutoacts/starred{/owner}{/repo},https://api.github.com/users/yutoacts/subscriptions,https://api.github.com/users/yutoacts/orgs,https://api.github.com/users/yutoacts/repos,https://api.github.com/users/yutoacts/events{/privacy},https://api.github.com/users/yutoacts/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35239,https://github.com/apache/spark/pull/35239,https://github.com/apache/spark/pull/35239.diff,https://github.com/apache/spark/pull/35239.patch,,https://api.github.com/repos/apache/spark/issues/35239/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
60,https://api.github.com/repos/apache/spark/issues/35233,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35233/labels{/name},https://api.github.com/repos/apache/spark/issues/35233/comments,https://api.github.com/repos/apache/spark/issues/35233/events,https://github.com/apache/spark/pull/35233,1106289548,PR_kwDOAQXtWs4xK-_2,35233,[SPARK-37290][SQL] - Exponential planning time in case of non-deterministic function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-17T21:57:37Z,2022-01-18T00:44:54Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

When using non-deterministic function, the method getAllValidConstraints can throw an OOM

```
  protected def getAllValidConstraints(projectList: Seq[NamedExpression]): ExpressionSet = {
    var allConstraints = child.constraints
    projectList.foreach {
      case a @ Alias(l: Literal, _) =>
        allConstraints += EqualNullSafe(a.toAttribute, l)
      case a @ Alias(e, _) =>
        // For every alias in `projectList`, replace the reference in constraints by its attribute.
        allConstraints ++= allConstraints.map(_ transform {
          case expr: Expression if expr.semanticEquals(e) =>
            a.toAttribute
        })
        allConstraints += EqualNullSafe(e, a.toAttribute)
      case _ => // Don't change.
    }

    allConstraints
  }
```
In particular, this line `allConstraints ++= allConstraints.map(...)` can generate an exponential number of expressions
This is because non deterministic functions are considered unique in a ExpressionSet
Therefore, the number of non-deterministic expressions double every time we go through this line

We can filter and keep only deterministic expression because
1 - the `semanticEquals` automatically discard non deterministic expressions
2 - this method is only used in one code path, and we keep only determinic expressions
```
lazy val constraints: ExpressionSet = {
    if (conf.constraintPropagationEnabled) {
      validConstraints
        .union(inferAdditionalConstraints(validConstraints))
        .union(constructIsNotNullConstraints(validConstraints, output))
        .filter { c =>
          c.references.nonEmpty && c.references.subsetOf(outputSet) && c.deterministic
        }
    } else {
      ExpressionSet()
    }
  }
```

### Why are the changes needed?
It can lead to an exponential number of expressions and / or OOM

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Local test
",https://api.github.com/repos/apache/spark/issues/35233/timeline,,spark,apache,Stelyus,11004599,MDQ6VXNlcjExMDA0NTk5,https://avatars.githubusercontent.com/u/11004599?v=4,,https://api.github.com/users/Stelyus,https://github.com/Stelyus,https://api.github.com/users/Stelyus/followers,https://api.github.com/users/Stelyus/following{/other_user},https://api.github.com/users/Stelyus/gists{/gist_id},https://api.github.com/users/Stelyus/starred{/owner}{/repo},https://api.github.com/users/Stelyus/subscriptions,https://api.github.com/users/Stelyus/orgs,https://api.github.com/users/Stelyus/repos,https://api.github.com/users/Stelyus/events{/privacy},https://api.github.com/users/Stelyus/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35233,https://github.com/apache/spark/pull/35233,https://github.com/apache/spark/pull/35233.diff,https://github.com/apache/spark/pull/35233.patch,,https://api.github.com/repos/apache/spark/issues/35233/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
61,https://api.github.com/repos/apache/spark/issues/35232,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35232/labels{/name},https://api.github.com/repos/apache/spark/issues/35232/comments,https://api.github.com/repos/apache/spark/issues/35232/events,https://github.com/apache/spark/pull/35232,1106263224,PR_kwDOAQXtWs4xK5jm,35232,[SPARK-37947][SQL] Extract generator from GeneratorOuter expression contained by a Generate operator.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-17T21:12:07Z,2022-01-26T18:26:32Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR updates the ExtractGenerator rule to extract a generator from a GeneratorOuter expression contained by a Generate operator.

### Why are the changes needed?

This works:

```
select * from values 1, 2 lateral view outer explode(array()) as b;
```

But this does not work:

```
select * from values 1, 2 lateral view explode_outer(array()) as b;
```

It produces the error:

```
Error in query: Column 'b' does not exist. Did you mean one of the following? [col1]; line 1 pos 26;
```

This is because the parser directly creates a Generate operator with the (as of yet unresolved) generator function. Later, the ResolveFunctions rule converts the unresolved function to a resolved generator wrapped by a GeneratorOuter expression. Although the ExtractGenerator rule will extract the generator function from a GeneratorOuter, it doesn't work if the GeneratorOuter is an expression in a Generate operator. Because the generator is not extracted, the ResolveGenerator rule fails to match on the Generate operator, and therefore fails to turn the generator's output expressions into attributes.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

- Existing unit tests.
- New unit test.
",https://api.github.com/repos/apache/spark/issues/35232/timeline,,spark,apache,bersprockets,21131848,MDQ6VXNlcjIxMTMxODQ4,https://avatars.githubusercontent.com/u/21131848?v=4,,https://api.github.com/users/bersprockets,https://github.com/bersprockets,https://api.github.com/users/bersprockets/followers,https://api.github.com/users/bersprockets/following{/other_user},https://api.github.com/users/bersprockets/gists{/gist_id},https://api.github.com/users/bersprockets/starred{/owner}{/repo},https://api.github.com/users/bersprockets/subscriptions,https://api.github.com/users/bersprockets/orgs,https://api.github.com/users/bersprockets/repos,https://api.github.com/users/bersprockets/events{/privacy},https://api.github.com/users/bersprockets/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35232,https://github.com/apache/spark/pull/35232,https://github.com/apache/spark/pull/35232.diff,https://github.com/apache/spark/pull/35232.patch,,https://api.github.com/repos/apache/spark/issues/35232/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
62,https://api.github.com/repos/apache/spark/issues/35223,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35223/labels{/name},https://api.github.com/repos/apache/spark/issues/35223/comments,https://api.github.com/repos/apache/spark/issues/35223/events,https://github.com/apache/spark/pull/35223,1105354063,PR_kwDOAQXtWs4xH5Xz,35223,[SPARK-37925][DOC] Update document to mention the workaround for YARN-11053,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2022-01-17T04:20:03Z,2022-02-01T21:35:40Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Update document ""Running multiple versions of the Spark Shuffle Service"" to mention the workaround for YARN-11053

### Why are the changes needed?

User may stuck when they following the current document to deploy multi-versions Spark Shuffle Service on YARN because of [YARN-11053](https://issues.apache.org/jira/browse/YARN-11053)

### Does this PR introduce _any_ user-facing change?

User document changes.

### How was this patch tested?

![image](https://user-images.githubusercontent.com/26535726/152048543-7e5b3296-3767-4ddb-b215-bc1b63baf94b.png)
",https://api.github.com/repos/apache/spark/issues/35223/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35223,https://github.com/apache/spark/pull/35223,https://github.com/apache/spark/pull/35223.diff,https://github.com/apache/spark/pull/35223.patch,,https://api.github.com/repos/apache/spark/issues/35223/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
63,https://api.github.com/repos/apache/spark/issues/35213,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35213/labels{/name},https://api.github.com/repos/apache/spark/issues/35213/comments,https://api.github.com/repos/apache/spark/issues/35213/events,https://github.com/apache/spark/pull/35213,1104334106,PR_kwDOAQXtWs4xEkhP,35213,[SPARK-37914][SQL] Make `RuntimeReplaceable` works for `AggregateFunction`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-15T03:52:27Z,2022-01-18T10:07:37Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark provides `RuntimeReplaceable` to replace function with another function. The last function must be exists in build-in functions, so `RuntimeReplaceable` make Spark could reuse the implement of build-in function.
But `RuntimeReplaceable` not works for aggregate function.


### Why are the changes needed?
Make `RuntimeReplaceable` works for `AggregateFunction`


### Does this PR introduce _any_ user-facing change?
'No'.
This change is for spark developers.


### How was this patch tested?
Exists tests.
",https://api.github.com/repos/apache/spark/issues/35213/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35213,https://github.com/apache/spark/pull/35213,https://github.com/apache/spark/pull/35213.diff,https://github.com/apache/spark/pull/35213.patch,,https://api.github.com/repos/apache/spark/issues/35213/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
64,https://api.github.com/repos/apache/spark/issues/35209,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35209/labels{/name},https://api.github.com/repos/apache/spark/issues/35209/comments,https://api.github.com/repos/apache/spark/issues/35209/events,https://github.com/apache/spark/pull/35209,1103295875,PR_kwDOAQXtWs4xBD86,35209,[SPARK-37908][K8S][TESTS] Refactoring on pod label test in BasicDriver/ExecutorFeatureStepSuite,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-14T09:00:37Z,2022-01-24T14:34:53Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
- Rename DRIVER_LABELS to CUSTOM_DRIVER_LABELS, LABELS  to CUSTOM_EXECUTORS_LABELS, make their names more clear.
- Refactoring on preset labels and add preset labels test.


### Why are the changes needed?
There are two type Pod label in current implementations: [preset label](https://github.com/apache/spark/blob/068d53bd5d89c96bf0cdb05d3ec7f2f023cf3875/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/KubernetesConf.scala#L158-L165) set by spark and custom label set by user, but there are some mix up in testcase, so this PR just fix it.

Also see realted: https://github.com/apache/spark/pull/34646#issuecomment-981371597


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
UT
",https://api.github.com/repos/apache/spark/issues/35209/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35209,https://github.com/apache/spark/pull/35209,https://github.com/apache/spark/pull/35209.diff,https://github.com/apache/spark/pull/35209.patch,,https://api.github.com/repos/apache/spark/issues/35209/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
65,https://api.github.com/repos/apache/spark/issues/35194,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35194/labels{/name},https://api.github.com/repos/apache/spark/issues/35194/comments,https://api.github.com/repos/apache/spark/issues/35194/events,https://github.com/apache/spark/pull/35194,1102019011,PR_kwDOAQXtWs4w8jp3,35194,[SPARK-37899][SQL] EliminateInnerJoin to support convert inner join to left semi join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-13T16:38:32Z,2022-01-14T02:31:06Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Add a new rule(`EliminateInnerJoin`) to support convert inner join to left semi join. It has two advantages:
1. Statistics are more accurate and more `BroadcastHashJoin`s can be planned.
2. We have 2 other rules(`PushDownLeftSemiAntiJoin` and `PushLeftSemiLeftAntiThroughJoin`) to optimize left semi join.

This is a real case:
```scala
sql(""CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(10000000)"")
sql(""CREATE TABLE t2 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(10000000)"")
sql(""CREATE TABLE t3 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(1000)"")

sql(
  """"""
    |SELECT tmp1.*
    |FROM   (SELECT *
    |        FROM   t1
    |        UNION
    |        SELECT *
    |        FROM   t2) tmp1
    |       INNER JOIN (SELECT DISTINCT a,
    |                                   b
    |                   FROM   t3) tmp2
    |               ON tmp1.a = tmp2.a
    |                  AND tmp1.b = tmp2.b 
  """""".stripMargin).explain
```

Before this pr:
```
== Optimized Logical Plan ==
Project [a#12L, b#13L, c#14L]
+- Join Inner, ((a#12L = a#18L) AND (b#13L = b#19L))
   :- Aggregate [a#12L, b#13L, c#14L], [a#12L, b#13L, c#14L]
   :  +- Union false, false
   :     :- Filter (isnotnull(a#12L) AND isnotnull(b#13L))
   :     :  +- Relation default.t1[a#12L,b#13L,c#14L] parquet
   :     +- Filter (isnotnull(a#15L) AND isnotnull(b#16L))
   :        +- Relation default.t2[a#15L,b#16L,c#17L] parquet
   +- Aggregate [a#18L, b#19L], [a#18L, b#19L]
      +- Project [a#18L, b#19L]
         +- Filter (isnotnull(a#18L) AND isnotnull(b#19L))
            +- Relation default.t3[a#18L,b#19L,c#20L] parquet
```

After this pr:
```
Aggregate [a#12L, b#13L, c#14L], [a#12L, b#13L, c#14L]
+- Union false, false
   :- Join LeftSemi, ((a#12L = a#18L) AND (b#13L = b#19L))
   :  :- Filter (isnotnull(a#12L) AND isnotnull(b#13L))
   :  :  +- Relation default.t1[a#12L,b#13L,c#14L] parquet
   :  +- Aggregate [a#18L, b#19L], [a#18L, b#19L]
   :     +- Project [a#18L, b#19L]
   :        +- Filter (isnotnull(a#18L) AND isnotnull(b#19L))
   :           +- Relation default.t3[a#18L,b#19L,c#20L] parquet
   +- Join LeftSemi, ((a#15L = a#18L) AND (b#16L = b#19L))
      :- Filter (isnotnull(a#15L) AND isnotnull(b#16L))
      :  +- Relation default.t2[a#15L,b#16L,c#17L] parquet
      +- Aggregate [a#18L, b#19L], [a#18L, b#19L]
         +- Project [a#18L, b#19L]
            +- Filter (isnotnull(a#18L) AND isnotnull(b#19L))
               +- Relation default.t3[a#18L,b#19L,c#20L] parquet
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and TPC-DS benchmark test.

SQL | Before this PR(Seconds) | After this PR(Seconds)
-- | -- | --
q14a | 174  | 150

",https://api.github.com/repos/apache/spark/issues/35194/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35194,https://github.com/apache/spark/pull/35194,https://github.com/apache/spark/pull/35194.diff,https://github.com/apache/spark/pull/35194.patch,,https://api.github.com/repos/apache/spark/issues/35194/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
66,https://api.github.com/repos/apache/spark/issues/35191,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35191/labels{/name},https://api.github.com/repos/apache/spark/issues/35191/comments,https://api.github.com/repos/apache/spark/issues/35191/events,https://github.com/apache/spark/pull/35191,1101460604,PR_kwDOAQXtWs4w6pBB,35191,[SPARK-37491][PYTHON]Fix Series.asof for unsorted values,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2022-01-13T09:52:46Z,2022-01-31T18:12:34Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Fix Series.asof when values of the series is not sorted

####  Before 
```python
import pandas as pd
from pyspark import pandas as ps
import numpy as np
pser = pd.Series([2, 1, np.nan, 4], index=[10, 20, 30, 40], name=""Koalas"")
psser = ps.from_pandas(pser)
psser.asof([5, 25])
5     NaN
25    2.0
Name: Koalas, dtype: float64

pser = pd.Series([4, np.nan, np.nan, 2], index=[10, 20, 30, 40], name=""Koalas"")
psser = ps.from_pandas(pser)
psser.asof([5, 100])

5      NaN
100    4.0
```

#### After

```python
import pandas as pd
from pyspark import pandas as ps
import numpy as np
pser = pd.Series([2, 1, np.nan, 4], index=[10, 20, 30, 40], name=""Koalas"")
psser = ps.from_pandas(pser)
psser.asof([5, 25])
5     NaN
25    1.0
Name: Koalas, dtype: float64

pser = pd.Series([4, np.nan, np.nan, 2], index=[10, 20, 30, 40], name=""Koalas"")
psser = ps.from_pandas(pser)
psser.asof([5, 100])
5      NaN
100    2.0
```


### Why are the changes needed?
There is a bug in ps.as_of, when the series is not sorted

### Does this PR introduce any user-facing change?
Yes user will be able to see the behavior exactly matching to pandas

### How was this patch tested?
unit tests",https://api.github.com/repos/apache/spark/issues/35191/timeline,,spark,apache,pralabhkumar,16147255,MDQ6VXNlcjE2MTQ3MjU1,https://avatars.githubusercontent.com/u/16147255?v=4,,https://api.github.com/users/pralabhkumar,https://github.com/pralabhkumar,https://api.github.com/users/pralabhkumar/followers,https://api.github.com/users/pralabhkumar/following{/other_user},https://api.github.com/users/pralabhkumar/gists{/gist_id},https://api.github.com/users/pralabhkumar/starred{/owner}{/repo},https://api.github.com/users/pralabhkumar/subscriptions,https://api.github.com/users/pralabhkumar/orgs,https://api.github.com/users/pralabhkumar/repos,https://api.github.com/users/pralabhkumar/events{/privacy},https://api.github.com/users/pralabhkumar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35191,https://github.com/apache/spark/pull/35191,https://github.com/apache/spark/pull/35191.diff,https://github.com/apache/spark/pull/35191.patch,,https://api.github.com/repos/apache/spark/issues/35191/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
67,https://api.github.com/repos/apache/spark/issues/35188,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35188/labels{/name},https://api.github.com/repos/apache/spark/issues/35188/comments,https://api.github.com/repos/apache/spark/issues/35188/events,https://github.com/apache/spark/pull/35188,1101401431,PR_kwDOAQXtWs4w6cVS,35188,[SPARK-37894][SQL] Add trash feature to FileCommitProtocol.deleteWithJob,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-13T09:02:12Z,2022-01-19T11:30:14Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Add trash feature to function `FileCommitProtocol.deleteWithJob`. When enable trash, use `Trash.moveToAppropriateTrash` instead of `fs.delete`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For some reason (e.g.  analyze or roll back data)  we need keep old data to trash.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing tests.",https://api.github.com/repos/apache/spark/issues/35188/timeline,,spark,apache,sleep1661,8802096,MDQ6VXNlcjg4MDIwOTY=,https://avatars.githubusercontent.com/u/8802096?v=4,,https://api.github.com/users/sleep1661,https://github.com/sleep1661,https://api.github.com/users/sleep1661/followers,https://api.github.com/users/sleep1661/following{/other_user},https://api.github.com/users/sleep1661/gists{/gist_id},https://api.github.com/users/sleep1661/starred{/owner}{/repo},https://api.github.com/users/sleep1661/subscriptions,https://api.github.com/users/sleep1661/orgs,https://api.github.com/users/sleep1661/repos,https://api.github.com/users/sleep1661/events{/privacy},https://api.github.com/users/sleep1661/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35188,https://github.com/apache/spark/pull/35188,https://github.com/apache/spark/pull/35188.diff,https://github.com/apache/spark/pull/35188.patch,,https://api.github.com/repos/apache/spark/issues/35188/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
68,https://api.github.com/repos/apache/spark/issues/35185,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35185/labels{/name},https://api.github.com/repos/apache/spark/issues/35185/comments,https://api.github.com/repos/apache/spark/issues/35185/events,https://github.com/apache/spark/pull/35185,1101107049,PR_kwDOAQXtWs4w5cpT,35185,[SPARK-37831][CORE] add task partition id in TaskInfo and Task Metrics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2022-01-13T02:59:52Z,2022-01-26T09:09:42Z,,CONTRIBUTOR,,False,"### Why are the changes needed?
There is no partition id in the current task metrics. It is difficult to track the task metrics of a specific partition or the stage metrics of processing data, especially when the stage was retried.
```
class TaskData private[spark](
    val taskId: Long,
    val index: Int,
    val attempt: Int,
    val launchTime: Date,
    val resultFetchStart: Option[Date],
    @JsonDeserialize(contentAs = classOf[JLong])
    val duration: Option[Long],
    val executorId: String,
    val host: String,
    val status: String,
    val taskLocality: String,
    val speculative: Boolean,
    val accumulatorUpdates: Seq[AccumulableInfo],
    val errorMessage: Option[String] = None,
    val taskMetrics: Option[TaskMetrics] = None,
    val executorLogs: Map[String, String],
    val schedulerDelay: Long,
    val gettingResultTime: Long) 
```

### Does this PR introduce _any_ user-facing change?
no

### How was this patch tested?
add new tests
",https://api.github.com/repos/apache/spark/issues/35185/timeline,,spark,apache,stczwd,10897625,MDQ6VXNlcjEwODk3NjI1,https://avatars.githubusercontent.com/u/10897625?v=4,,https://api.github.com/users/stczwd,https://github.com/stczwd,https://api.github.com/users/stczwd/followers,https://api.github.com/users/stczwd/following{/other_user},https://api.github.com/users/stczwd/gists{/gist_id},https://api.github.com/users/stczwd/starred{/owner}{/repo},https://api.github.com/users/stczwd/subscriptions,https://api.github.com/users/stczwd/orgs,https://api.github.com/users/stczwd/repos,https://api.github.com/users/stczwd/events{/privacy},https://api.github.com/users/stczwd/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35185,https://github.com/apache/spark/pull/35185,https://github.com/apache/spark/pull/35185.diff,https://github.com/apache/spark/pull/35185.patch,,https://api.github.com/repos/apache/spark/issues/35185/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
69,https://api.github.com/repos/apache/spark/issues/35180,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35180/labels{/name},https://api.github.com/repos/apache/spark/issues/35180/comments,https://api.github.com/repos/apache/spark/issues/35180/events,https://github.com/apache/spark/pull/35180,1100420099,PR_kwDOAQXtWs4w3K5E,35180,[SPARK-37881][CORE] Cleanup ShuffleBlockResolver from polluted methods to create a developer API,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2022-01-12T14:39:28Z,2022-01-27T13:00:22Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?

Cleaning up `ShuffleBlockResolver` from polluted methods to create a developer API.

### Why are the changes needed?

`ShuffleBlockResolver` is intended to be part of a generic Shuffle API but currently it contains local disk specific (and pushed based shuffle) methods.


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing unit tests.",https://api.github.com/repos/apache/spark/issues/35180/timeline,,spark,apache,attilapiros,2017933,MDQ6VXNlcjIwMTc5MzM=,https://avatars.githubusercontent.com/u/2017933?v=4,,https://api.github.com/users/attilapiros,https://github.com/attilapiros,https://api.github.com/users/attilapiros/followers,https://api.github.com/users/attilapiros/following{/other_user},https://api.github.com/users/attilapiros/gists{/gist_id},https://api.github.com/users/attilapiros/starred{/owner}{/repo},https://api.github.com/users/attilapiros/subscriptions,https://api.github.com/users/attilapiros/orgs,https://api.github.com/users/attilapiros/repos,https://api.github.com/users/attilapiros/events{/privacy},https://api.github.com/users/attilapiros/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35180,https://github.com/apache/spark/pull/35180,https://github.com/apache/spark/pull/35180.diff,https://github.com/apache/spark/pull/35180.patch,,https://api.github.com/repos/apache/spark/issues/35180/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
70,https://api.github.com/repos/apache/spark/issues/35178,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35178/labels{/name},https://api.github.com/repos/apache/spark/issues/35178/comments,https://api.github.com/repos/apache/spark/issues/35178/events,https://github.com/apache/spark/pull/35178,1100192083,PR_kwDOAQXtWs4w2bE7,35178,[WIP][SPARK-37877][SQL] Support clear shuffle dependencies eagerly for thrift server,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,"[{'login': 'yaooqinn', 'id': 8326978, 'node_id': 'MDQ6VXNlcjgzMjY5Nzg=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8326978?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yaooqinn', 'html_url': 'https://github.com/yaooqinn', 'followers_url': 'https://api.github.com/users/yaooqinn/followers', 'following_url': 'https://api.github.com/users/yaooqinn/following{/other_user}', 'gists_url': 'https://api.github.com/users/yaooqinn/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yaooqinn/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yaooqinn/subscriptions', 'organizations_url': 'https://api.github.com/users/yaooqinn/orgs', 'repos_url': 'https://api.github.com/users/yaooqinn/repos', 'events_url': 'https://api.github.com/users/yaooqinn/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yaooqinn/received_events', 'type': 'User', 'site_admin': False}]",,2,2022-01-12T10:54:17Z,2022-01-14T06:23:44Z,,CONTRIBUTOR,,True,"
<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


In this PR, we add a config to control whether thrift server operations will clear shuffle dependencies eagerly after being executed.

In long-running applications, like thrift server, with large driver JVMs, where there is little memory pressure on the driver, the driver gc may happen very occasionally or not at all. Not cleaning at all may lead to executors running out of disk space after a while.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


For the thrift server, it currently relies on a periodical system.gc to release shuffle meta/data e.t.c, which is not efficient enough towards its usage scenario as the gc does not always occur immediately. Unclean data may cause thrift server memory issues, like OOM, or disk issues on the work side, like no space left for the device.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

added new conf

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

",https://api.github.com/repos/apache/spark/issues/35178/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35178,https://github.com/apache/spark/pull/35178,https://github.com/apache/spark/pull/35178.diff,https://github.com/apache/spark/pull/35178.patch,,https://api.github.com/repos/apache/spark/issues/35178/reactions,0,0,0,0,0,0,0,0,0,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False
71,https://api.github.com/repos/apache/spark/issues/35172,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35172/labels{/name},https://api.github.com/repos/apache/spark/issues/35172/comments,https://api.github.com/repos/apache/spark/issues/35172/events,https://github.com/apache/spark/pull/35172,1099650538,PR_kwDOAQXtWs4w0tEr,35172,[SPARK-36664][CORE] Log time waiting for cluster resources,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-11T21:52:52Z,2022-01-11T22:34:15Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Keep track of and communicate to the listener bus how long we are waiting for execs to be allocated from the underlying cluster manager.

Replaces WIP PR https://github.com/apache/spark/pull/34650 

### Why are the changes needed?

Sometimes the cluster manager may choke or otherwise not be able to allocate resources and we don't have a good way of detecting this situation making it difficult for the user to debug and tell apart from Spark not scaling up correctly.

### Does this PR introduce _any_ user-facing change?

New field in the listener bus message for when a executor is allocated.

### How was this patch tested?

New unit test in the listener suite.",https://api.github.com/repos/apache/spark/issues/35172/timeline,,spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35172,https://github.com/apache/spark/pull/35172,https://github.com/apache/spark/pull/35172.diff,https://github.com/apache/spark/pull/35172.patch,,https://api.github.com/repos/apache/spark/issues/35172/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
72,https://api.github.com/repos/apache/spark/issues/35168,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35168/labels{/name},https://api.github.com/repos/apache/spark/issues/35168/comments,https://api.github.com/repos/apache/spark/issues/35168/events,https://github.com/apache/spark/pull/35168,1099173001,PR_kwDOAQXtWs4wzJn1,35168,[SPARK-37865][SQL]Spark should not dedup the grouping Expressions when the first child of union has duplicate columns,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2022-01-11T13:48:55Z,2022-01-23T14:07:00Z,,NONE,,False,"### What changes were proposed in this pull request?

When the first child of Union has duplicate columns like select a, a from t1 union select a, b from t2, spark only use the first column to aggregate the results, which would make the results incorrect, and this behavior is inconsistent with other engines like PostgreSQL, MySQL. **We could alias the attribute of the first child of union to resolve this, or you could argue that this is the feature of Spark SQL**.

sample query:
select
a,
a
from values (1, 1), (1, 2) as t1(a, b)
UNION ALL
SELECT
c,
d
from values (2, 3), (2, 3) as t2(c, d)

result is   (1, 1), (1, 1), (3, 3), (3, 3) 
expected (1, 1), (1, 1), (2, 3), (2, 3)

---

select
a,
a
from values (1, 1), (1, 2) as t1(a, b)
UNION
SELECT
c,
d
from values (2, 3), (2, 3) as t2(c, d)

result is  (1, 1), (2, 2)
expected (1, 1), (2, 3)


### Why are the changes needed?

It is possibly a bug.


### Does this PR introduce _any_ user-facing change?
 
Yes. When we union with the first child has duplicate columns, the result would be different.


### How was this patch tested?

Add new UT.
",https://api.github.com/repos/apache/spark/issues/35168/timeline,,spark,apache,chasingegg,18375889,MDQ6VXNlcjE4Mzc1ODg5,https://avatars.githubusercontent.com/u/18375889?v=4,,https://api.github.com/users/chasingegg,https://github.com/chasingegg,https://api.github.com/users/chasingegg/followers,https://api.github.com/users/chasingegg/following{/other_user},https://api.github.com/users/chasingegg/gists{/gist_id},https://api.github.com/users/chasingegg/starred{/owner}{/repo},https://api.github.com/users/chasingegg/subscriptions,https://api.github.com/users/chasingegg/orgs,https://api.github.com/users/chasingegg/repos,https://api.github.com/users/chasingegg/events{/privacy},https://api.github.com/users/chasingegg/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35168,https://github.com/apache/spark/pull/35168,https://github.com/apache/spark/pull/35168.diff,https://github.com/apache/spark/pull/35168.patch,,https://api.github.com/repos/apache/spark/issues/35168/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
73,https://api.github.com/repos/apache/spark/issues/35140,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35140/labels{/name},https://api.github.com/repos/apache/spark/issues/35140/comments,https://api.github.com/repos/apache/spark/issues/35140/events,https://github.com/apache/spark/pull/35140,1096637888,PR_kwDOAQXtWs4wrOY0,35140,[SPARK-37829][SQL] DataFrame.joinWith should return null rows for missing values,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2022-01-07T20:15:23Z,2022-01-10T09:15:44Z,,NONE,,False,"### What changes were proposed in this pull request?
Add a unit test demonstrating the regression on `DataFrame.joinWith`.
Revert  [commit cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59) making the test pass.

### Why are the changes needed?
Doing an outer-join using joinWith on DataFrames used to return missing values as null in Spark 2.4.8, but returns them as Rows with null values in Spark 3.0.0+.
The regression has been introduced in [commit cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59).

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
A unit test was added.

Ran unit tests for the `sql-core` and `sql-catalyst` submodules with `./build/mvn clean package -pl sql/core,cql/catalyst`",https://api.github.com/repos/apache/spark/issues/35140/timeline,,spark,apache,cdegroc,1451981,MDQ6VXNlcjE0NTE5ODE=,https://avatars.githubusercontent.com/u/1451981?v=4,,https://api.github.com/users/cdegroc,https://github.com/cdegroc,https://api.github.com/users/cdegroc/followers,https://api.github.com/users/cdegroc/following{/other_user},https://api.github.com/users/cdegroc/gists{/gist_id},https://api.github.com/users/cdegroc/starred{/owner}{/repo},https://api.github.com/users/cdegroc/subscriptions,https://api.github.com/users/cdegroc/orgs,https://api.github.com/users/cdegroc/repos,https://api.github.com/users/cdegroc/events{/privacy},https://api.github.com/users/cdegroc/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35140,https://github.com/apache/spark/pull/35140,https://github.com/apache/spark/pull/35140.diff,https://github.com/apache/spark/pull/35140.patch,,https://api.github.com/repos/apache/spark/issues/35140/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
74,https://api.github.com/repos/apache/spark/issues/35139,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35139/labels{/name},https://api.github.com/repos/apache/spark/issues/35139/comments,https://api.github.com/repos/apache/spark/issues/35139/events,https://github.com/apache/spark/pull/35139,1096580246,PR_kwDOAQXtWs4wrC1P,35139,[SPARK-37829][SQL] DataFrame.joinWith should return null rows for missing values,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-07T18:52:17Z,2022-01-17T17:12:51Z,,NONE,,False,"### What changes were proposed in this pull request?
We add a unit test demonstrating a regression on `DataFrame.joinWith` and fix the regression by updating `ExpressionEncoder`. The fix is equivalent to reverting [this commit](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59).

### Why are the changes needed?
Doing an outer-join using joinWith on DataFrames used to return missing values as null in Spark 2.4.8, but returns them as Rows with null values in Spark 3.0.0+.

The regression has been introduced in [this commit](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59).

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Added a unit test. This unit test succeeds with Spark 2.4.8 but fails with Spark 3.0.0+.

The new unit test does a left outer join on two DataFrames using the `joinWith` method.
The join is performed on the `b` field of `ClassData` (Ints).
The row `ClassData(""a"", 1)` on the left side of the join has no matching row on the right side of the join as there is no row with value `1` for field `b`.
The missing value (of Row type) is represented as a `GenericRowWithSchema(Array(null, null), rightFieldSchema)` instead of a `null` value making the test fail.

This new test is identical to [this one](https://github.com/apache/spark/blob/6061e60ed7691bf3ca0e0964acac4c53c69bcc07/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala#L1269-L1292) and only differs in that it uses DataFrames instead of Datasets.

I've run unit tests for the `sql-core` and `sql-catalyst` submodules locally with `./build/mvn clean package -pl sql/core,cql/catalyst`",https://api.github.com/repos/apache/spark/issues/35139/timeline,,spark,apache,cdegroc,1451981,MDQ6VXNlcjE0NTE5ODE=,https://avatars.githubusercontent.com/u/1451981?v=4,,https://api.github.com/users/cdegroc,https://github.com/cdegroc,https://api.github.com/users/cdegroc/followers,https://api.github.com/users/cdegroc/following{/other_user},https://api.github.com/users/cdegroc/gists{/gist_id},https://api.github.com/users/cdegroc/starred{/owner}{/repo},https://api.github.com/users/cdegroc/subscriptions,https://api.github.com/users/cdegroc/orgs,https://api.github.com/users/cdegroc/repos,https://api.github.com/users/cdegroc/events{/privacy},https://api.github.com/users/cdegroc/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35139,https://github.com/apache/spark/pull/35139,https://github.com/apache/spark/pull/35139.diff,https://github.com/apache/spark/pull/35139.patch,,https://api.github.com/repos/apache/spark/issues/35139/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
75,https://api.github.com/repos/apache/spark/issues/35124,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35124/labels{/name},https://api.github.com/repos/apache/spark/issues/35124/comments,https://api.github.com/repos/apache/spark/issues/35124/events,https://github.com/apache/spark/pull/35124,1095948816,PR_kwDOAQXtWs4wpB6D,35124,[WIP][SPARK-37398][PYTHON] Inline type hints for python/pyspark/ml/classification.py,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2022-01-07T03:32:33Z,2022-01-08T00:25:13Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
- Add inline types hints for ml/classification.py. 
- Umbrella: [SPARK-37395](https://issues.apache.org/jira/browse/SPARK-37395)
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To use static type checking within functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing tests.",https://api.github.com/repos/apache/spark/issues/35124/timeline,,spark,apache,javierivanov,7876890,MDQ6VXNlcjc4NzY4OTA=,https://avatars.githubusercontent.com/u/7876890?v=4,,https://api.github.com/users/javierivanov,https://github.com/javierivanov,https://api.github.com/users/javierivanov/followers,https://api.github.com/users/javierivanov/following{/other_user},https://api.github.com/users/javierivanov/gists{/gist_id},https://api.github.com/users/javierivanov/starred{/owner}{/repo},https://api.github.com/users/javierivanov/subscriptions,https://api.github.com/users/javierivanov/orgs,https://api.github.com/users/javierivanov/repos,https://api.github.com/users/javierivanov/events{/privacy},https://api.github.com/users/javierivanov/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35124,https://github.com/apache/spark/pull/35124,https://github.com/apache/spark/pull/35124.diff,https://github.com/apache/spark/pull/35124.patch,,https://api.github.com/repos/apache/spark/issues/35124/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
76,https://api.github.com/repos/apache/spark/issues/35119,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35119/labels{/name},https://api.github.com/repos/apache/spark/issues/35119/comments,https://api.github.com/repos/apache/spark/issues/35119/events,https://github.com/apache/spark/pull/35119,1095397891,PR_kwDOAQXtWs4wnOPq,35119,DO NOT MERGE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-06T14:56:41Z,2022-01-13T13:40:10Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35119/timeline,,spark,apache,cloud-fan,3182036,MDQ6VXNlcjMxODIwMzY=,https://avatars.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35119,https://github.com/apache/spark/pull/35119,https://github.com/apache/spark/pull/35119.diff,https://github.com/apache/spark/pull/35119.patch,,https://api.github.com/repos/apache/spark/issues/35119/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
77,https://api.github.com/repos/apache/spark/issues/35116,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35116/labels{/name},https://api.github.com/repos/apache/spark/issues/35116/comments,https://api.github.com/repos/apache/spark/issues/35116/events,https://github.com/apache/spark/pull/35116,1095048781,PR_kwDOAQXtWs4wmHCY,35116,[SPARK-37825][SQL][LAUNCHER] Make spark beeline be able to handle javaOpts,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2022-01-06T07:35:19Z,2022-01-14T08:52:03Z,,CONTRIBUTOR,,False,"

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Currently, we build the beeline command with SPARK_DRIVER_MEMORY only and are not able to set extra java opts. Besides, the beeline is not a DRIVER-type thing.

In this PR, we add 

```java
# - SPARK_BEELINE_OPTS, to set config properties only for the beeline cli (e.g. ""-Dx=y"")
# - SPARK_BEELINE_MEMORY, Memory for beeline (e.g. 1000M, 2G) (Default: 1G)
```

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Make spark beeline  configurable for extra JVM settings, like GC 

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, add 2 environment variables.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

add a new java ut",https://api.github.com/repos/apache/spark/issues/35116/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35116,https://github.com/apache/spark/pull/35116,https://github.com/apache/spark/pull/35116.diff,https://github.com/apache/spark/pull/35116.patch,,https://api.github.com/repos/apache/spark/issues/35116/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
78,https://api.github.com/repos/apache/spark/issues/35115,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35115/labels{/name},https://api.github.com/repos/apache/spark/issues/35115/comments,https://api.github.com/repos/apache/spark/issues/35115/events,https://github.com/apache/spark/pull/35115,1095020902,PR_kwDOAQXtWs4wmBYz,35115,"[SPARK-37787][CORE] fix bug, Long running Spark Job throw HDFS_DELEGATE_TOKEN not found in cache Exception","[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2022-01-06T06:47:44Z,2022-01-06T19:56:41Z,,NONE,,False,"…TE_TOKEN not found in cache Exception

### What changes were proposed in this pull request?
This PR is add a judgment. If the renewable delegationTokenProvider returns None, changes the result renewal time  to the current time + delay（sparkconf  spark.security.credentials.retryWait）.

### Why are the changes needed?
HadoopFSDelegationTokenProvider.obtaindelegationtokens returns None when an exception is encountered. HadoopDelegationTokenProvider that cannot be renewed also returns None. If the obtaindelegationtokens of all HadoopDelegationTokenProvider return None,HadoopDelegationTokenManager in obtaindelegationtokens, call foldleft (long. Maxvalue) (math. Min) to return long.MaxValue. Therefore, the log shows ""scheduling renewal in 1921535501304.2 H""
At this time, HadoopFSDelegationToken cannot be renewed in time, resulting in throw ""hdfs_delete_token not found in cache"" Exception

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
For testability, modify the method obtaindelegationtokens  protected and modify the  parameters. A method is added to the HadoopDelegationTokenManagerSuite to test Hadoop delegationtokenmanager.obtaindelegationtokens method.

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35115/timeline,,spark,apache,huangzhir,1221600,MDQ6VXNlcjEyMjE2MDA=,https://avatars.githubusercontent.com/u/1221600?v=4,,https://api.github.com/users/huangzhir,https://github.com/huangzhir,https://api.github.com/users/huangzhir/followers,https://api.github.com/users/huangzhir/following{/other_user},https://api.github.com/users/huangzhir/gists{/gist_id},https://api.github.com/users/huangzhir/starred{/owner}{/repo},https://api.github.com/users/huangzhir/subscriptions,https://api.github.com/users/huangzhir/orgs,https://api.github.com/users/huangzhir/repos,https://api.github.com/users/huangzhir/events{/privacy},https://api.github.com/users/huangzhir/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35115,https://github.com/apache/spark/pull/35115,https://github.com/apache/spark/pull/35115.diff,https://github.com/apache/spark/pull/35115.patch,,https://api.github.com/repos/apache/spark/issues/35115/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
79,https://api.github.com/repos/apache/spark/issues/35094,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35094/labels{/name},https://api.github.com/repos/apache/spark/issues/35094/comments,https://api.github.com/repos/apache/spark/issues/35094/events,https://github.com/apache/spark/pull/35094,1093116780,PR_kwDOAQXtWs4wf2tu,35094,[SPARK-36543][CORE] Rearranged logging in decommission loop,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2022-01-04T08:23:47Z,2022-01-06T05:07:40Z,,NONE,,False,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Lower logging level, and transferred logInfo in a loop to outside

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

[SPARK-36543] Decommission logs too frequent when waiting migration to finish

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, it changes the level of logs, and make it less frequent.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
It doesn't need",https://api.github.com/repos/apache/spark/issues/35094/timeline,,spark,apache,sungpeo,13159599,MDQ6VXNlcjEzMTU5NTk5,https://avatars.githubusercontent.com/u/13159599?v=4,,https://api.github.com/users/sungpeo,https://github.com/sungpeo,https://api.github.com/users/sungpeo/followers,https://api.github.com/users/sungpeo/following{/other_user},https://api.github.com/users/sungpeo/gists{/gist_id},https://api.github.com/users/sungpeo/starred{/owner}{/repo},https://api.github.com/users/sungpeo/subscriptions,https://api.github.com/users/sungpeo/orgs,https://api.github.com/users/sungpeo/repos,https://api.github.com/users/sungpeo/events{/privacy},https://api.github.com/users/sungpeo/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35094,https://github.com/apache/spark/pull/35094,https://github.com/apache/spark/pull/35094.diff,https://github.com/apache/spark/pull/35094.patch,,https://api.github.com/repos/apache/spark/issues/35094/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
80,https://api.github.com/repos/apache/spark/issues/35088,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35088/labels{/name},https://api.github.com/repos/apache/spark/issues/35088/comments,https://api.github.com/repos/apache/spark/issues/35088/events,https://github.com/apache/spark/pull/35088,1092194993,PR_kwDOAQXtWs4wc3fn,35088,[SPARK-37758][PYTHON][BUILD] Enable PySpark test scheduled job on ARM runner,"[{'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,0,2022-01-03T03:24:01Z,2022-01-03T03:24:18Z,,CONTRIBUTOR,,True,"### What changes were proposed in this pull request?
This patch adds the pyspark arm scheduled job on ARM runner.
- Add pyspark-arm64-scheduled job.
- Runs on arm64 self-hosted runner with `ubuntu-20.04-arm64` label
- Using `yikunkero/apache-spark-github-action-image:arm64` in arm test.
- Installed minicoda with `-u` (upgrade if exist otherwise install directly), and also add multi-arch improvement.


### Why are the changes needed?
Migrate PySpark Arm Job from Jenkins to GitHub Actions.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
- Trigger local test based on this patch in: https://github.com/Yikun/spark/pull/53
- CI passed.",https://api.github.com/repos/apache/spark/issues/35088/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35088,https://github.com/apache/spark/pull/35088,https://github.com/apache/spark/pull/35088.diff,https://github.com/apache/spark/pull/35088.patch,,https://api.github.com/repos/apache/spark/issues/35088/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
81,https://api.github.com/repos/apache/spark/issues/35085,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35085/labels{/name},https://api.github.com/repos/apache/spark/issues/35085/comments,https://api.github.com/repos/apache/spark/issues/35085/events,https://github.com/apache/spark/pull/35085,1092029028,PR_kwDOAQXtWs4wcWx3,35085,[SPARK-37618][CORE] Remove shuffle blocks using the shuffle service for released executors,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2022-01-02T14:16:00Z,2022-02-01T16:59:06Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add support for removing shuffle files on released executors via the external shuffle service. The shuffle service already supports removing shuffle service cached RDD blocks, so I reused this mechanism to remove shuffle blocks as well, so as not to require updating the shuffle service itself.

To support this change functioning in a secure Yarn environment, I updated permissions on some of the block manager folders and files. Specifically:
- Block manager sub directories have the group write posix permission added to them. This gives the shuffle service permission to delete files from within these folders.
- Shuffle files have the world readable posix permission added to them. This is because when the sub directories are marked group writable, they lose the setgid bit that gets set in a secure Yarn environment. Without this, the permissions on the files would be `rw-r-----`, and since the group running Yarn (and therefore the shuffle service), is no longer the group owner of the file, it does not have access to read the file. The sub directories still do not have world execute permissions, so there's no security issue opening up these files.

Both of these changes are done after creating a file so that umasks don't affect the resulting permissions.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
External shuffle services are very useful for long running jobs and dynamic allocation. However, currently if an executor is removed (either through dynamic deallocation or through some error), the shuffle files created by that executor will live until the application finishes. This results in local disks slowly filling up over time, eventually causing problems for long running applications. 


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
New unit test. Not sure if there's a better way I could have tested for the files being deleted or any other tests I should add.
",https://api.github.com/repos/apache/spark/issues/35085/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35085,https://github.com/apache/spark/pull/35085,https://github.com/apache/spark/pull/35085.diff,https://github.com/apache/spark/pull/35085.patch,,https://api.github.com/repos/apache/spark/issues/35085/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
82,https://api.github.com/repos/apache/spark/issues/35083,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35083/labels{/name},https://api.github.com/repos/apache/spark/issues/35083/comments,https://api.github.com/repos/apache/spark/issues/35083/events,https://github.com/apache/spark/pull/35083,1091720846,PR_kwDOAQXtWs4wbbEp,35083,[WIP][SPARK-37798] PySpark Pandas API: Cross and conditional merging,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2022-01-01T02:37:33Z,2022-01-09T02:19:23Z,,NONE,,False,"JIRA: https://issues.apache.org/jira/browse/SPARK-37798

Pandas currently supports a `how=""cross""` merge which provides a cartesian product of the left/right tables. This can be achieved by doing a `spark.sql.dataframe.join(..., on=None, how=""inner"")`.

Additionally, I am currently in the middle of adding conditional merging in the pandas API (see PR here: https://github.com/pandas-dev/pandas/pull/42964). This is much easier to achieve in spark, since the functionality is already available, and we can trivially expose it in the pyspark pandas API. Due to the demand  of this functionality (countless SO/pandas issues either asking how to do this, or asking questions that would be solved by this), I think that this would be worth adding even before it makes it into the core pandas API.

These changes will be purely incremental on top of the existing API, and will be completely backwards compatible.

Still need to add tests and docstring examples.


**Examples:**

**Example DFs:**
```
>>> df1 = pd.DataFrame([['Bill', 23], ['Mary', 33], ['Ted', 36]], columns=['name', 'age'])
>>> df2 = pd.DataFrame([['President', 35], ['Senator', 30]], columns=['job', 'min_age'])
>>> df1
   name  age
0  Bill   23
1  Mary   33
2   Ted   36

>>> df2
         job  min_age
0  President       35
1    Senator       30
```

**Cross  Merge Example:**
```
>>> df1.merge(df2, how=""cross"")
   name  age        job  min_age
0  Bill   23  President       35
1  Bill   23    Senator       30
2  Mary   33  President       35
3  Mary   33    Senator       30
4   Ted   36  President       35
5   Ted   36    Senator       30
```

**Conditional Merge Example:**
```
>>> df1.merge(df2, on=lambda left, right: left.age >= right.min_age)
   name  age        job  min_age
0  Mary   33    Senator       30
1   Ted   36  President       35
2   Ted   36    Senator       30
```
",https://api.github.com/repos/apache/spark/issues/35083/timeline,,spark,apache,aa1371,13501084,MDQ6VXNlcjEzNTAxMDg0,https://avatars.githubusercontent.com/u/13501084?v=4,,https://api.github.com/users/aa1371,https://github.com/aa1371,https://api.github.com/users/aa1371/followers,https://api.github.com/users/aa1371/following{/other_user},https://api.github.com/users/aa1371/gists{/gist_id},https://api.github.com/users/aa1371/starred{/owner}{/repo},https://api.github.com/users/aa1371/subscriptions,https://api.github.com/users/aa1371/orgs,https://api.github.com/users/aa1371/repos,https://api.github.com/users/aa1371/events{/privacy},https://api.github.com/users/aa1371/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35083,https://github.com/apache/spark/pull/35083,https://github.com/apache/spark/pull/35083.diff,https://github.com/apache/spark/pull/35083.patch,,https://api.github.com/repos/apache/spark/issues/35083/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
83,https://api.github.com/repos/apache/spark/issues/35082,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35082/labels{/name},https://api.github.com/repos/apache/spark/issues/35082/comments,https://api.github.com/repos/apache/spark/issues/35082/events,https://github.com/apache/spark/pull/35082,1091550722,PR_kwDOAQXtWs4wa4qQ,35082,[SPARK-37677][PYTHON] Decompress the ZIP file and grant the executable permission to the file,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-31T12:18:09Z,2022-01-20T13:39:37Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
`Unpacking an archive s3a://zjx/python3.6.9.zip#python3 from /tmp/spark-d861d788-bd72-4d3c-88fc-8f79b30b081d/python3.6.9.zip to /opt/spark/work-dir/./python3
Exception in thread ""main"" java.io.IOException: Cannot run program ""python3/bin/python3"": error=13, Permission denied
	at java.base/java.lang.ProcessBuilder.start(Unknown Source)
	at java.base/java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
	at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1045)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1054)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: error=13, Permission denied
	at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
	at java.base/java.lang.ProcessImpl.<init>(Unknown Source)
	at java.base/java.lang.ProcessImpl.start(Unknown Source)
	... 16 more`
When we set parameter ""--archives hdfs:///user/zjx/python3.6.9.zip"" to submit the Spark job, driver will unzip it, but it will lost executable permission after unzipping, so we should keep those permission. In this PR, I add the executable permission for all file after unzipping. It may cost some time to unzip that, but the time cost is controllable.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->



### Why are the changes needed?
When we submit a py job and want to use our own version of Python，we may add ""--archives hdfs:///user/zjx/python3.6.9.zip"" ,after driver unzip this file, and want to execute the program using python3, it will report permission denied. So we should add executable permission to those script.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
After the fix, the zip package submitted by the user for Python will run normally
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
First, I tested python3.6.9 and Python3.9. I compiled Python3.6.9 (which contains no other third party dependencies) and Python3.6.9 (which contains many third party dependencies) to include 8398 and 63570 files respectively and compressed them into a ZIP file, The zip file upload is then specified in spark-Submit, and the execution permission process is logged to record the time spent. The conclusion is that python3.9.zip took 50 to 65 milliseconds to find permissions, and the other one took 600 to 800 milliseconds.Then upload and test python3.6.9.zip and python3.6.9.tgz 100 times and find that the average unzip time of ZIP file is 15637.45 milliseconds, while the unzip time of TGZ file is 15758.78 milliseconds。That is to say, counting the time of adding execution permission, Zip also decompresses faster than TGZ.

<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/35082/timeline,,spark,apache,zhongjingxiong,84573424,MDQ6VXNlcjg0NTczNDI0,https://avatars.githubusercontent.com/u/84573424?v=4,,https://api.github.com/users/zhongjingxiong,https://github.com/zhongjingxiong,https://api.github.com/users/zhongjingxiong/followers,https://api.github.com/users/zhongjingxiong/following{/other_user},https://api.github.com/users/zhongjingxiong/gists{/gist_id},https://api.github.com/users/zhongjingxiong/starred{/owner}{/repo},https://api.github.com/users/zhongjingxiong/subscriptions,https://api.github.com/users/zhongjingxiong/orgs,https://api.github.com/users/zhongjingxiong/repos,https://api.github.com/users/zhongjingxiong/events{/privacy},https://api.github.com/users/zhongjingxiong/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35082,https://github.com/apache/spark/pull/35082,https://github.com/apache/spark/pull/35082.diff,https://github.com/apache/spark/pull/35082.patch,,https://api.github.com/repos/apache/spark/issues/35082/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
84,https://api.github.com/repos/apache/spark/issues/35076,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35076/labels{/name},https://api.github.com/repos/apache/spark/issues/35076/comments,https://api.github.com/repos/apache/spark/issues/35076/events,https://github.com/apache/spark/pull/35076,1091411846,PR_kwDOAQXtWs4wab-B,35076,[SPARK-37793][CORE][SHUFFLE] Fallback to fetch original blocks when noLocalMergedBlockDataError,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-12-31T04:29:31Z,2022-01-11T23:26:36Z,,MEMBER,,False,"### What changes were proposed in this pull request?

When enable push-based shuffle, there is a chance that task hang at 

```
59  Executor task launch worker for task 424.0 in stage 753.0 (TID 106778)
WAITING	Lock(java.util.concurrent.ThreadPoolExecutor$Worker@1660371198})
  sun.misc.Unsafe.park(Native Method)
  java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
  java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2044)
  java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
  org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:756)
  org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:85)
  org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
  scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
  scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
  scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
  org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
  org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
  scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.sort_addToSorter_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)
  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.smj_findNextJoinRows_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_1$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithKeys_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)
  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
  scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
  org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
  org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
  org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
  org.apache.spark.scheduler.Task.run(Task.scala:136)
  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)
  org.apache.spark.executor.Executor$TaskRunner$$Lambda$518/852390142.apply(Unknown Source)
  org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1470)
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  java.lang.Thread.run(Thread.java:748)
```

And `org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:756)` is 
```
while (result == null) {
  ...
  result = results.take() // line 756
  ...
}
```

After some investigations, found that the last `FetchResult` put into `result` is `PushMergedLocalMetaFetchResult`, and there is a chance that `bufs` is empty, will cause no `SuccessFetchResult` be added to `results`, and thread hang if no other `FetchResult` is put into `results`.

```scala
while (result == null) {
  ...
  result = results.take()
  ...

  result match {
    case r @ SuccessFetchResult(blockId, mapIndex, address, size, buf, isNetworkReqDone) =>
      ...
      case PushMergedLocalMetaFetchResult(
        shuffleId, shuffleMergeId, reduceId, bitmaps, localDirs) =>
        val shuffleBlockId = ShuffleMergedBlockId(shuffleId, shuffleMergeId, reduceId)
        try {
          val bufs: Seq[ManagedBuffer] = blockManager.getLocalMergedBlockData(shuffleBlockId,
            localDirs)
          // THERE IS A CHANCE THAT bufs.isEmpty!
          ...
          bufs.zipWithIndex.foreach { case (buf, chunkId) =>
            buf.retain()
            val shuffleChunkId = ShuffleBlockChunkId(shuffleId, shuffleMergeId, reduceId,
              chunkId)
            pushBasedFetchHelper.addChunk(shuffleChunkId, bitmaps(chunkId))
            results.put(SuccessFetchResult(shuffleChunkId, SHUFFLE_PUSH_MAP_ID,
              pushBasedFetchHelper.localShuffleMergerBlockMgrId, buf.size(), buf,
              isNetworkReqDone = false))
          }
        } catch {
          case e: Exception =>
            pushBasedFetchHelper.initiateFallbackFetchForPushMergedBlock(
              shuffleBlockId, pushBasedFetchHelper.localShuffleMergerBlockMgrId)
        }
        result = null
    ...
  }
}
```

### Why are the changes needed?

Fallback to fetch original blocks when noLocalMergedBlockDataError to avoid task hang.

### Does this PR introduce _any_ user-facing change?
Bug fix, to make push-based shuffle more stable.

### How was this patch tested?
Pass 1T TPC-DS tests",https://api.github.com/repos/apache/spark/issues/35076/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35076,https://github.com/apache/spark/pull/35076,https://github.com/apache/spark/pull/35076.diff,https://github.com/apache/spark/pull/35076.patch,,https://api.github.com/repos/apache/spark/issues/35076/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
85,https://api.github.com/repos/apache/spark/issues/35067,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35067/labels{/name},https://api.github.com/repos/apache/spark/issues/35067/comments,https://api.github.com/repos/apache/spark/issues/35067/events,https://github.com/apache/spark/pull/35067,1090858330,PR_kwDOAQXtWs4wYo-P,35067,[SPARK-37423][PYTHON] Inline type hints for fpm.py in python/pyspark/mllib,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-30T03:53:40Z,2021-12-30T03:55:05Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for fpm.py, test.py in python/pyspark/mllib/


### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing tests
",https://api.github.com/repos/apache/spark/issues/35067/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35067,https://github.com/apache/spark/pull/35067,https://github.com/apache/spark/pull/35067.diff,https://github.com/apache/spark/pull/35067.patch,,https://api.github.com/repos/apache/spark/issues/35067/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
86,https://api.github.com/repos/apache/spark/issues/35049,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35049/labels{/name},https://api.github.com/repos/apache/spark/issues/35049/comments,https://api.github.com/repos/apache/spark/issues/35049/events,https://github.com/apache/spark/pull/35049,1090233855,PR_kwDOAQXtWs4wWnuT,35049,[SPARK-37757][BUILD] Enable Spark test scheduled job on ARM runner,"[{'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-29T02:54:24Z,2022-01-01T00:20:37Z,,CONTRIBUTOR,,True,"### What changes were proposed in this pull request?
This patch adds the scheduled job on ARM runner.


### Why are the changes needed?
Migrate Spark Arm Job from Jenkins to GitHub Actions.


### Does this PR introduce _any_ user-facing change?
NO


### How was this patch tested?
- Trigger arm64 test in the repo: https://github.com/Yikun/spark/pull/51
```
Current runner version: '2.285.1'
Runner name: 'ubuntu-20.04-arm64'
Runner group name: 'Default'
Machine name: 'yikun-arm'
```
- Trigger original x86 test in this patch",https://api.github.com/repos/apache/spark/issues/35049/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35049,https://github.com/apache/spark/pull/35049,https://github.com/apache/spark/pull/35049.diff,https://github.com/apache/spark/pull/35049.patch,,https://api.github.com/repos/apache/spark/issues/35049/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
87,https://api.github.com/repos/apache/spark/issues/35045,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35045/labels{/name},https://api.github.com/repos/apache/spark/issues/35045/comments,https://api.github.com/repos/apache/spark/issues/35045/events,https://github.com/apache/spark/pull/35045,1090162794,PR_kwDOAQXtWs4wWZfN,35045,[SPARK-37765][PYSPARK][WIP] DynamicDataFrame implementation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-28T22:45:17Z,2021-12-29T11:33:44Z,,NONE,,False,"This is still missing a way to handle typing and also a set of tests
for the API, but it's a functional implementation to showcase the examples
written down in the PR and the JIRA ticket.

### What changes were proposed in this pull request?
It adds a new class, `DynamicDataFrame`, that allows users to implement inheritance on the `DataFrames` without losing chainability. The PR also has the autogenerator of the `DynamicDataFrame` class code in case any reviewer wants to remove one of the methods from the default-inherited ones.

### Why are the changes needed?
In typical development settings, multiple tables with very different concepts are mapped to the same `DataFrame` class. The inheritance from the pyspark `DataFrame` class is a bit cumbersome because of the chainable methods and it also makes it difficult to abstract regularly used queries. The proposal is to generate a `DynamicDataFrame` that allows easy inheritance retaining `DataFrame` methods without losing chainability neither for the newly generated queries nor for the usual dataframe ones.

In our experience, this allowed us to iterate much faster, generating business-centric classes in a couple of lines of code.


### Does this PR introduce _any_ user-facing change?
Yes: it adds a new class. It doesn't change any already existing code.


### How was this patch tested?
There is no test suite here since we are not sure about how to properly test the new API (we are inclined to generate one test case per method, ensuring that it generates objects of the proper type). We add here two code examples:

#### Inheriting from DataFrame
This shows the typical issues encountered when we try to inherit from pyspark `DataFrame`. It should work on the `master` branch as well

```python
import pyspark
from pyspark.sql import DataFrame
from pyspark.sql import functions as F

spark = pyspark.sql.SparkSession.builder.getOrCreate()


class Inventory(DataFrame):
    def __init__(self, df: DataFrame):
        super().__init__(df._jdf, df.sql_ctx)

    def update_prices(self, factor: float = 2.0):
        return self.withColumn(""price"", F.col(""price"") * factor)


base_dataframe = spark.createDataFrame(
    data=[[""product_1"", 2.0], [""product_2"", 4.0]],
    schema=[""name"", ""price""],
)
inventory = Inventory(base_dataframe)
inventory_updated = inventory.update_prices(2.0)
print(""inventory_updated.show():"")
inventory_updated.show()
print(""But after one use of the query we have a plain dataframe again"")
print(f""type(inventory_updated): {type(inventory_updated) }"")
# This would raise an AttributeError
# inventory_updated.update_prices(5.0)

print(""The same happens when we use DataFrame methods"")
expensive_inventory = inventory.filter(F.col(""price"") > 3.0)
print(""expensive_inventory.show():"")
expensive_inventory.show()
print(f""type(expensive_inventory): {type(expensive_inventory) }"")
```

and its output
```
inventory_updated.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_1|  4.0|
|product_2|  8.0|
+---------+-----+

But after one use of the query we have a plain dataframe again
type(inventory_updated): <class 'pyspark.sql.dataframe.DataFrame'>
The same happens when we use DataFrame methods
expensive_inventory.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_2|  4.0|
+---------+-----+

type(expensive_inventory): <class 'pyspark.sql.dataframe.DataFrame'>
```
#### Inheritance from DynamicDataFrame
This is what inheritance would look like if we used `DynamicDataFrame`, that runs on the current branch:

```python
import pyspark
from pyspark.sql import DynamicDataFrame
from pyspark.sql import functions as F

spark = pyspark.sql.SparkSession.builder.getOrCreate()


class Inventory(DynamicDataFrame):
    def update_prices(self, factor: float = 2.0):
        return self.withColumn(""price"", F.col(""price"") * factor)


base_dataframe = spark.createDataFrame(
    data=[[""product_1"", 2.0], [""product_2"", 4.0]],
    schema=[""name"", ""price""],
)
print(""Doing an inheritance mediated by DynamicDataFrame"")
inventory = Inventory(base_dataframe)
inventory_updated = inventory.update_prices(2.0).update_prices(5.0)
print(""inventory_updated.show():"")
inventory_updated.show()
print(""After multiple uses of the query we still have the desired type"")
print(f""type(inventory_updated): {type(inventory_updated)}"")
print(""We can still use the usual dataframe methods"")
expensive_inventory = inventory_updated.filter(F.col(""price"") > 25)
print(""expensive_inventory.show():"")
expensive_inventory.show()
print(""And retain the desired type"")
print(f""type(expensive_inventory): {type(expensive_inventory)}"")
```

and its output:
```
Doing an inheritance mediated by DynamicDataFrame
inventory_updated.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_1| 20.0|
|product_2| 40.0|
+---------+-----+

After multiple uses of the query we still have the desired type
type(inventory_updated): <class '__main__.Inventory'>
We can still use the usual dataframe methods
expensive_inventory.show():
+---------+-----+
|     name|price|
+---------+-----+
|product_2| 40.0|
+---------+-----+

And retain the desired type
type(expensive_inventory): <class '__main__.Inventory'>
```
",https://api.github.com/repos/apache/spark/issues/35045/timeline,,spark,apache,pabloalcain,6975120,MDQ6VXNlcjY5NzUxMjA=,https://avatars.githubusercontent.com/u/6975120?v=4,,https://api.github.com/users/pabloalcain,https://github.com/pabloalcain,https://api.github.com/users/pabloalcain/followers,https://api.github.com/users/pabloalcain/following{/other_user},https://api.github.com/users/pabloalcain/gists{/gist_id},https://api.github.com/users/pabloalcain/starred{/owner}{/repo},https://api.github.com/users/pabloalcain/subscriptions,https://api.github.com/users/pabloalcain/orgs,https://api.github.com/users/pabloalcain/repos,https://api.github.com/users/pabloalcain/events{/privacy},https://api.github.com/users/pabloalcain/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35045,https://github.com/apache/spark/pull/35045,https://github.com/apache/spark/pull/35045.diff,https://github.com/apache/spark/pull/35045.patch,,https://api.github.com/repos/apache/spark/issues/35045/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
88,https://api.github.com/repos/apache/spark/issues/35043,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35043/labels{/name},https://api.github.com/repos/apache/spark/issues/35043/comments,https://api.github.com/repos/apache/spark/issues/35043/events,https://github.com/apache/spark/pull/35043,1090108135,PR_kwDOAQXtWs4wWOTm,35043,Remove characters added by IPython,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-28T20:18:06Z,2022-01-02T16:14:56Z,,NONE,,False,"When running `PYSPARK_DRIVER_PYTHON=ipython pyspark` on xterm the find-spark-home script calls `ipython /path/to/find_spark_home.py` and the string printed by that script gets assigned to SPARK_HOME. When run with IPython that string will start with a sequence bounded by control characters before the path determined by find_spark_home.py. While this part of the string does not appear on echo it will cause pyspark to compose paths improperly when using SPARK_HOME.

To see the sequence run:

>>> import os
>>> p = os.popen('ipython somescript.py')
>>> p.read()

`'\x1b[22;0t\x1b]0;IPython: {current directory}\x07the expected output\n'`

The cut command removes the sequence before ""the expected output"". Lines without a bell character (\x07), such as you get when running `python3 find_spark_home.py`, remain unchanged.

### What changes were proposed in this pull request?
Fixing the assignment to SPARK_HOME in find-spark-home to remove the control characters added when using ipython.


### Why are the changes needed?
On xterm running `PYSPARK_DRIVER_PYTHON=ipython pyspark` causes pyspark to compose paths improperly, prepending the current working directory to SPARK_HOME as determined by find_spark_home.py, making it unable to find the files it seeks.


### Does this PR introduce _any_ user-facing change?
Yes. Before the change I would get ""No such file or directory"" errors as the current working directory would get prepended to SPARK_HOME. After the change the pyspark interactive prompt starts as expected with an ipython prompt.


### How was this patch tested?
I ran pyspark with PYSPARK_DRIVER_PYTHON set to ""python"", ""python3"" and ""ipython"". All three variations gave the appropriate prompt with the expected session and context variables set. I also tested the pipe to the cut command with lines with and without bell characters to ensure that the addition had no effect on the latter. I didn't modify the current testing scheme because I couldn't find an extant test for any of the relevant bash scripts.
",https://api.github.com/repos/apache/spark/issues/35043/timeline,,spark,apache,Shooter23,44271378,MDQ6VXNlcjQ0MjcxMzc4,https://avatars.githubusercontent.com/u/44271378?v=4,,https://api.github.com/users/Shooter23,https://github.com/Shooter23,https://api.github.com/users/Shooter23/followers,https://api.github.com/users/Shooter23/following{/other_user},https://api.github.com/users/Shooter23/gists{/gist_id},https://api.github.com/users/Shooter23/starred{/owner}{/repo},https://api.github.com/users/Shooter23/subscriptions,https://api.github.com/users/Shooter23/orgs,https://api.github.com/users/Shooter23/repos,https://api.github.com/users/Shooter23/events{/privacy},https://api.github.com/users/Shooter23/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35043,https://github.com/apache/spark/pull/35043,https://github.com/apache/spark/pull/35043.diff,https://github.com/apache/spark/pull/35043.patch,,https://api.github.com/repos/apache/spark/issues/35043/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
89,https://api.github.com/repos/apache/spark/issues/35041,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35041/labels{/name},https://api.github.com/repos/apache/spark/issues/35041/comments,https://api.github.com/repos/apache/spark/issues/35041/events,https://github.com/apache/spark/pull/35041,1089777512,PR_kwDOAQXtWs4wVKcA,35041,[SPARK-37691][SQL] Support ANSI Aggregation Function: `percentile_disc`,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-28T10:29:20Z,2022-01-27T01:11:44Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`PERCENTILE_DISC` is an ANSI aggregate functions.

The mainstream database supports `percentile_disc` show below:
**Postgresql**
https://www.postgresql.org/docs/9.4/functions-aggregate.html
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/cPkFySIBORL~M938Zv07Cg
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/percentile_disc.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/PERCENTILE_DISC.html#GUID-7C34FDDA-C241-474F-8C5C-50CC0182E005
**H2**
http://www.h2database.com/html/functions-aggregate.html#percentile_disc
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc01776.1601/doc/html/san1278453110413.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/percentile_disc.htm
**RedShift**
https://docs.aws.amazon.com/redshift/latest/dg/r_APPROXIMATE_PERCENTILE_DISC.html
**Yellowbrick**
https://www.yellowbrick.com/docs/2.2/ybd_sqlref/percentile_disc.html
**Mariadb**
https://mariadb.com/kb/en/percentile_disc/
**Phoenix**
http://phoenix.incubator.apache.org/language/functions.html#percentile_disc
**Singlestore**
https://docs.singlestore.com/db/v7.6/en/reference/sql-reference/window-functions/percentile_disc.html

### Why are the changes needed?
`PERCENTILE_DISC` is very useful. Exposing the expression can make the migration from other systems to Spark SQL easier.

### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/35041/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35041,https://github.com/apache/spark/pull/35041,https://github.com/apache/spark/pull/35041.diff,https://github.com/apache/spark/pull/35041.patch,,https://api.github.com/repos/apache/spark/issues/35041/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
90,https://api.github.com/repos/apache/spark/issues/35017,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/35017/labels{/name},https://api.github.com/repos/apache/spark/issues/35017/comments,https://api.github.com/repos/apache/spark/issues/35017/events,https://github.com/apache/spark/pull/35017,1088555915,PR_kwDOAQXtWs4wRYLX,35017,[SPARK-36853][BUILD] Code failing on checkstyle,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-25T06:32:52Z,2021-12-29T16:29:17Z,,CONTRIBUTOR,,False,"What changes were proposed in this pull request?

Fixed the checkstyle error thrown in windows in maven installing phase.

Why are the changes needed?

Due to some problems in CI system, these obvious errors have not been detected. I think these checkstyle errors should be solved.

Does this PR introduce any user-facing change?

No.

How was this patch tested?

I have verified by executing following commands in the win10 system, except for (modifier) RedundantModifier in TimSort.java and (naming) MethodName in GroupStateTimeout.java and OutputMode.java and Trigger.java, the rest of the errors have been fixed.
`mvn -DskipTest clean install`",https://api.github.com/repos/apache/spark/issues/35017/timeline,,spark,apache,Shockang,28219857,MDQ6VXNlcjI4MjE5ODU3,https://avatars.githubusercontent.com/u/28219857?v=4,,https://api.github.com/users/Shockang,https://github.com/Shockang,https://api.github.com/users/Shockang/followers,https://api.github.com/users/Shockang/following{/other_user},https://api.github.com/users/Shockang/gists{/gist_id},https://api.github.com/users/Shockang/starred{/owner}{/repo},https://api.github.com/users/Shockang/subscriptions,https://api.github.com/users/Shockang/orgs,https://api.github.com/users/Shockang/repos,https://api.github.com/users/Shockang/events{/privacy},https://api.github.com/users/Shockang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/35017,https://github.com/apache/spark/pull/35017,https://github.com/apache/spark/pull/35017.diff,https://github.com/apache/spark/pull/35017.patch,,https://api.github.com/repos/apache/spark/issues/35017/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
91,https://api.github.com/repos/apache/spark/issues/34995,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34995/labels{/name},https://api.github.com/repos/apache/spark/issues/34995/comments,https://api.github.com/repos/apache/spark/issues/34995/events,https://github.com/apache/spark/pull/34995,1087396103,PR_kwDOAQXtWs4wNqOw,34995,[SPARK-37722][SQL] Escape dot character in partition names,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-12-23T05:45:19Z,2022-01-11T03:20:19Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR escapes `.` character in partition names as certain file systems (for example, ABFS) may not support it.

I simply added the `.` character to the list of special characters in `ExternalCatalogUtils`. The class already covers backward compatibility so it would read the existing unescaped values with dots correctly.

### Why are the changes needed?
Fixes an issue when ABFS throws `Caused by: java.lang.IllegalArgumentException: ABFS does not allow files or directories to end with a dot.` exception if a partition name contains `.` symbol.

### Does this PR introduce _any_ user-facing change?
Yes.

Forward compatibility is fine, Spark will be able to read both escaped and unescaped `.` characters.

Although there are no changes in reading string values, there are the following backward incompatible changes:
- Double values will not be inferred correctly, this would require backports to older Spark versions.
- There is a user-facing change when displaying any double values or string values containing a dot. For example, the command `SHOW PARTITIONS` would return slightly different result as well as directory names:

Before:
```
SHOW PARTITIONS escapeDots;

+----------+
| partition|
+----------+
|value=%2E.|
|   value=.|
|  value=a.|
|value=b.c.|
+----------+
```


After:
```
SHOW PARTITIONS escapeDots;

+--------------+
|     partition|
+--------------+
|value=%252E%2E|
|     value=%2E|
|    value=a%2E|
|value=b%2Ec%2E|
+--------------+
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Added a unit test in `InsertSuite` to confirm that dots are now escaped + existing unit tests.",https://api.github.com/repos/apache/spark/issues/34995/timeline,,spark,apache,sadikovi,7788766,MDQ6VXNlcjc3ODg3NjY=,https://avatars.githubusercontent.com/u/7788766?v=4,,https://api.github.com/users/sadikovi,https://github.com/sadikovi,https://api.github.com/users/sadikovi/followers,https://api.github.com/users/sadikovi/following{/other_user},https://api.github.com/users/sadikovi/gists{/gist_id},https://api.github.com/users/sadikovi/starred{/owner}{/repo},https://api.github.com/users/sadikovi/subscriptions,https://api.github.com/users/sadikovi/orgs,https://api.github.com/users/sadikovi/repos,https://api.github.com/users/sadikovi/events{/privacy},https://api.github.com/users/sadikovi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34995,https://github.com/apache/spark/pull/34995,https://github.com/apache/spark/pull/34995.diff,https://github.com/apache/spark/pull/34995.patch,,https://api.github.com/repos/apache/spark/issues/34995/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
92,https://api.github.com/repos/apache/spark/issues/34990,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34990/labels{/name},https://api.github.com/repos/apache/spark/issues/34990/comments,https://api.github.com/repos/apache/spark/issues/34990/events,https://github.com/apache/spark/pull/34990,1087307237,PR_kwDOAQXtWs4wNX_6,34990,[SPARK-37717][SQL] Improve logging in BroadcastExchangeExec,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-23T01:55:36Z,2021-12-24T20:17:49Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
The logging and errors from inside

`
override lazy val relationFuture: Future[broadcast.Broadcast[Any]] = {
  SQLExecution.withThreadLocalCaptured[broadcast.Broadcast[Any]](
    session, BroadcastExchangeExec.executionContext) {
        try {
.....     
} 
`
are hard to relate to the right join in a plan with lost of Broadcast joins.
Spark UI displays

`BroadcastExchangeExec.runId`

but it's not in the logs.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Logs will now look like
`2021-11-13 21:14:59,028 INFO [broadcast-exchange-6-e20800a6-5204-40c5-82d8-40662affc244] [tenant:] [app: ] [appID:application_1627787411812_5950609] [executor:driver] [cid: ] [prid: ] TaskMemoryManager: org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:238) - null (TID 0) acquired 16.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@33bfacf0`

instead of

`2021-11-13 21:14:59,028 INFO [broadcast-exchange-6] [tenant:] [app: ] [appID:application_1627787411812_5950609] [executor:driver] [cid: ] [prid: ] TaskMemoryManager: org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:238) - null (TID 0) acquired 16.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@33bfacf0`

### How was this patch tested?
Existing tests
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34990/timeline,,spark,apache,ekoifman,4297661,MDQ6VXNlcjQyOTc2NjE=,https://avatars.githubusercontent.com/u/4297661?v=4,,https://api.github.com/users/ekoifman,https://github.com/ekoifman,https://api.github.com/users/ekoifman/followers,https://api.github.com/users/ekoifman/following{/other_user},https://api.github.com/users/ekoifman/gists{/gist_id},https://api.github.com/users/ekoifman/starred{/owner}{/repo},https://api.github.com/users/ekoifman/subscriptions,https://api.github.com/users/ekoifman/orgs,https://api.github.com/users/ekoifman/repos,https://api.github.com/users/ekoifman/events{/privacy},https://api.github.com/users/ekoifman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34990,https://github.com/apache/spark/pull/34990,https://github.com/apache/spark/pull/34990.diff,https://github.com/apache/spark/pull/34990.patch,,https://api.github.com/repos/apache/spark/issues/34990/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
93,https://api.github.com/repos/apache/spark/issues/34984,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34984/labels{/name},https://api.github.com/repos/apache/spark/issues/34984/comments,https://api.github.com/repos/apache/spark/issues/34984/events,https://github.com/apache/spark/pull/34984,1086713325,PR_kwDOAQXtWs4wLbFj,34984,[SPARK-37463][SQL] Read/Write Timestamp ntz from/to Orc uses int64,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-12-22T11:09:00Z,2022-01-13T19:08:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
#33588 (comment) show Spark cannot read/write timestamp ntz and ltz correctly. Based on the discussion https://github.com/apache/spark/pull/34741#issuecomment-983660633, we just to fix read/write timestamp ntz to Orc uses int64.

### Why are the changes needed?
Fix the bug about read/write timestamp ntz from/to Orc with different times zone.


### Does this PR introduce _any_ user-facing change?
Yes. Orc timestamp ntz is a new feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34984/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34984,https://github.com/apache/spark/pull/34984,https://github.com/apache/spark/pull/34984.diff,https://github.com/apache/spark/pull/34984.patch,,https://api.github.com/repos/apache/spark/issues/34984/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
94,https://api.github.com/repos/apache/spark/issues/34970,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34970/labels{/name},https://api.github.com/repos/apache/spark/issues/34970/comments,https://api.github.com/repos/apache/spark/issues/34970/events,https://github.com/apache/spark/pull/34970,1085588191,PR_kwDOAQXtWs4wHsP7,34970,[DO NOT MERGE] investigate test failures if we test ANSI mode in github actions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-21T08:55:18Z,2022-01-04T12:13:53Z,,MEMBER,,False,I am thinking about adding a new Github action job for testing ANSI mode. This PR is to collect the related test failures.,https://api.github.com/repos/apache/spark/issues/34970/timeline,,spark,apache,gengliangwang,1097932,MDQ6VXNlcjEwOTc5MzI=,https://avatars.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34970,https://github.com/apache/spark/pull/34970,https://github.com/apache/spark/pull/34970.diff,https://github.com/apache/spark/pull/34970.patch,,https://api.github.com/repos/apache/spark/issues/34970/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
95,https://api.github.com/repos/apache/spark/issues/34964,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34964/labels{/name},https://api.github.com/repos/apache/spark/issues/34964/comments,https://api.github.com/repos/apache/spark/issues/34964/events,https://github.com/apache/spark/pull/34964,1085357606,PR_kwDOAQXtWs4wG8OV,34964,[WIP][SPARK-37681][SQL] Support ANSI Aggregate Function: regr_sxy,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-21T01:57:06Z,2021-12-22T13:15:36Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_sxy`

The mainstream database supports `regr_sxy` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/MXr3jFyWutZ9J4fhlXv_fQ
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_sxy.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_sxy
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-sxy-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm


### Why are the changes needed?
`regr_sxy` is very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34964/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34964,https://github.com/apache/spark/pull/34964,https://github.com/apache/spark/pull/34964.diff,https://github.com/apache/spark/pull/34964.patch,,https://api.github.com/repos/apache/spark/issues/34964/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
96,https://api.github.com/repos/apache/spark/issues/34956,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34956/labels{/name},https://api.github.com/repos/apache/spark/issues/34956/comments,https://api.github.com/repos/apache/spark/issues/34956/events,https://github.com/apache/spark/pull/34956,1084465193,PR_kwDOAQXtWs4wECV4,34956,[SPARK-37688][CORE] ExecutorMonitor should ignore SparkListenerBlockUpdated event if executor was not active,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-20T07:48:34Z,2022-01-06T23:51:00Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
`ExecutorMonitor` should ignore `SparkListenerBlockUpdated` event if executor was not active

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If not ignored, `ExecutorMonitor` will create a new executor tracker with UNKNOWN_RESOURCE_PROFILE_ID for the dead executor. And `ExecutorAllocationManager` will not remove executor with UNKNOWN_RESOURCE_PROFILE_ID, which cause a executor slot is occupied by the dead executor, so a new one cannot be created.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Add a new test.",https://api.github.com/repos/apache/spark/issues/34956/timeline,,spark,apache,sleep1661,8802096,MDQ6VXNlcjg4MDIwOTY=,https://avatars.githubusercontent.com/u/8802096?v=4,,https://api.github.com/users/sleep1661,https://github.com/sleep1661,https://api.github.com/users/sleep1661/followers,https://api.github.com/users/sleep1661/following{/other_user},https://api.github.com/users/sleep1661/gists{/gist_id},https://api.github.com/users/sleep1661/starred{/owner}{/repo},https://api.github.com/users/sleep1661/subscriptions,https://api.github.com/users/sleep1661/orgs,https://api.github.com/users/sleep1661/repos,https://api.github.com/users/sleep1661/events{/privacy},https://api.github.com/users/sleep1661/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34956,https://github.com/apache/spark/pull/34956,https://github.com/apache/spark/pull/34956.diff,https://github.com/apache/spark/pull/34956.patch,,https://api.github.com/repos/apache/spark/issues/34956/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
97,https://api.github.com/repos/apache/spark/issues/34953,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34953/labels{/name},https://api.github.com/repos/apache/spark/issues/34953/comments,https://api.github.com/repos/apache/spark/issues/34953/events,https://github.com/apache/spark/pull/34953,1084329653,PR_kwDOAQXtWs4wDlqE,34953,[SPARK-37682][SQL]Apply 'merged column' and 'bit vector' in RewriteDistinctAggregates,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-20T04:00:02Z,2021-12-23T06:27:33Z,,NONE,,False,"### What changes were proposed in this pull request?
Adjust the grouping rules of `distinctAggGroups`, specifically in `RewriteDistinctAggregates.groupDistinctAggExpr`, so that some 'distinct' can be grouped together, and conditions(eg. CaseWhen, If) involved in them will be stored in the 'if_vector' to avoid unnecessary expanding. The 'if_vector' and 'filter_vector' introduced here can reduce the number of columns in the expand. Besides, children in distinct aggregate function with same datatype will share same project column.
Here is a example comparing the difference between the original expand rewriting and the new with 'merged column' and 'bit vector' (in sql):
```sql
SELECT
  COUNT(DISTINCT cat1) FILTER (WHERE id > 1) as cat1_filter_cnt_dist,
  COUNT(DISTINCT cat2) FILTER (WHERE id > 2) as cat2_filter_cnt_dist,
  COUNT(DISTINCT IF(value > 5, cat1, null)) as cat1_if_cnt_dist,
  COUNT(DISTINCT id) as id_cnt_dist,
  SUM(DISTINCT value) as id_sum_dist
FROM data
GROUP BY key
```
Current rule will rewrite the above sql plan to the following (pseudo) logical plan:
```
Aggregate(
   key = ['key]
   functions = [
       count('cat1) FILTER (WHERE (('gid = 1) AND 'max(id > 1))),
       count('(IF((value > 5), cat1, null))) FILTER (WHERE ('gid = 5)),
       count('cat2) FILTER (WHERE (('gid = 3) AND 'max(id > 2))),
       count('id) FILTER (WHERE ('gid = 2)),
       sum('value) FILTER (WHERE ('gid = 4))
   ]
   output = ['key, 'cat1_filter_cnt_dist, 'cat2_filter_cnt_dist, 'cat1_if_cnt_dist,
             'id_cnt_dist, 'id_sum_dist])
  Aggregate(
     key = ['key, 'cat1, 'value, 'cat2, '(IF((value > 5), cat1, null)), 'id, 'gid]
     functions = [max('id > 1), max('id > 2)]
     output = ['key, 'cat1, 'value, 'cat2, '(IF((value > 5), cat1, null)), 'id, 'gid,
               'max(id > 1), 'max(id > 2)])
    Expand(
       projections = [
         ('key, 'cat1, null, null, null, null, 1, ('id > 1), null),
         ('key, null, null, null, null, 'id, 2, null, null),
         ('key, null, null, 'cat2, null, null, 3, null, ('id > 2)),
         ('key, null, 'value, null, null, null, 4, null, null),
         ('key, null, null, null, if (('value > 5)) 'cat1 else null, null, 5, null, null)
       ]
       output = ['key, 'cat1, 'value, 'cat2, '(IF((value > 5), cat1, null)), 'id,
                 'gid, '(id > 1), '(id > 2)])
      LocalTableScan [...]
```
After applying 'merged column' and 'bit vector' tricks, the logical plan will become:
```
Aggregate(
   key = ['key]
   functions = [
       count('merged_string_1) FILTER (WHERE (('gid = 1) AND NOT (('filter_vector_1 & 1) = 0))),
       count(if (NOT (('if_vector_1 & 1) = 0)) 'merged_string_1 else null) FILTER (WHERE ('gid = 1)),
       count('merged_string_1) FILTER (WHERE (('gid = 2) AND NOT (('filter_vector_1 & 1) = 0))),
       count('merged_integer_1) FILTER (WHERE ('gid = 3)),
       sum('merged_integer_1) FILTER (WHERE ('gid = 4))
   ]
   output = ['key, 'cat1_filter_cnt_dist, 'cat2_filter_cnt_dist, 'cat1_if_cnt_dist,
             'id_cnt_dist, 'id_sum_dist])
  Aggregate(
     key = ['key, 'merged_string_1, 'merged_integer_1, 'gid]
     functions = [bit_or('if_vector_1),bit_or('filter_vector_1)]
     output = ['key, 'merged_string_1, 'merged_integer_1, 'gid, 'bit_or(if_vector_1), 'bit_or(filter_vector_1)])
    Expand(
       projections = [
         ('key, 'cat1, null, 1, if ('value > 5) 1 else 0, if ('id > 1) 1 else 0),
         ('key, 'cat2, null, 2, null, if ('id > 2) 1 else 0),
         ('key, null, 'id, 3, null, null),
         ('key, null, 'value, 4, null, null)
       ]
       output = ['key, 'merged_string_1, 'merged_integer_1, 'gid, 'if_vector_1, 'filter_vector_1])
      LocalTableScan [...]
```


### Why are the changes needed?
It can save mass memory and improve performance in some cases like:
```sql
SELECT 
  count(distinct case when cond1 then col1 end),
  count(distinct case when cond2 then col1 end),
  ...
FROM data
```

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing test and a new UT in DataFrameAggregateSuite to test 'Vector Size larger than 64'.
I have written some SQL locally to test the correctness of the distinct calculation, but it seems difficult to cover most of the cases. Perhaps spark's existing test set will be more comprehensive, so I didn't leave it in the code.
",https://api.github.com/repos/apache/spark/issues/34953/timeline,,spark,apache,Flyangz,39462475,MDQ6VXNlcjM5NDYyNDc1,https://avatars.githubusercontent.com/u/39462475?v=4,,https://api.github.com/users/Flyangz,https://github.com/Flyangz,https://api.github.com/users/Flyangz/followers,https://api.github.com/users/Flyangz/following{/other_user},https://api.github.com/users/Flyangz/gists{/gist_id},https://api.github.com/users/Flyangz/starred{/owner}{/repo},https://api.github.com/users/Flyangz/subscriptions,https://api.github.com/users/Flyangz/orgs,https://api.github.com/users/Flyangz/repos,https://api.github.com/users/Flyangz/events{/privacy},https://api.github.com/users/Flyangz/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34953,https://github.com/apache/spark/pull/34953,https://github.com/apache/spark/pull/34953.diff,https://github.com/apache/spark/pull/34953.patch,,https://api.github.com/repos/apache/spark/issues/34953/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
98,https://api.github.com/repos/apache/spark/issues/34943,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34943/labels{/name},https://api.github.com/repos/apache/spark/issues/34943/comments,https://api.github.com/repos/apache/spark/issues/34943/events,https://github.com/apache/spark/pull/34943,1083774405,PR_kwDOAQXtWs4wB6Eb,34943,[WIP][SPARK-37672][SQL] Support ANSI Aggregate Function: regr_sxx,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-18T07:46:42Z,2021-12-22T13:15:53Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_sxx`

The mainstream database supports `regr_sxx` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/PBEW1OPIaxqkIf3CJfIr6A
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_sxx.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_sxx
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-sxx-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm


### Why are the changes needed?
`regr_sxx` is very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34943/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34943,https://github.com/apache/spark/pull/34943,https://github.com/apache/spark/pull/34943.diff,https://github.com/apache/spark/pull/34943.patch,,https://api.github.com/repos/apache/spark/issues/34943/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
99,https://api.github.com/repos/apache/spark/issues/34940,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34940/labels{/name},https://api.github.com/repos/apache/spark/issues/34940/comments,https://api.github.com/repos/apache/spark/issues/34940/events,https://github.com/apache/spark/pull/34940,1083746549,PR_kwDOAQXtWs4wB0yr,34940,[PYTHON] Use raise ... from instead of simply raise where applicable,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-18T04:49:31Z,2022-01-15T00:27:46Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
- use raise from to maintain cause traceability instead of raising a new exception during the handling of a prior exception


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
- using `raise ... from` maintains exception traceability and provides better tracebacks in tools such as Sentry
- see e.g. https://stackoverflow.com/questions/24752395/python-raise-from-usage


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
- yes, traceback chainings will be improved and more understandable


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
- this change is mainly cosmetic, no tests were added",https://api.github.com/repos/apache/spark/issues/34940/timeline,,spark,apache,martimlobao,6430786,MDQ6VXNlcjY0MzA3ODY=,https://avatars.githubusercontent.com/u/6430786?v=4,,https://api.github.com/users/martimlobao,https://github.com/martimlobao,https://api.github.com/users/martimlobao/followers,https://api.github.com/users/martimlobao/following{/other_user},https://api.github.com/users/martimlobao/gists{/gist_id},https://api.github.com/users/martimlobao/starred{/owner}{/repo},https://api.github.com/users/martimlobao/subscriptions,https://api.github.com/users/martimlobao/orgs,https://api.github.com/users/martimlobao/repos,https://api.github.com/users/martimlobao/events{/privacy},https://api.github.com/users/martimlobao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34940,https://github.com/apache/spark/pull/34940,https://github.com/apache/spark/pull/34940.diff,https://github.com/apache/spark/pull/34940.patch,,https://api.github.com/repos/apache/spark/issues/34940/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
100,https://api.github.com/repos/apache/spark/issues/34934,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34934/labels{/name},https://api.github.com/repos/apache/spark/issues/34934/comments,https://api.github.com/repos/apache/spark/issues/34934/events,https://github.com/apache/spark/pull/34934,1083106432,PR_kwDOAQXtWs4v_uSB,34934,[SPARK-37675][CORE][SHUFFLE] Return PushMergedRemoteMetaFailedFetchResult if no available push-merged block,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-12-17T10:23:20Z,2022-01-22T19:32:47Z,,MEMBER,,False,"### What changes were proposed in this pull request?

When push-based shuffle enabled, reduce task will ask ESS for MergedMetas, then `PushMergedRemoteMetaFailedFetchResult` should be returned instead of `PushMergedRemoteMetaFetchResult` if there is no available push-merged block on ESS.

### Why are the changes needed?

Because that push-based shuffle works as best-effort, there are opportunities that no chunks of the block are available on ESS, in current implementation, it will cause reduce task failed with `ArithmeticException: / by zero`.

```
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.spark.storage.PushBasedFetchHelper.createChunkBlockInfosFromMetaResponse(PushBasedFetchHelper.scala:117)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:980)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:84)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
```

After the change, a `PushMergedRemoteMetaFetchResult` will let reduce task fall back to fetching the original blocks.

### Does this PR introduce _any_ user-facing change?

Yes, it'a bug fix. The change makes push-based shuffle stable. Before this change, my 1T TPCDS test failed several times w/ `ArithmeticException: / by zero`, after the change, passed w/o any exception.

### How was this patch tested?

Existing tests, and run 1T TPCDS manually.",https://api.github.com/repos/apache/spark/issues/34934/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34934,https://github.com/apache/spark/pull/34934,https://github.com/apache/spark/pull/34934.diff,https://github.com/apache/spark/pull/34934.patch,,https://api.github.com/repos/apache/spark/issues/34934/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
101,https://api.github.com/repos/apache/spark/issues/34933,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34933/labels{/name},https://api.github.com/repos/apache/spark/issues/34933/comments,https://api.github.com/repos/apache/spark/issues/34933/events,https://github.com/apache/spark/pull/34933,1083025946,PR_kwDOAQXtWs4v_dOV,34933,[SPARK-37674][SQL] Reduce the output partition of output stage to avoid producing small files.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-17T08:54:18Z,2022-01-10T02:16:01Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Reduce the output partition of output stage to avoid producing small files.

### Why are the changes needed?

The partition size of the finalStage with `DataWritingCommand` or `V2TableWriteExec`  may use the `ADVISORY_PARTITION_SIZE_IN_BYTES` which is smaller one, and  may produce some small files, it is bad for production.

Sometime, we may adjust `ADVISORY_PARTITION_SIZE_IN_BYTES` to a big one to avoid above , but it is NOT a good idea, it may take effect other Jobs or stages to  coalesce small shuffle partitions or split skewed shuffle partition.

So we should introduce a new partition size instead of  `ADVISORY_PARTITION_SIZE_IN_BYTES`  for  the finalStage with `DataWritingCommand` or `V2TableWriteExec`  to avoid small files.


### Does this PR introduce _any_ user-facing change?

NO

### How was this patch tested?
Added unittests.
",https://api.github.com/repos/apache/spark/issues/34933/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34933,https://github.com/apache/spark/pull/34933,https://github.com/apache/spark/pull/34933.diff,https://github.com/apache/spark/pull/34933.patch,,https://api.github.com/repos/apache/spark/issues/34933/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
102,https://api.github.com/repos/apache/spark/issues/34929,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34929/labels{/name},https://api.github.com/repos/apache/spark/issues/34929/comments,https://api.github.com/repos/apache/spark/issues/34929/events,https://github.com/apache/spark/pull/34929,1082897559,PR_kwDOAQXtWs4v_Cgv,34929,[SPARK-37670][SQL] Support predicate pushdown and column pruning for de-duped CTEs,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-12-17T05:18:06Z,2022-01-06T16:24:18Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adds predicate push-down and column pruning to CTEs that are not inlined as well as fixes a few potential correctness issues:
  1) Replace (previously not inlined) CTE refs with Repartition operations at the end of logical plan optimization so that WithCTE is not carried over to physical plan. As a result, we can simplify the logic of physical planning, as well as avoid a correctness issue where the logical link of a physical plan node can point to `WithCTE` and lead to unexpected behaviors in AQE, e.g., class cast exceptions in DPP.
  2) Pull (not inlined) CTE defs from subqueries up to the main query level, in order to avoid creating copies of the same CTE def during predicate push-downs and other transformations.
  3) Make CTE IDs more deterministic by starting from 0 for each query.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Improve de-duped CTEs' performance with predicate pushdown and column pruning; fixes de-duped CTEs' correctness issues.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Added UTs.",https://api.github.com/repos/apache/spark/issues/34929/timeline,,spark,apache,maryannxue,4171904,MDQ6VXNlcjQxNzE5MDQ=,https://avatars.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34929,https://github.com/apache/spark/pull/34929,https://github.com/apache/spark/pull/34929.diff,https://github.com/apache/spark/pull/34929.patch,,https://api.github.com/repos/apache/spark/issues/34929/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
103,https://api.github.com/repos/apache/spark/issues/34924,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34924/labels{/name},https://api.github.com/repos/apache/spark/issues/34924/comments,https://api.github.com/repos/apache/spark/issues/34924/events,https://github.com/apache/spark/pull/34924,1082450530,PR_kwDOAQXtWs4v9k0w,34924,[SPARK-37145][K8S] Add support for extending user feature steps with configuration,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-12-16T17:12:46Z,2022-01-29T09:14:57Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This patch adds the support for extending user feature steps with configuration.

Before this patch user can only add feature step like:
- `class TestStep extends KubernetesFeatureConfigStep`

After this patch user can add feature step with configuration like:
- `class TestStep extends KubernetesFeatureConfigStep`
- `class TestStepWithK8SConf(conf: KubernetesConf) extends KubernetesFeatureConfigStep`
- `class TestStepWithDriverConf(conf: KubernetesDriverConf) extends KubernetesFeatureConfigStep`
- `class TestStepWithExecConf(conf: KubernetesExecutorConf) extends KubernetesFeatureConfigStep`

### Why are the changes needed?
In https://github.com/apache/spark/pull/30206 , a developer API for custom feature steps has been added, but it didn't support initialize user feature step with kubernetes conf (like `KubernetesConf`/`KubernetesDriverConf`/`KubernetesExecutorConf`).

In most of scenarios, users want to make corresponding changes in their feature steps according to the configuration. Such as, the customized scheduler scenario, user wants to configure pod according to passed job configuration.

### Does this PR introduce _any_ user-facing change?
Improve the developer API for for custom feature steps.


### How was this patch tested?
- Added UT
- Runing k8s integration test manaully: `build/sbt -Pkubernetes -Pkubernetes-integration-tests -Dtest.exclude.tags=minikube,r ""kubernetes-integration-tests/test`
",https://api.github.com/repos/apache/spark/issues/34924/timeline,,spark,apache,Yikun,1736354,MDQ6VXNlcjE3MzYzNTQ=,https://avatars.githubusercontent.com/u/1736354?v=4,,https://api.github.com/users/Yikun,https://github.com/Yikun,https://api.github.com/users/Yikun/followers,https://api.github.com/users/Yikun/following{/other_user},https://api.github.com/users/Yikun/gists{/gist_id},https://api.github.com/users/Yikun/starred{/owner}{/repo},https://api.github.com/users/Yikun/subscriptions,https://api.github.com/users/Yikun/orgs,https://api.github.com/users/Yikun/repos,https://api.github.com/users/Yikun/events{/privacy},https://api.github.com/users/Yikun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34924,https://github.com/apache/spark/pull/34924,https://github.com/apache/spark/pull/34924.diff,https://github.com/apache/spark/pull/34924.patch,,https://api.github.com/repos/apache/spark/issues/34924/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
104,https://api.github.com/repos/apache/spark/issues/34923,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34923/labels{/name},https://api.github.com/repos/apache/spark/issues/34923/comments,https://api.github.com/repos/apache/spark/issues/34923/events,https://github.com/apache/spark/pull/34923,1082262223,PR_kwDOAQXtWs4v88Ld,34923,[SPARK-35739][SQL] Add Java-compatible Dataset.join overloads,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-16T14:28:53Z,2021-12-16T14:37:27Z,,NONE,,False,"### What changes were proposed in this pull request?
Adds 3 new syntactic sugar overloads to Dataset's join method as proposed in [SPARK-35739](https://issues.apache.org/jira/browse/SPARK-35739).

### Why are the changes needed?
Improved development experience for developers using Spark SQL, specifically when coding in Java.  

Prior to changes the Seq overloads required developers to use less-known Java-to-Scala converter methods that made code less readable.  The overloads internalize those converter calls for two of the new methods and the third method adds a single-item overload that is useful for both Java and Scala.

### Does this PR introduce _any_ user-facing change?
Yes, the three new overloads technically constitute an API change to the Dataset class.  These overloads are net-new and have been commented appropriately in line with the existing methods.

### How was this patch tested?
Test cases were not added because it is unclear to me where/how syntactic sugar overloads fit into the testing suites (if at all).  Happy to add them if I can be pointed in the correct direction.

* Changes were tested in Scala via spark-shell.
* Changes were tested in Java by modifying an example:
  ```
  diff --git a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  index 86a9045d8a..342810c1e6 100644
  --- a/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  +++ b/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
  @@ -124,6 +124,10 @@ public class JavaSparkSQLExample {
       // |-- age: long (nullable = true)
       // |-- name: string (nullable = true)

  +    df.join(df, new String[] {""age""}).show();
  +    df.join(df, ""age"", ""left"").show();
  +    df.join(df, new String[] {""age""}, ""left"").show();
  +
       // Select only the ""name"" column
       df.select(""name"").show();
       // +-------+
  ```

#### Notes
Re-opening of #33323 with comments addressed.",https://api.github.com/repos/apache/spark/issues/34923/timeline,,spark,apache,brandondahler,1155895,MDQ6VXNlcjExNTU4OTU=,https://avatars.githubusercontent.com/u/1155895?v=4,,https://api.github.com/users/brandondahler,https://github.com/brandondahler,https://api.github.com/users/brandondahler/followers,https://api.github.com/users/brandondahler/following{/other_user},https://api.github.com/users/brandondahler/gists{/gist_id},https://api.github.com/users/brandondahler/starred{/owner}{/repo},https://api.github.com/users/brandondahler/subscriptions,https://api.github.com/users/brandondahler/orgs,https://api.github.com/users/brandondahler/repos,https://api.github.com/users/brandondahler/events{/privacy},https://api.github.com/users/brandondahler/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34923,https://github.com/apache/spark/pull/34923,https://github.com/apache/spark/pull/34923.diff,https://github.com/apache/spark/pull/34923.patch,,https://api.github.com/repos/apache/spark/issues/34923/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
105,https://api.github.com/repos/apache/spark/issues/34920,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34920/labels{/name},https://api.github.com/repos/apache/spark/issues/34920/comments,https://api.github.com/repos/apache/spark/issues/34920/events,https://github.com/apache/spark/pull/34920,1081933181,PR_kwDOAQXtWs4v72uH,34920,[SPARK-37661][SQL] SparkSQLCLIDriver shall avoid using hive defaults to resolve warehouse dir,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-16T09:00:06Z,2021-12-18T16:35:58Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

```
21/12/16 15:27:26.713 main INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
21/12/16 15:27:26.761 main INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.

...
...

21/12/16 15:27:36.559 main INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/12/16 15:27:36.561 main INFO SharedState: Warehouse path is 'file:/Users/kentyao/Downloads/spark/spark-3.2.0-bin-hadoop3.2/spark-warehouse'.
```

With log4j file appender, you will be able to see the logs like the above one when using spark-sql shell.

The first time the warehouse is resolved with hive defaults from codebase but the latter does not.

Later, we do extra effort in HiveClient to set it right again.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

1. fix a minor bug in the log,
2. simply to fix potential bugs


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
the existing tests shall be enough",https://api.github.com/repos/apache/spark/issues/34920/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34920,https://github.com/apache/spark/pull/34920,https://github.com/apache/spark/pull/34920.diff,https://github.com/apache/spark/pull/34920.patch,,https://api.github.com/repos/apache/spark/issues/34920/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
106,https://api.github.com/repos/apache/spark/issues/34916,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34916/labels{/name},https://api.github.com/repos/apache/spark/issues/34916/comments,https://api.github.com/repos/apache/spark/issues/34916/events,https://github.com/apache/spark/pull/34916,1081710675,PR_kwDOAQXtWs4v7IUM,34916,[SPARK-35355][SQL] Avoid shuffle in some scenes with CollectLimitExec,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-16T02:46:57Z,2022-01-16T13:51:02Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Some scenes retain the combination of GlobalLimitExec - LocalLimitExec, which can actually be optimized to CollectLimitExec, like: `INSERT INTO t2 SELECT c1 FROM t1 LIMIT 1`

The PR aims to 2 points:

- Improve `INSERT INTO t2 SELECT c1 FROM t1 LIMIT 1` to `CollectLimitExec`
- Add a flag to identifies `CollectLimitExec` as subExec without shuffling and scan all data. 

Example:
> CREATE TABLE t1(id STRING) USING parquet
CREATE TABLE t2(id STRING) USING parquet
INSERT INTO t1 SELECT * FROM t2 LIMIT 5

Before:
![Before PR](https://user-images.githubusercontent.com/51110188/146298688-4c79ab16-0df6-4677-af52-b880c7480cf0.png)

After:
<img width=""669"" alt=""After PR"" src=""https://user-images.githubusercontent.com/51110188/146298706-78ef867d-8342-4340-8ad5-7a648c597e50.png"">


### Why are the changes needed?
Optimize logic


### Does this PR introduce _any_ user-facing change?
No, optimize only


### How was this patch tested?
Unit Test and manually test.
",https://api.github.com/repos/apache/spark/issues/34916/timeline,,spark,apache,Yikf,51110188,MDQ6VXNlcjUxMTEwMTg4,https://avatars.githubusercontent.com/u/51110188?v=4,,https://api.github.com/users/Yikf,https://github.com/Yikf,https://api.github.com/users/Yikf/followers,https://api.github.com/users/Yikf/following{/other_user},https://api.github.com/users/Yikf/gists{/gist_id},https://api.github.com/users/Yikf/starred{/owner}{/repo},https://api.github.com/users/Yikf/subscriptions,https://api.github.com/users/Yikf/orgs,https://api.github.com/users/Yikf/repos,https://api.github.com/users/Yikf/events{/privacy},https://api.github.com/users/Yikf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34916,https://github.com/apache/spark/pull/34916,https://github.com/apache/spark/pull/34916.diff,https://github.com/apache/spark/pull/34916.patch,,https://api.github.com/repos/apache/spark/issues/34916/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
107,https://api.github.com/repos/apache/spark/issues/34908,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34908/labels{/name},https://api.github.com/repos/apache/spark/issues/34908/comments,https://api.github.com/repos/apache/spark/issues/34908/events,https://github.com/apache/spark/pull/34908,1080998439,PR_kwDOAQXtWs4v42a5,34908,[SPARK-37652][SQL]Support optimize skewed join through union,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-15T12:45:31Z,2021-12-30T01:30:41Z,,NONE,,False,"### What changes were proposed in this pull request?

Each child of the union handles data skew separately.


### Why are the changes needed?
`OptimizeSkewedJoin` rule will take effect only when the plan has two ShuffleQueryStageExec.

With `Union`, it might break the assumption. For example, the following plans

<b>scenes 1</b>
```
Union
    SMJ
        ShuffleQueryStage
        ShuffleQueryStage
    SMJ
        ShuffleQueryStage
        ShuffleQueryStage
```

<b>scenes 2</b>
```
Union
    SMJ
        ShuffleQueryStage
        ShuffleQueryStage
    HashAggregate
```
when one or more of the SMJ data in the above plan is skewed, it cannot be processed at present.

It's better to support partial optimize with Union.

### Does this PR introduce any user-facing change?

Probably yes, the result partition might changed.

### How was this patch tested?

Add test",https://api.github.com/repos/apache/spark/issues/34908/timeline,,spark,apache,mcdull-zhang,63445864,MDQ6VXNlcjYzNDQ1ODY0,https://avatars.githubusercontent.com/u/63445864?v=4,,https://api.github.com/users/mcdull-zhang,https://github.com/mcdull-zhang,https://api.github.com/users/mcdull-zhang/followers,https://api.github.com/users/mcdull-zhang/following{/other_user},https://api.github.com/users/mcdull-zhang/gists{/gist_id},https://api.github.com/users/mcdull-zhang/starred{/owner}{/repo},https://api.github.com/users/mcdull-zhang/subscriptions,https://api.github.com/users/mcdull-zhang/orgs,https://api.github.com/users/mcdull-zhang/repos,https://api.github.com/users/mcdull-zhang/events{/privacy},https://api.github.com/users/mcdull-zhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34908,https://github.com/apache/spark/pull/34908,https://github.com/apache/spark/pull/34908.diff,https://github.com/apache/spark/pull/34908.patch,,https://api.github.com/repos/apache/spark/issues/34908/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
108,https://api.github.com/repos/apache/spark/issues/34903,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34903/labels{/name},https://api.github.com/repos/apache/spark/issues/34903/comments,https://api.github.com/repos/apache/spark/issues/34903/events,https://github.com/apache/spark/pull/34903,1080437729,PR_kwDOAQXtWs4v2_iw,34903,[SPARK-37650][PYTHON] Tell spark-env.sh the python interpreter,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-12-15T00:43:33Z,2022-01-04T16:05:41Z,,NONE,,False,"When loading config defaults via spark-env.sh, it can be useful to know
the current pyspark python interpreter to allow the configuration to set
values properly. Pass this value in the environment as
_PYSPARK_DRIVER_SYS_EXECUTABLE to the environment script.

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
It's currently possible to set sensible site-wide spark configuration defaults by using `$SPARK_CONF_DIR/spark-env.sh`. In the case where a user is using pyspark, however, there are a number of things that aren't discoverable by that script, due to the way that it's called. There is a chain of calls (java_gateway.py -> shell script -> java -> shell script) that ends up obliterating any bit of the python context.

This change proposes to add en environment variable `_PYSPARK_DRIVER_SYS_EXECUTABLE` which points to the filename of the top-level python executable within pyspark's `java_gateway.py` bootstrapping process. With that, spark-env.sh will be able to infer enough information about the python environment to set the appropriate configuration variables.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Right now, there a number of config options useful to pyspark that can't be reliably set by `spark-env.sh` because it is unaware of the python context that spawning the executor. To give the most trivial example, it is currently possible to set `spark.kubernetes.container.image` or `spark.driver.host` based on information readily available from the environment (e.g. the k8s downward API). However, `spark.pyspark.python` and family cannot be set because when `spark-env.sh` executes it's lost all of the python context. We can instruct users to add the appropriate config variables, but this form of cargo-culting is error-prone and not scalable. It would be much better to expose important python variables so that pyspark can not be a second-class citizen.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. With this change, if python spawns the JVM, `spark-env.sh` will receive an environment variable `_PYSPARK_DRIVER_SYS_EXECUTABLE` pointing to the python executor.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
To be perfectly honest, I don't know where this fits into the testing infrastructure. I monkey-patched a binary 3.2.0 install to add the lines to java_gateway.py and that works, but in terms of adding this to the CI ... I'm at a loss. I'm more than willing to add the additional info, if needed.
",https://api.github.com/repos/apache/spark/issues/34903/timeline,,spark,apache,PerilousApricot,93354,MDQ6VXNlcjkzMzU0,https://avatars.githubusercontent.com/u/93354?v=4,,https://api.github.com/users/PerilousApricot,https://github.com/PerilousApricot,https://api.github.com/users/PerilousApricot/followers,https://api.github.com/users/PerilousApricot/following{/other_user},https://api.github.com/users/PerilousApricot/gists{/gist_id},https://api.github.com/users/PerilousApricot/starred{/owner}{/repo},https://api.github.com/users/PerilousApricot/subscriptions,https://api.github.com/users/PerilousApricot/orgs,https://api.github.com/users/PerilousApricot/repos,https://api.github.com/users/PerilousApricot/events{/privacy},https://api.github.com/users/PerilousApricot/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34903,https://github.com/apache/spark/pull/34903,https://github.com/apache/spark/pull/34903.diff,https://github.com/apache/spark/pull/34903.patch,,https://api.github.com/repos/apache/spark/issues/34903/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
109,https://api.github.com/repos/apache/spark/issues/34900,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34900/labels{/name},https://api.github.com/repos/apache/spark/issues/34900/comments,https://api.github.com/repos/apache/spark/issues/34900/events,https://github.com/apache/spark/pull/34900,1079934979,PR_kwDOAQXtWs4v1SPn,34900,"[SPARK-37643][SQL] when charVarcharAsString is true, char datatype partition table query incorrect","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-14T16:01:09Z,2021-12-27T13:28:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
after add ApplyCharTypePadding rule, when partition data type is char, if partition value length is less then defined, partition expr filter will be right-padding, then will query incorrect result


### Why are the changes needed?
fix query incorrect issue when filter is partition type and partition type is char.

### Does this PR introduce _any_ user-facing change?
before this fix, if we using char partition type, then we should be careful to set charVarcharAsString to true.

### How was this patch tested?
add new UT.
",https://api.github.com/repos/apache/spark/issues/34900/timeline,,spark,apache,fhygh,25889738,MDQ6VXNlcjI1ODg5NzM4,https://avatars.githubusercontent.com/u/25889738?v=4,,https://api.github.com/users/fhygh,https://github.com/fhygh,https://api.github.com/users/fhygh/followers,https://api.github.com/users/fhygh/following{/other_user},https://api.github.com/users/fhygh/gists{/gist_id},https://api.github.com/users/fhygh/starred{/owner}{/repo},https://api.github.com/users/fhygh/subscriptions,https://api.github.com/users/fhygh/orgs,https://api.github.com/users/fhygh/repos,https://api.github.com/users/fhygh/events{/privacy},https://api.github.com/users/fhygh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34900,https://github.com/apache/spark/pull/34900,https://github.com/apache/spark/pull/34900.diff,https://github.com/apache/spark/pull/34900.patch,,https://api.github.com/repos/apache/spark/issues/34900/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
110,https://api.github.com/repos/apache/spark/issues/34896,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34896/labels{/name},https://api.github.com/repos/apache/spark/issues/34896/comments,https://api.github.com/repos/apache/spark/issues/34896/events,https://github.com/apache/spark/pull/34896,1079736293,PR_kwDOAQXtWs4v0n1n,34896,[SPARK-37568][SQL] Support 2-arguments by the convert_timezone() function,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-12-14T13:06:57Z,2022-01-17T10:15:55Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Support 2-arguments  by the convert_timezone() function.
If users omit sourceTz, spark takes it from sourceTimestamp or SESSION_LOCAL_TIMEZONE.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Users can call this API with fewer arguments. The same function in Snowflake supports 2 arguments.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, users can call this function with 2 arguments.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
unit tests",https://api.github.com/repos/apache/spark/issues/34896/timeline,,spark,apache,yoda-mon,14937752,MDQ6VXNlcjE0OTM3NzUy,https://avatars.githubusercontent.com/u/14937752?v=4,,https://api.github.com/users/yoda-mon,https://github.com/yoda-mon,https://api.github.com/users/yoda-mon/followers,https://api.github.com/users/yoda-mon/following{/other_user},https://api.github.com/users/yoda-mon/gists{/gist_id},https://api.github.com/users/yoda-mon/starred{/owner}{/repo},https://api.github.com/users/yoda-mon/subscriptions,https://api.github.com/users/yoda-mon/orgs,https://api.github.com/users/yoda-mon/repos,https://api.github.com/users/yoda-mon/events{/privacy},https://api.github.com/users/yoda-mon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34896,https://github.com/apache/spark/pull/34896,https://github.com/apache/spark/pull/34896.diff,https://github.com/apache/spark/pull/34896.patch,,https://api.github.com/repos/apache/spark/issues/34896/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
111,https://api.github.com/repos/apache/spark/issues/34894,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34894/labels{/name},https://api.github.com/repos/apache/spark/issues/34894/comments,https://api.github.com/repos/apache/spark/issues/34894/events,https://github.com/apache/spark/pull/34894,1079474439,PR_kwDOAQXtWs4vzvtm,34894,[WIP][SPARK-37641][SQL] Support ANSI Aggregate Function: regr_r2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-12-14T08:47:33Z,2021-12-22T13:16:26Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to support ANSI aggregate Function: `regr_r2`

The mainstream database supports `regr_r2` show below:
**Teradata**
https://docs.teradata.com/r/756LNiPSFdY~4JcCCcR5Cw/exhFe2f_YyGqKFakYYUn2A
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_r2.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_r2
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-r2-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`regr_r2` is very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34894/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34894,https://github.com/apache/spark/pull/34894,https://github.com/apache/spark/pull/34894.diff,https://github.com/apache/spark/pull/34894.patch,,https://api.github.com/repos/apache/spark/issues/34894/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
112,https://api.github.com/repos/apache/spark/issues/34882,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34882/labels{/name},https://api.github.com/repos/apache/spark/issues/34882/comments,https://api.github.com/repos/apache/spark/issues/34882/events,https://github.com/apache/spark/pull/34882,1078431255,PR_kwDOAQXtWs4vwRBB,34882,[WIP][SPARK-37623][SQL] Support ANSI Aggregate Function: regr_slope & regr_intercept,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-12-13T11:47:08Z,2021-12-22T13:16:42Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`REGR_SLOPE` and `REGR_INTERCEPT` are ANSI aggregate functions

The mainstream database supports `regr_count` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/I0~kqsq3f3uNmjUaZr8hDg
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_slope.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_slope
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/topic/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-slope-function.html
**Presto**
https://prestodb.io/docs/current/functions/aggregate.html
Exasol
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`REGR_SLOPE` and `REGR_INTERCEPT` are very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34882/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34882,https://github.com/apache/spark/pull/34882,https://github.com/apache/spark/pull/34882.diff,https://github.com/apache/spark/pull/34882.patch,,https://api.github.com/repos/apache/spark/issues/34882/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
113,https://api.github.com/repos/apache/spark/issues/34872,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34872/labels{/name},https://api.github.com/repos/apache/spark/issues/34872/comments,https://api.github.com/repos/apache/spark/issues/34872/events,https://github.com/apache/spark/pull/34872,1077834416,PR_kwDOAQXtWs4vuVyT,34872,"[SPARK-37617][SQL][HIVE] In CTAS, Replace Parquet name columns that have not alias","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-12T15:27:52Z,2021-12-26T13:09:11Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
In CTAS, Replace name columns that have not alias.Mostly, columns without alias
  always is operator such as sum, divide that will lead to schema check error. Also,  provide a config option, ""spark.sql.schema.replace.alias.asColumn"", default is false to choose if we should replace the alias  or not. 


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
if we do not replace the alias ,the error 'Column name ""$name"" contains invalid character(s).
         |Please use alias to rename it.' will appear

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, 
spark sql(""CREATE TABLE `alias` (
  `a` INT,
  `b` STRING)
USING parquet"")
spark.sql("" create table  alias_b as select a, "" +
            ""max(unix_timestamp(b)) from alias group by a"")

Before this pr:
logical plan like this :
```
OptimizedCreateHiveTableAsSelectCommand [Database: default, TableName: alias_b, InsertIntoHadoopFsRelationCommand]
+- Aggregate [a#10], [a#10, max(unix_timestamp(b#11, yyyy-MM-dd HH:mm:ss, Some(America/Los_Angeles), false)) AS max(unix_timestamp(b, yyyy-MM-dd HH:mm:ss))#19L]
   +- SubqueryAlias spark_catalog.default.alias
      +- Relation default.alias[a#10,b#11] parquet

```
and error will throw:
```
Column name ""max(unix_timestamp(b, yyyy-MM-dd HH:mm:ss))"" contains invalid character(s). Please use alias to rename it.
```
After this pr and set spark.sql.schema.replace.alias.asColumn =true:
logical plan is :
```
== Analyzed Logical Plan ==
OptimizedCreateHiveTableAsSelectCommand [Database: default, TableName: alias_b, InsertIntoHadoopFsRelationCommand]
+- Aggregate [a#10], [a#10, max(unix_timestamp(b#11, yyyy-MM-dd HH:mm:ss, Some(America/Los_Angeles), false)) AS max_unix_timestamp_b__yyyy-MM-dd_HH:mm:ss__#14L]
   +- SubqueryAlias spark_catalog.default.alias
      +- Relation default.alias[a#10,b#11] parquet
```
and sql(""desc table alias_b"").show(false):
```
+-------------------------------------------+---------+-------+
|col_name                                   |data_type|comment|
+-------------------------------------------+---------+-------+
|a                                          |int      |null   |
|max_unix_timestamp_b__yyyy-MM-dd_HH:mm:ss__|bigint   |null   |
+-------------------------------------------+---------+-------+
```
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
unit test
",https://api.github.com/repos/apache/spark/issues/34872/timeline,,spark,apache,monkeyboy123,9074114,MDQ6VXNlcjkwNzQxMTQ=,https://avatars.githubusercontent.com/u/9074114?v=4,,https://api.github.com/users/monkeyboy123,https://github.com/monkeyboy123,https://api.github.com/users/monkeyboy123/followers,https://api.github.com/users/monkeyboy123/following{/other_user},https://api.github.com/users/monkeyboy123/gists{/gist_id},https://api.github.com/users/monkeyboy123/starred{/owner}{/repo},https://api.github.com/users/monkeyboy123/subscriptions,https://api.github.com/users/monkeyboy123/orgs,https://api.github.com/users/monkeyboy123/repos,https://api.github.com/users/monkeyboy123/events{/privacy},https://api.github.com/users/monkeyboy123/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34872,https://github.com/apache/spark/pull/34872,https://github.com/apache/spark/pull/34872.diff,https://github.com/apache/spark/pull/34872.patch,,https://api.github.com/repos/apache/spark/issues/34872/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
114,https://api.github.com/repos/apache/spark/issues/34871,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34871/labels{/name},https://api.github.com/repos/apache/spark/issues/34871/comments,https://api.github.com/repos/apache/spark/issues/34871/events,https://github.com/apache/spark/pull/34871,1077786843,PR_kwDOAQXtWs4vuMoZ,34871,[SPARK-37616][SQL] Support pushing down a dynamic partition pruning from one join to other joins.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-12T11:53:57Z,2021-12-17T09:46:52Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
 Introduce a rule to handle pushing down a dynamic partition pruning from a child join to its parent join, when the following conditions are met:
 (1) the pruning side of the parent join is a partition table
 (2) the table to prune is filterable by the JOIN key
 (3) the parent join operation is one of the following types: INNER, LEFT SEMI,
  LEFT OUTER (partitioned on right), or RIGHT OUTER (partitioned on left)
 
A query example: 
```sql
SELECT f.store_id, k.units_sold
FROM fact_stats f
JOIN dim_stats d
    ON f.store_id = d.store_id
        AND d.country = 'NL'
LEFT JOIN fact_sk k
    ON f.store_id = k.store_id
```

Before the PR:

![image](https://user-images.githubusercontent.com/39684231/145710866-70b44119-12ba-4afc-8a02-385189712950.png)

After the PR:
![image](https://user-images.githubusercontent.com/39684231/145710859-93ed90ae-1b5b-4516-a780-dfb1c14e5e0d.png)

### Why are the changes needed?
Push down a dynamic partition pruning from one join to other joins to prune the table of other joins  to improve performance.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No.

### How was this patch tested?
Added unittests.
",https://api.github.com/repos/apache/spark/issues/34871/timeline,,spark,apache,weixiuli,39684231,MDQ6VXNlcjM5Njg0MjMx,https://avatars.githubusercontent.com/u/39684231?v=4,,https://api.github.com/users/weixiuli,https://github.com/weixiuli,https://api.github.com/users/weixiuli/followers,https://api.github.com/users/weixiuli/following{/other_user},https://api.github.com/users/weixiuli/gists{/gist_id},https://api.github.com/users/weixiuli/starred{/owner}{/repo},https://api.github.com/users/weixiuli/subscriptions,https://api.github.com/users/weixiuli/orgs,https://api.github.com/users/weixiuli/repos,https://api.github.com/users/weixiuli/events{/privacy},https://api.github.com/users/weixiuli/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34871,https://github.com/apache/spark/pull/34871,https://github.com/apache/spark/pull/34871.diff,https://github.com/apache/spark/pull/34871.patch,,https://api.github.com/repos/apache/spark/issues/34871/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
115,https://api.github.com/repos/apache/spark/issues/34868,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34868/labels{/name},https://api.github.com/repos/apache/spark/issues/34868/comments,https://api.github.com/repos/apache/spark/issues/34868/events,https://github.com/apache/spark/pull/34868,1077529296,PR_kwDOAQXtWs4vthd4,34868,[SPARK-37614][SQL] Support ANSI Aggregate Function: regr_avgx & regr_avgy,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-12-11T12:51:51Z,2022-01-27T00:51:56Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`REGR_AVGX` and `REGR_AVGY` are ANSI aggregate functions

The mainstream database supports `regr_count` show below:
**Teradata**
https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/KkJgUSq2O6JRU3bCK~0cug
**Snowflake**
https://docs.snowflake.com/en/sql-reference/functions/regr_avgx.html
**Oracle**
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/REGR_-Linear-Regression-Functions.html#GUID-A675B68F-2A88-4843-BE2C-FCDE9C65F9A9
**Vertica**
https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/Functions/Aggregate/REGR_AVGX.htm?tocpath=SQL%20Reference%20Manual%7CSQL%20Functions%7CAggregate%20Functions%7C_____24
**H2**
http://www.h2database.com/html/functions-aggregate.html#regr_avgx
**Postgresql**
https://www.postgresql.org/docs/8.4/functions-aggregate.html
**Sybase**
https://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/regr-avgx-function.html
**Exasol**
https://docs.exasol.com/sql_references/functions/alphabeticallistfunctions/regr_function.htm

### Why are the changes needed?
`REGR_AVGX` and `REGR_AVGY` are very useful.


### Does this PR introduce _any_ user-facing change?
'Yes'. New feature.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34868/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34868,https://github.com/apache/spark/pull/34868,https://github.com/apache/spark/pull/34868.diff,https://github.com/apache/spark/pull/34868.patch,,https://api.github.com/repos/apache/spark/issues/34868/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
116,https://api.github.com/repos/apache/spark/issues/34864,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34864/labels{/name},https://api.github.com/repos/apache/spark/issues/34864/comments,https://api.github.com/repos/apache/spark/issues/34864/events,https://github.com/apache/spark/pull/34864,1077261110,PR_kwDOAQXtWs4vsylT,34864,[SHUFFLE] [WIP] Prototype: store shuffle file on external storage like S3,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2021-12-10T21:52:31Z,2022-01-06T00:00:06Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR ([design doc](https://docs.google.com/document/d/10rhvjXUlbQfWg-zh02_aqRqDT_ZnwYmICPAR--aRv64)) provides support to store shuffle files on external shuffle storage like S3. It helps Dynamic
Allocation on Kubernetes. Spark driver could release idle executors without worrying about losing
shuffle data because the shuffle data is store on external shuffle storage which are different
from executors.

This could be viewed as a followup work for https://issues.apache.org/jira/browse/SPARK-25299.

There is previously Worker Decommission feature ([SPARK-33545](https://issues.apache.org/jira/browse/SPARK-33545)), which is a great feature to copy shuffle data to fallback storage like S3. People appreciate that work to address the critical issue to handle shuffle data on Spark executor termination. The work in the PR does not intent to replace that feature. The intent is to get further discussion about how to save shuffle data on S3 during normal execution time.

### Why are the changes needed?

To better support Dynamic Allocation on Kubernetes, we need to decouple shuffle data from Spark
executor. This PR implements another Shuffle Manager and support writing shuffle data on S3.

### Does this PR introduce _any_ user-facing change?

Yes, this PR adds two Spark config like following to plug in another StarShuffleManager and store
shuffle data on provided S3 location.
```
spark.shuffle.manager=org.apache.spark.shuffle.StarShuffleManager
spark.shuffle.star.rootDir=s3://my_bucket_name/my_shuffle_folder
```

### How was this patch tested?

Added a unit test for StartShuffleManager. A lot of classes are copied from Spark, thus not add tests
for those classes. We will work with the community to get feedback first, then work on removing code
copy/duplication.",https://api.github.com/repos/apache/spark/issues/34864/timeline,,spark,apache,hiboyang,14280154,MDQ6VXNlcjE0MjgwMTU0,https://avatars.githubusercontent.com/u/14280154?v=4,,https://api.github.com/users/hiboyang,https://github.com/hiboyang,https://api.github.com/users/hiboyang/followers,https://api.github.com/users/hiboyang/following{/other_user},https://api.github.com/users/hiboyang/gists{/gist_id},https://api.github.com/users/hiboyang/starred{/owner}{/repo},https://api.github.com/users/hiboyang/subscriptions,https://api.github.com/users/hiboyang/orgs,https://api.github.com/users/hiboyang/repos,https://api.github.com/users/hiboyang/events{/privacy},https://api.github.com/users/hiboyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34864,https://github.com/apache/spark/pull/34864,https://github.com/apache/spark/pull/34864.diff,https://github.com/apache/spark/pull/34864.patch,,https://api.github.com/repos/apache/spark/issues/34864/reactions,9,9,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
117,https://api.github.com/repos/apache/spark/issues/34859,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34859/labels{/name},https://api.github.com/repos/apache/spark/issues/34859/comments,https://api.github.com/repos/apache/spark/issues/34859/events,https://github.com/apache/spark/pull/34859,1076428781,PR_kwDOAQXtWs4vqE4o,34859,[SPARK-37605][SQL] Support the configuration of the initial number of scan partitions when executing a take on a query,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-10T04:57:39Z,2021-12-13T02:11:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Support the configuration of the initial number of scan partitions when executing a take on a query.
`spark.sql.limit.initialPartitionNum`


### Why are the changes needed?
Now the initial number of scanned partitions is 1 by default when executing a take on a query.
This number does not support configuration.
Sometimes the first task runs slower. If we have this configuration, we can increase the initial parallelism.



### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
manual test
",https://api.github.com/repos/apache/spark/issues/34859/timeline,,spark,apache,cxzl25,3898450,MDQ6VXNlcjM4OTg0NTA=,https://avatars.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34859,https://github.com/apache/spark/pull/34859,https://github.com/apache/spark/pull/34859.diff,https://github.com/apache/spark/pull/34859.patch,,https://api.github.com/repos/apache/spark/issues/34859/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
118,https://api.github.com/repos/apache/spark/issues/34856,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34856/labels{/name},https://api.github.com/repos/apache/spark/issues/34856/comments,https://api.github.com/repos/apache/spark/issues/34856/events,https://github.com/apache/spark/pull/34856,1076263237,PR_kwDOAQXtWs4vpjfJ,34856,[SPARK-37602][CORE] Add config property to set default Spark listeners,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-10T01:10:27Z,2021-12-15T19:39:58Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Adds a new config property `spark.defaultListeners` which is intended to be set by administrators to signify the ""default"" set of Spark listeners to be used in the job. This list will be combined with `spark.extraListeners` at runtime. This is similar in spirit to `spark.driver.defaultJavaOptions` and `spark.plugins.defaultList`.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
`spark.extraListeners` allows users to custom Spark Listeners. Spark platform administrators would want to set their own set of ""default"" listeners for all jobs on the platform, however using `spark.extraListeners` makes it easy for end users to unknowingly override the default listeners.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, adds new a config property.


### How was this patch tested?
Modified existing UT to test new config property
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34856/timeline,,spark,apache,shardulm94,6961317,MDQ6VXNlcjY5NjEzMTc=,https://avatars.githubusercontent.com/u/6961317?v=4,,https://api.github.com/users/shardulm94,https://github.com/shardulm94,https://api.github.com/users/shardulm94/followers,https://api.github.com/users/shardulm94/following{/other_user},https://api.github.com/users/shardulm94/gists{/gist_id},https://api.github.com/users/shardulm94/starred{/owner}{/repo},https://api.github.com/users/shardulm94/subscriptions,https://api.github.com/users/shardulm94/orgs,https://api.github.com/users/shardulm94/repos,https://api.github.com/users/shardulm94/events{/privacy},https://api.github.com/users/shardulm94/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34856,https://github.com/apache/spark/pull/34856,https://github.com/apache/spark/pull/34856.diff,https://github.com/apache/spark/pull/34856.patch,,https://api.github.com/repos/apache/spark/issues/34856/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
119,https://api.github.com/repos/apache/spark/issues/34855,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34855/labels{/name},https://api.github.com/repos/apache/spark/issues/34855/comments,https://api.github.com/repos/apache/spark/issues/34855/events,https://github.com/apache/spark/pull/34855,1075929570,PR_kwDOAQXtWs4vodWE,34855,[WIP][SPARK-37600][BUILD] Upgrade to Hadoop 3.3.2,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-09T19:35:39Z,2022-01-27T00:34:32Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Upgrade to Hadoop 3.3.2.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34855/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34855,https://github.com/apache/spark/pull/34855,https://github.com/apache/spark/pull/34855.diff,https://github.com/apache/spark/pull/34855.patch,,https://api.github.com/repos/apache/spark/issues/34855/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
120,https://api.github.com/repos/apache/spark/issues/34849,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34849/labels{/name},https://api.github.com/repos/apache/spark/issues/34849/comments,https://api.github.com/repos/apache/spark/issues/34849/events,https://github.com/apache/spark/pull/34849,1075446422,PR_kwDOAQXtWs4vm2wE,34849,[SPARK-37596][SQL] Add the support for struct type column in the DropDuplicate,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-09T11:08:53Z,2021-12-10T06:10:16Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add the support for struct type column in the DropDuplicate in spark.Currently on using the struct col in the DropDuplicate we will get the below exception
```
case class StructDropDup(c1: Int, c2: Int)
val df = Seq((""d1"", StructDropDup(1, 2)),
      (""d1"", StructDropDup(1, 2))).toDF(""a"", ""b"")
df.dropDuplicates(""b.c1"")

org.apache.spark.sql.AnalysisException: Cannot resolve column name ""b.c1"" among (a, b)
  at org.apache.spark.sql.Dataset.$anonfun$dropDuplicates$1(Dataset.scala:2576)
  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
  at scala.collection.Iterator.foreach(Iterator.scala:941)
  at scala.collection.Iterator.foreach$(Iterator.scala:941)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429) 
```

As workAround in order to find the the duplicate using the struct column

df1.withColumn(""b.c1"", col(""b.c1"")).dropDuplicates(""b.c1"").drop(""b.c1"").collect
```
df.withColumn(""b.c1"", col(""b.c1"")).dropDuplicates(""b.c1"").drop(""b.c1"").collect
res25: Array[org.apache.spark.sql.Row] = Array([d1,[1,2]])
```
Used the similar approach from the workaround to provide the support for the struct column whenever struct col is present in the dropDuplicate

After fix

```
case class StructDropDup(c1: Int, c2: Int)
val df = Seq((""d1"", StructDropDup(1, 2)),
      (""d1"", StructDropDup(1, 2))).toDF(""a"", ""b"")
df.dropDuplicates(""b.c1"").show

+---+------+
|  a|     b|
+---+------+
| d1|{1, 2}|
+---+------+

scala>  df.dropDuplicates(""b.c1"").queryExecution
== Parsed Logical Plan ==
Project [a#7, b#8]
+- Deduplicate [b.c1#63]
   +- Project [a#7, b#8, b#8.c1 AS c1#62 AS b.c1#63]
      +- Project [_1#2 AS a#7, _2#3 AS b#8]
         +- LocalRelation [_1#2, _2#3]

== Analyzed Logical Plan ==
a: string, b: struct<c1:int,c2:int>
Project [a#7, b#8]
+- Deduplicate [b.c1#63]
   +- Project [a#7, b#8, b#8.c1 AS b.c1#63]
      +- Project [_1#2 AS a#7, _2#3 AS b#8]
         +- LocalRelation [_1#2, _2#3]

== Optimized Logical Plan ==
Aggregate [b.c1#63], [first(a#7, false) AS a#68, first(b#8, false) AS b#70]
+- LocalRelation [a#7, b#8, b.c1#63]

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- SortAggregate(key=[b.c1#63], functions=[first(a#7, false), first(b#8, false)], output=[a#68, b#70])
   +- Sort [b.c1#63 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(b.c1#63, 200), ENSURE_REQUIREMENTS, [id=#74]
         +- SortAggregate(key=[b.c1#63], functions=[partial_first(a#7, false), partial_first(b#8, false)], output=[b.c1#63, first#75, valueSet#76, first#77, valueSet#78])
            +- Sort [b.c1#63 ASC NULLS FIRST], false, 0
               +- LocalTableScan [a#7, b#8, b.c1#63]

```

### Why are the changes needed?
There is need to provide the the way to use the dropDuplicate with struct Column. In existing code we are getting exception on using the struct Column in the dropDuplicates.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Added the Unit test and also tested using the spark-shell
",https://api.github.com/repos/apache/spark/issues/34849/timeline,,spark,apache,SaurabhChawla100,34540906,MDQ6VXNlcjM0NTQwOTA2,https://avatars.githubusercontent.com/u/34540906?v=4,,https://api.github.com/users/SaurabhChawla100,https://github.com/SaurabhChawla100,https://api.github.com/users/SaurabhChawla100/followers,https://api.github.com/users/SaurabhChawla100/following{/other_user},https://api.github.com/users/SaurabhChawla100/gists{/gist_id},https://api.github.com/users/SaurabhChawla100/starred{/owner}{/repo},https://api.github.com/users/SaurabhChawla100/subscriptions,https://api.github.com/users/SaurabhChawla100/orgs,https://api.github.com/users/SaurabhChawla100/repos,https://api.github.com/users/SaurabhChawla100/events{/privacy},https://api.github.com/users/SaurabhChawla100/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34849,https://github.com/apache/spark/pull/34849,https://github.com/apache/spark/pull/34849.diff,https://github.com/apache/spark/pull/34849.patch,,https://api.github.com/repos/apache/spark/issues/34849/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
121,https://api.github.com/repos/apache/spark/issues/34848,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34848/labels{/name},https://api.github.com/repos/apache/spark/issues/34848/comments,https://api.github.com/repos/apache/spark/issues/34848/events,https://github.com/apache/spark/pull/34848,1075428219,PR_kwDOAQXtWs4vmy2K,34848,"[SPARK-37582][SPARK-37583][SQL] CONTAINS, STARTSWITH, ENDSWITH should support all data type","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-12-09T10:49:56Z,2022-01-25T02:36:04Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In current realization, `contains`, `startsWith` and `endsWith` only support StringType as input, in this pr, we make these three string function support all type, it follow the blow rules:

1.   contains(binary, binary) -> convert to binary contains
2.  for others, convert to string contains and rely on type coercion


### Why are the changes needed?
Make  function `contains`, `startsWith`, `endsWith` support all type.

### Does this PR introduce _any_ user-facing change?
user can use binary as input.


expression | result 
-- | -- 
contains(encode('Spark SQL', 'utf-8'), 'Spark') |  true
startsWith(encode('Spark SQL', 'utf-8'), 'Spark') |  true
contains(encode('Spark SQL', 'utf-8'), 'SparkSQL') |  false
endsWith(encode('Spark SQL', 'utf-8'), 'Spark') |  false
endsWith(encode('Spark SQL', 'utf-8'), 'SQL') |  true





### How was this patch tested?
added UT
",https://api.github.com/repos/apache/spark/issues/34848/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34848,https://github.com/apache/spark/pull/34848,https://github.com/apache/spark/pull/34848.diff,https://github.com/apache/spark/pull/34848.patch,,https://api.github.com/repos/apache/spark/issues/34848/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
122,https://api.github.com/repos/apache/spark/issues/34846,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34846/labels{/name},https://api.github.com/repos/apache/spark/issues/34846/comments,https://api.github.com/repos/apache/spark/issues/34846/events,https://github.com/apache/spark/pull/34846,1075351096,PR_kwDOAQXtWs4vmisE,34846,[SPARK-37593][CORE] Optimize HeapMemoryAllocator to avoid memory waste when using G1GC,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-12-09T09:31:07Z,2022-01-04T20:32:22Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Spark's tungsten memory model usually tries to allocate memory by one `page` each time and allocated by `long[pageSizeBytes/8]` in `HeapMemoryAllocator.allocate`. 

Remember that java long array needs extra object header (usually 16 bytes in 64bit system), so the really bytes allocated is `pageSize+16`.

Assume that the `G1HeapRegionSize` is 4M and `pageSizeBytes` is 4M as well. Since every time we need to allocate 4M+16byte memory, so two regions are used with one region only occupies 16byte. Then there are about **50%** memory waste.
It can happenes under different combinations of G1HeapRegionSize (varies from 1M to 32M) and pageSizeBytes (varies from 1M to 64M).

 We can demo it using following piece of code.

```
public static void bufferSizeTest(boolean optimize) {
    long totalAllocatedSize = 0L;
    int blockSize = 1024 * 1024 * 4; // 4m
    if (optimize) {
      blockSize -= 16;
    }
    List<long[]> buffers = new ArrayList<>();
    while (true) {
      long[] arr = new long[blockSize/8];
      buffers.add(arr);
      totalAllocatedSize += blockSize;
      System.out.println(""Total allocated size: "" + totalAllocatedSize);
    }
  }
```

Run it using following jvm params
```
java -Xmx100m -XX:+UseG1GC -XX:G1HeapRegionSize=4m -XX:-UseGCOverheadLimit -verbose:gc -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xss4m -XX:+ExitOnOutOfMemoryError -XX:ParallelGCThreads=4 -XX:ConcGCThreads=4
```

with optimized = false
```
Total allocated size: 46137344
[GC pause (G1 Humongous Allocation) (young) 44M->44M(100M), 0.0007091 secs]
[GC pause (G1 Evacuation Pause) (young) (initial-mark)-- 48M->48M(100M), 0.0021528 secs]
[GC concurrent-root-region-scan-start]
[GC concurrent-root-region-scan-end, 0.0000021 secs]
[GC concurrent-mark-start]
[GC pause (G1 Evacuation Pause) (young) 48M->48M(100M), 0.0011289 secs]
[Full GC (Allocation Failure)  48M->48M(100M), 0.0017284 secs]
[Full GC (Allocation Failure)  48M->48M(100M), 0.0013437 secs]
Terminating due to java.lang.OutOfMemoryError: Java heap space
```

with optimzied = true
```
Total allocated size: 96468624
[GC pause (G1 Humongous Allocation) (young)-- 92M->92M(100M), 0.0024416 secs]
[Full GC (Allocation Failure)  92M->92M(100M), 0.0019883 secs]
[GC pause (G1 Evacuation Pause) (young) (initial-mark) 96M->96M(100M), 0.0004282 secs]
[GC concurrent-root-region-scan-start]
[GC concurrent-root-region-scan-end, 0.0000040 secs]
[GC concurrent-mark-start]
[GC pause (G1 Evacuation Pause) (young) 96M->96M(100M), 0.0003269 secs]
[Full GC (Allocation Failure)  96M->96M(100M), 0.0012409 secs]
[Full GC (Allocation Failure)  96M->96M(100M), 0.0012607 secs]
Terminating due to java.lang.OutOfMemoryError: Java heap space
```

This PR try to optimize the pageSize to avoid memory waste.

This case exists not only in `MemoryManagement`, but also in other places such as `TorrentBroadcast.blockSize`.  I would like to submit a followup PR if this modification is reasonable.



### Why are the changes needed?
To avoid memory waste in G1 GC


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing UT",https://api.github.com/repos/apache/spark/issues/34846/timeline,,spark,apache,WangGuangxin,1312321,MDQ6VXNlcjEzMTIzMjE=,https://avatars.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34846,https://github.com/apache/spark/pull/34846,https://github.com/apache/spark/pull/34846.diff,https://github.com/apache/spark/pull/34846.patch,,https://api.github.com/repos/apache/spark/issues/34846/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
123,https://api.github.com/repos/apache/spark/issues/34831,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34831/labels{/name},https://api.github.com/repos/apache/spark/issues/34831/comments,https://api.github.com/repos/apache/spark/issues/34831/events,https://github.com/apache/spark/pull/34831,1074005286,PR_kwDOAQXtWs4viKCt,34831,[SPARK-37574][CORE][SHUFFLE] Simplify fetchBlocks w/o retry,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-08T04:42:19Z,2021-12-20T13:37:46Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Simplify code of fetchBlocks w/o retry

### Why are the changes needed?

Simplify code.

The original code added in SPARK-4188, the `RetryingBlockTransferor` looks quite stable now.

#33340 renames `RetryingBlockFetcher` to `RetryingBlockTransferor`, the comment still calls it as `Fetcher`, it's a little misleading.

```
if (maxRetries > 0) {
  // Note this Fetcher will correctly handle maxRetries == 0; we avoid it just in case there's
  // a bug in this code. We should remove the if statement once we're sure of the stability.
  new RetryingBlockTransferor(transportConf, blockFetchStarter, blockIds, listener).start()
} else {
  blockFetchStarter.createAndStart(blockIds, listener)
}
```

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?

Update RetryingBlockTransferorSuite
",https://api.github.com/repos/apache/spark/issues/34831/timeline,,spark,apache,pan3793,26535726,MDQ6VXNlcjI2NTM1NzI2,https://avatars.githubusercontent.com/u/26535726?v=4,,https://api.github.com/users/pan3793,https://github.com/pan3793,https://api.github.com/users/pan3793/followers,https://api.github.com/users/pan3793/following{/other_user},https://api.github.com/users/pan3793/gists{/gist_id},https://api.github.com/users/pan3793/starred{/owner}{/repo},https://api.github.com/users/pan3793/subscriptions,https://api.github.com/users/pan3793/orgs,https://api.github.com/users/pan3793/repos,https://api.github.com/users/pan3793/events{/privacy},https://api.github.com/users/pan3793/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34831,https://github.com/apache/spark/pull/34831,https://github.com/apache/spark/pull/34831.diff,https://github.com/apache/spark/pull/34831.patch,,https://api.github.com/repos/apache/spark/issues/34831/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
124,https://api.github.com/repos/apache/spark/issues/34829,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34829/labels{/name},https://api.github.com/repos/apache/spark/issues/34829/comments,https://api.github.com/repos/apache/spark/issues/34829/events,https://github.com/apache/spark/pull/34829,1073662768,PR_kwDOAQXtWs4vhCjV,34829,[WIP][SPARK-23607][CORE] Use HDFS extended attributes to store application summary information in SHS,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-12-07T18:52:05Z,2021-12-08T04:57:07Z,,CONTRIBUTOR,,False," ### What changes were proposed in this pull request?

 This PR seeks to improve the performance of serving the application list in History Server by storing the required information of the application as part of HDFS extended attributes instead of parsing the log file each time.

 ### Why are the changes needed?

 Improves the performance of the History Server listing page

 ### Does this PR introduce _any_ user-facing change?

 No.

 ### How was this patch tested?
 Will add unit tests",https://api.github.com/repos/apache/spark/issues/34829/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34829,https://github.com/apache/spark/pull/34829,https://github.com/apache/spark/pull/34829.diff,https://github.com/apache/spark/pull/34829.patch,,https://api.github.com/repos/apache/spark/issues/34829/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
125,https://api.github.com/repos/apache/spark/issues/34820,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34820/labels{/name},https://api.github.com/repos/apache/spark/issues/34820/comments,https://api.github.com/repos/apache/spark/issues/34820/events,https://github.com/apache/spark/pull/34820,1072071004,PR_kwDOAQXtWs4vbyRO,34820,[SPARK-37559][SQL] ShuffledRowRDD get preferred locations order by reduce size,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-12-06T12:03:34Z,2021-12-07T10:01:40Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Pass `MapOutputStatistics` to `ShuffledRowRDD` in AQE code path so we can do sort according to the origin reduce partition size.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The coalesced partition can contain several reduce partitions. The preferred locations of the RDD partition should be the biggest reduce partition before coalesced. 

So it can get a better data locality and reduce the network traffic.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added test",https://api.github.com/repos/apache/spark/issues/34820/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34820,https://github.com/apache/spark/pull/34820,https://github.com/apache/spark/pull/34820.diff,https://github.com/apache/spark/pull/34820.patch,,https://api.github.com/repos/apache/spark/issues/34820/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
126,https://api.github.com/repos/apache/spark/issues/34812,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34812/labels{/name},https://api.github.com/repos/apache/spark/issues/34812/comments,https://api.github.com/repos/apache/spark/issues/34812/events,https://github.com/apache/spark/pull/34812,1071588253,PR_kwDOAQXtWs4vaMnC,34812,[SPARK-37553][PYTHON] Fix underscore (`_`) bug in pyspark.pandas.frames.DataFrame.pivot_table,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-05T23:29:55Z,2021-12-21T16:41:07Z,,NONE,,False,"### What changes were proposed in this pull request?
- Adds example code changes to allow for underscores in (1) the elements for the `columns` arg and (2) for the column names used for the `values` arg when `len(values) > 1`. 


### Why are the changes needed?
Fixes a bug with the method `pyspark.pandas.frames.DataFrame.pivot_table` that causes a `KeyError` when an underscore is present (more details in [SPARK-37553](https://issues.apache.org/jira/browse/SPARK-37553)).
```python
>>> import numpy as np
>>> import pandas as pd
>>> from pyspark import pandas as ps
>>> pdf = pd.DataFrame(
        {
            ""a"": [4, 2, 3, 4, 8, 6],
            ""b_b"": [1, 2, 2, 4, 2, 4],
            ""e"": [10, 20, 20, 40, 20, 40],
            ""c"": [1, 2, 9, 4, 7, 4],
            ""d"": [-1, -2, -3, -4, -5, -6],
        },
        index=np.random.rand(6),
    )
>>> psdf = ps.from_pandas(pdf)
>>> psdf.pivot_table(index=[""c""], columns=""a"", values=[""b_b"", ""e""])

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-8-32d5bb0e1166> in <module>
----> 1 psdf.pivot_table(index=[""c""], columns=""a"", values=[""b_b"", ""e""])

~/.pyenv/versions/3.7.9/envs/venv37/lib/python3.7/site-packages/pyspark/pandas/frame.py in pivot_table(self, values, index, columns, aggfunc, fill_value)
   6053                     column_labels = [
   6054                         tuple(list(column_name_to_index[name.split(""_"")[1]]) + [name.split(""_"")[0]])
-> 6055                         for name in data_columns
   6056                     ]
   6057                     column_label_names = (

~/.pyenv/versions/3.7.9/envs/venv37/lib/python3.7/site-packages/pyspark/pandas/frame.py in <listcomp>(.0)
   6053                     column_labels = [
   6054                         tuple(list(column_name_to_index[name.split(""_"")[1]]) + [name.split(""_"")[0]])
-> 6055                         for name in data_columns
   6056                     ]
   6057                     column_label_names = (

KeyError: 'b'
```


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
- [x] Add unit tests for code changes
- [] Build package via Github Actions 
",https://api.github.com/repos/apache/spark/issues/34812/timeline,,spark,apache,oeuf,3674511,MDQ6VXNlcjM2NzQ1MTE=,https://avatars.githubusercontent.com/u/3674511?v=4,,https://api.github.com/users/oeuf,https://github.com/oeuf,https://api.github.com/users/oeuf/followers,https://api.github.com/users/oeuf/following{/other_user},https://api.github.com/users/oeuf/gists{/gist_id},https://api.github.com/users/oeuf/starred{/owner}{/repo},https://api.github.com/users/oeuf/subscriptions,https://api.github.com/users/oeuf/orgs,https://api.github.com/users/oeuf/repos,https://api.github.com/users/oeuf/events{/privacy},https://api.github.com/users/oeuf/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34812,https://github.com/apache/spark/pull/34812,https://github.com/apache/spark/pull/34812.diff,https://github.com/apache/spark/pull/34812.patch,,https://api.github.com/repos/apache/spark/issues/34812/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
127,https://api.github.com/repos/apache/spark/issues/34810,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34810/labels{/name},https://api.github.com/repos/apache/spark/issues/34810/comments,https://api.github.com/repos/apache/spark/issues/34810/events,https://github.com/apache/spark/pull/34810,1071454844,PR_kwDOAQXtWs4vZzII,34810,[SPARK-37549][SQL] Support set parallel through data source properties,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1496047285, 'node_id': 'MDU6TGFiZWwxNDk2MDQ3Mjg1', 'url': 'https://api.github.com/repos/apache/spark/labels/EXAMPLES', 'name': 'EXAMPLES', 'color': 'ededed', 'default': False, 'description': ''}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984361638, 'node_id': 'MDU6TGFiZWwxOTg0MzYxNjM4', 'url': 'https://api.github.com/repos/apache/spark/labels/R', 'name': 'R', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-05T13:05:58Z,2022-01-28T09:02:17Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr add support set parallel through data source properties when reading data. For example:
```scala
spark.read.option(""Parallel"", 1000).parquet(""path/to/parquet"")
```
```sql
CREATE TABLE very_large_partitioned_bucketed_table (
  id STRING,
  foo STRING,
  bar STRING,
  other STRING,
  dt STRING,
  type STRING)
USING parquet
OPTIONS (
  compression 'gzip',
  PARALLEL '12000'
)
PARTITIONED BY (dt, type)
CLUSTERED BY (id)
INTO 6000 BUCKETS
```

Oracle has similar feature:
https://docs.oracle.com/cd/B19306_01/server.102/b14200/clauses006.htm
https://docs.oracle.com/cd/E11882_01/server.112/e25523/parallel002.htm#BEIDFDEH

How to alter table parallelism:
```sql
ALTER TABLE table_name SET TBLPROPERTIES ('parallel' = 'parallel number');
```

### Why are the changes needed?

1. To decrease the degree of parallelism if it is very large partitioned and bucketed table as it is not always use bucket scan since [SPARK-32859](https://issues.apache.org/jira/browse/SPARK-32859).
2. To increase the degree of parallelism on the stream side if it is `BroadcastNestedLoopJoinExec`.
3. To support setting parallel through hint in the future(Oracle has similar feature: https://docs.oracle.com/cd/E11882_01/server.112/e41573/hintsref.htm#CHDJIGDG).

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test.
",https://api.github.com/repos/apache/spark/issues/34810/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34810,https://github.com/apache/spark/pull/34810,https://github.com/apache/spark/pull/34810.diff,https://github.com/apache/spark/pull/34810.patch,,https://api.github.com/repos/apache/spark/issues/34810/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
128,https://api.github.com/repos/apache/spark/issues/34800,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34800/labels{/name},https://api.github.com/repos/apache/spark/issues/34800/comments,https://api.github.com/repos/apache/spark/issues/34800/events,https://github.com/apache/spark/pull/34800,1070684037,PR_kwDOAQXtWs4vXb9K,34800,[SPARK-37538][SQL] Replace single projection expand,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-03T15:06:07Z,2021-12-09T06:24:38Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
In the `Optimizer` replace all instances of `Expand` with only 1 projection with a `Project`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Both grouping sets and distinct aggregations can create `Expand` with only 1 projection. Removing those can improve the performance in two ways:
* Enable optimization rules, that can not work with `Expand`
* Avoid unnecessary copying - `ExpandExec` has `needCopyResult: Boolean = true`

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
New UT",https://api.github.com/repos/apache/spark/issues/34800/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34800,https://github.com/apache/spark/pull/34800,https://github.com/apache/spark/pull/34800.diff,https://github.com/apache/spark/pull/34800.patch,,https://api.github.com/repos/apache/spark/issues/34800/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
129,https://api.github.com/repos/apache/spark/issues/34794,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34794/labels{/name},https://api.github.com/repos/apache/spark/issues/34794/comments,https://api.github.com/repos/apache/spark/issues/34794/events,https://github.com/apache/spark/pull/34794,1070330861,PR_kwDOAQXtWs4vWRh-,34794,[SPARK-37532][CORE] Limit the length of RDD name,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-12-03T08:17:01Z,2021-12-06T03:46:01Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

APIs like sc.newHadoopFile accepts a string representation of comma separate paths, which could be very very long, and we set it directly as the RDD name. This could be an unfriendly name on the UI and cost tons of driver memory for the whole RDD scope.



### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

make the RDD name on UI more friendly
make it cost less driver memory

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, for a single RDD like this, user will see a much short one

![image](https://user-images.githubusercontent.com/8326978/144568404-bbfa9074-52b7-4a2b-a71a-a73719233f4c.png)

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

passing GA
",https://api.github.com/repos/apache/spark/issues/34794/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34794,https://github.com/apache/spark/pull/34794,https://github.com/apache/spark/pull/34794.diff,https://github.com/apache/spark/pull/34794.patch,,https://api.github.com/repos/apache/spark/issues/34794/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
130,https://api.github.com/repos/apache/spark/issues/34791,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34791/labels{/name},https://api.github.com/repos/apache/spark/issues/34791/comments,https://api.github.com/repos/apache/spark/issues/34791/events,https://github.com/apache/spark/pull/34791,1070249492,PR_kwDOAQXtWs4vWAvU,34791,[SPARK-37528][SQL][CORE] Support reorder tasks during scheduling by shuffle partition size in AQE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-12-03T06:01:32Z,2021-12-03T13:34:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
In order to let task know its input size, we need to add a new method in `org.apache.spark.Partition`. Then at SQL side, we can pass the data size into partition before executing the shuffle read stage (thanks to the stage level scheduler in AQE). So, overall the changes include:

- Add a new method `predictedInputBytes` in `org.apache.spark.Partition`
- Pass the data size to `ShuffledRowRDD.getPartitions`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

This PR tries to reorder tasks by `predictedInputBytes` with big first. Assume the larger amount of input data takes longer to execute. It can save the whole stage execution time. Let's say we have one stage with 4 tasks and the `defaultParallelism` is 2 and the 4 tasks have different execution time with [1s, 3s, 2s, 4s].

- in normal, the execution time of the stage is: 7s
- after reorder the tasks, the execution time of the stage is: 5s

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, a new config `spark.scheduler.reorderTasks.enabled` to decide if we allow to reorder tasks.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Add test in:

- org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite
- org.apache.spark.scheduler.DAGSchedulerSuite
",https://api.github.com/repos/apache/spark/issues/34791/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34791,https://github.com/apache/spark/pull/34791,https://github.com/apache/spark/pull/34791.diff,https://github.com/apache/spark/pull/34791.patch,,https://api.github.com/repos/apache/spark/issues/34791/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
131,https://api.github.com/repos/apache/spark/issues/34789,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34789/labels{/name},https://api.github.com/repos/apache/spark/issues/34789/comments,https://api.github.com/repos/apache/spark/issues/34789/events,https://github.com/apache/spark/pull/34789,1070182138,PR_kwDOAQXtWs4vVyty,34789,[SPARK-37519][SQL] Support Relation With LateralView,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-12-03T03:28:41Z,2021-12-07T03:37:54Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Users can more convenient to use `LATERAL VIEW` with relation instead of creating a subqury


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Before this change:
```
SELECT *
FROM ( SELECT CC1.C_AGE1 FROM PERSON1 AS P1
LATERAL VIEW EXPLODE(ARRAY(30, 60)) CC1 AS C_AGE1 ) AS P1
LEFT JOIN PERSON2 P2 ON P1.ID = P2.ID  AND P1.C_AGE1=P2.AGE;
```
After:
```
SELECT *
FROM PERSON1 AS P1
LATERAL VIEW EXPLODE(ARRAY(30, 60)) CC1 AS C_AGE1
LEFT JOIN PERSON2 P2 ON P1.ID = P2.ID  AND CC1.C_AGE1=P2.AGE;
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Add unit tests
",https://api.github.com/repos/apache/spark/issues/34789/timeline,,spark,apache,TongWei1105,68682646,MDQ6VXNlcjY4NjgyNjQ2,https://avatars.githubusercontent.com/u/68682646?v=4,,https://api.github.com/users/TongWei1105,https://github.com/TongWei1105,https://api.github.com/users/TongWei1105/followers,https://api.github.com/users/TongWei1105/following{/other_user},https://api.github.com/users/TongWei1105/gists{/gist_id},https://api.github.com/users/TongWei1105/starred{/owner}{/repo},https://api.github.com/users/TongWei1105/subscriptions,https://api.github.com/users/TongWei1105/orgs,https://api.github.com/users/TongWei1105/repos,https://api.github.com/users/TongWei1105/events{/privacy},https://api.github.com/users/TongWei1105/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34789,https://github.com/apache/spark/pull/34789,https://github.com/apache/spark/pull/34789.diff,https://github.com/apache/spark/pull/34789.patch,,https://api.github.com/repos/apache/spark/issues/34789/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
132,https://api.github.com/repos/apache/spark/issues/34785,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34785/labels{/name},https://api.github.com/repos/apache/spark/issues/34785/comments,https://api.github.com/repos/apache/spark/issues/34785/events,https://github.com/apache/spark/pull/34785,1069914068,PR_kwDOAQXtWs4vU61V,34785,[SPARK-37523][SQL] Support optimize skewed partitions in Distribution and Ordering if numPartitions is not specified,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-12-02T19:36:31Z,2021-12-30T16:52:23Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?
Support optimize skewed partitions in Distribution and Ordering if numPartitions is not specified

### Why are the changes needed?
When doing repartition in distribution and sort, we will use Rebalance operator instead of RepartitionByExpression to optimize skewed partitions when
1. numPartitions is not specified by the data source, and
2. sortOrder is specified. This is because the requested distribution needs to be guaranteed, which can only be achieved by using RangePartitioning, not HashPartitioning.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Existing and new tests
",https://api.github.com/repos/apache/spark/issues/34785/timeline,,spark,apache,huaxingao,13592258,MDQ6VXNlcjEzNTkyMjU4,https://avatars.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34785,https://github.com/apache/spark/pull/34785,https://github.com/apache/spark/pull/34785.diff,https://github.com/apache/spark/pull/34785.patch,,https://api.github.com/repos/apache/spark/issues/34785/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
133,https://api.github.com/repos/apache/spark/issues/34781,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34781/labels{/name},https://api.github.com/repos/apache/spark/issues/34781/comments,https://api.github.com/repos/apache/spark/issues/34781/events,https://github.com/apache/spark/pull/34781,1069459520,PR_kwDOAQXtWs4vTaWA,34781,[WIP] Use error-classes for spark-core errors - 1st batch,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-12-02T11:52:46Z,2022-01-12T06:53:43Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This change is to refactor the 1st batch of errors in spark-core to use error-classes. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This is to follow the error class framework in spark-core.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing unit tests. ",https://api.github.com/repos/apache/spark/issues/34781/timeline,,spark,apache,bozhang2820,44179472,MDQ6VXNlcjQ0MTc5NDcy,https://avatars.githubusercontent.com/u/44179472?v=4,,https://api.github.com/users/bozhang2820,https://github.com/bozhang2820,https://api.github.com/users/bozhang2820/followers,https://api.github.com/users/bozhang2820/following{/other_user},https://api.github.com/users/bozhang2820/gists{/gist_id},https://api.github.com/users/bozhang2820/starred{/owner}{/repo},https://api.github.com/users/bozhang2820/subscriptions,https://api.github.com/users/bozhang2820/orgs,https://api.github.com/users/bozhang2820/repos,https://api.github.com/users/bozhang2820/events{/privacy},https://api.github.com/users/bozhang2820/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34781,https://github.com/apache/spark/pull/34781,https://github.com/apache/spark/pull/34781.diff,https://github.com/apache/spark/pull/34781.patch,,https://api.github.com/repos/apache/spark/issues/34781/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
134,https://api.github.com/repos/apache/spark/issues/34780,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34780/labels{/name},https://api.github.com/repos/apache/spark/issues/34780/comments,https://api.github.com/repos/apache/spark/issues/34780/events,https://github.com/apache/spark/pull/34780,1069445095,PR_kwDOAQXtWs4vTXRU,34780,[SPARK-37517][SQL] Keep consistent order of columns with user specify for v1 table,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-12-02T11:35:55Z,2021-12-17T12:32:33Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
1. keep columns order with user specified instead of put partition columns at last.
2. Modify the `partitionSchema` and `dataSchema` implementation.

### Why are the changes needed?
discuss at [#34719](https://github.com/apache/spark/pull/34719#discussion_r758157813).

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Add test case.
",https://api.github.com/repos/apache/spark/issues/34780/timeline,,spark,apache,Peng-Lei,41178002,MDQ6VXNlcjQxMTc4MDAy,https://avatars.githubusercontent.com/u/41178002?v=4,,https://api.github.com/users/Peng-Lei,https://github.com/Peng-Lei,https://api.github.com/users/Peng-Lei/followers,https://api.github.com/users/Peng-Lei/following{/other_user},https://api.github.com/users/Peng-Lei/gists{/gist_id},https://api.github.com/users/Peng-Lei/starred{/owner}{/repo},https://api.github.com/users/Peng-Lei/subscriptions,https://api.github.com/users/Peng-Lei/orgs,https://api.github.com/users/Peng-Lei/repos,https://api.github.com/users/Peng-Lei/events{/privacy},https://api.github.com/users/Peng-Lei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34780,https://github.com/apache/spark/pull/34780,https://github.com/apache/spark/pull/34780.diff,https://github.com/apache/spark/pull/34780.patch,,https://api.github.com/repos/apache/spark/issues/34780/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
135,https://api.github.com/repos/apache/spark/issues/34779,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34779/labels{/name},https://api.github.com/repos/apache/spark/issues/34779/comments,https://api.github.com/repos/apache/spark/issues/34779/events,https://github.com/apache/spark/pull/34779,1069412780,PR_kwDOAQXtWs4vTQY8,34779,[SPARK-37518][SQL] Inject an early scan pushdown rule,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-12-02T11:01:55Z,2021-12-14T01:22:16Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently, Spark supports push down filters, aggregates and limit. All the job is completed by `V2ScanRelationPushDown`.
But `V2ScanRelationPushDown` have a lot limit.
Users want apply custom rule for push down after `V2ScanRelationPushDown` failed.


### Why are the changes needed?
Easy for users to apply custom pushdown rules.


### Does this PR introduce _any_ user-facing change?
'Yes'.
Users can inject custom early scan pushdown rules.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34779/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34779,https://github.com/apache/spark/pull/34779,https://github.com/apache/spark/pull/34779.diff,https://github.com/apache/spark/pull/34779.patch,,https://api.github.com/repos/apache/spark/issues/34779/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
136,https://api.github.com/repos/apache/spark/issues/34769,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34769/labels{/name},https://api.github.com/repos/apache/spark/issues/34769/comments,https://api.github.com/repos/apache/spark/issues/34769/events,https://github.com/apache/spark/pull/34769,1068338424,PR_kwDOAQXtWs4vPspz,34769,[SPARK-37463][SQL] Read/Write Timestamp ntz to Orc uses UTC timestamp,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-12-01T12:16:54Z,2021-12-01T17:27:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR used to fix the issue
https://github.com/apache/spark/pull/33588#issuecomment-978719988

The root cause is Orc write/read timestamp with local timezone in default. The local timezone will be changed.
If the Orc writer write timestamp with local timezone(e.g. America/Los_Angeles), when the Orc reader reading the timestamp with local timezone(e.g. Europe/Amsterdam), the value of timestamp will be different.

If we let the Orc writer write timestamp with UTC timezone, when the Orc reader reading the timestamp with  UTC timezone too, the value of timestamp will be correct.

This PR let Orc write/read Timestamp with UTC timezone by call `useUTCTimestamp(true)` for readers or writers.

The related Orc source:
https://github.com/apache/orc/blob/3f1e57cf1cebe58027c1bd48c09eef4e9717a9e3/java/core/src/java/org/apache/orc/impl/WriterImpl.java#L525

https://github.com/apache/orc/blob/1f68ac0c7f2ae804b374500dcf1b4d7abe30ffeb/java/core/src/java/org/apache/orc/impl/TreeReaderFactory.java#L1184

Another problem is Spark 3.3 or newer read the Orc file written by Spark 3.2 or prior. Because the older Spark write timestamp with local timezone, no need to read them with UTC timezone. Otherwise, an incorrect value of timestamp occurs.

### Why are the changes needed?
Fix the bug for Orc timestamp.


### Does this PR introduce _any_ user-facing change?
Orc timestamp ntz is a new feature not release yet.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34769/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34769,https://github.com/apache/spark/pull/34769,https://github.com/apache/spark/pull/34769.diff,https://github.com/apache/spark/pull/34769.patch,,https://api.github.com/repos/apache/spark/issues/34769/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
137,https://api.github.com/repos/apache/spark/issues/34765,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34765/labels{/name},https://api.github.com/repos/apache/spark/issues/34765/comments,https://api.github.com/repos/apache/spark/issues/34765/events,https://github.com/apache/spark/pull/34765,1068014301,PR_kwDOAQXtWs4vOn5J,34765,[WIP][SPARK-37487][SQL][CORE] Avoid performing CollectMetrics twice if the operation is followed by global sort.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-12-01T06:26:59Z,2021-12-01T09:26:08Z,,MEMBER,,True,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR fixes an issue that `CollectMetrics` performs twice if it's followed by global sort like as follows.
```
val df = spark.range(100)
  .observe(
    name = ""my_event"",
    min($""id"").as(""min_val""),
    max($""id"").as(""max_val""),
    sum($""id""),
    count(when($""id"" % 2 === 0, 1)).as(""num_even""))
  .sort($""id"".desc)
```

The expected statistics calculated by `CollectMetrics` is `[0,99,4950,50]` but the actual result is `[0,99,9900,100]`.
The reason is that jobs for sampling can run before the global sort, which performs extra `CollectMetrics`.
https://github.com/apache/spark/blob/e7fa28930dce468df02b5915e1792ada758a96e3/core/src/main/scala/org/apache/spark/Partitioner.scala#L171
https://github.com/apache/spark/blob/e7fa28930dce468df02b5915e1792ada758a96e3/core/src/main/scala/org/apache/spark/Partitioner.scala#L195

The solution this PR proposes to introduce a property `spark.job.isSamplingJob` which is intended to be get/set internally.
Before the sampling jobs run, Spark sets the property, and reset it after the jobs finish.
Then, `CollectMetrics` can judge a task is whether of a sampling job or not.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Bug fix.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
New test.",https://api.github.com/repos/apache/spark/issues/34765/timeline,,spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34765,https://github.com/apache/spark/pull/34765,https://github.com/apache/spark/pull/34765.diff,https://github.com/apache/spark/pull/34765.patch,,https://api.github.com/repos/apache/spark/issues/34765/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
138,https://api.github.com/repos/apache/spark/issues/34755,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34755/labels{/name},https://api.github.com/repos/apache/spark/issues/34755/comments,https://api.github.com/repos/apache/spark/issues/34755/events,https://github.com/apache/spark/pull/34755,1066959758,PR_kwDOAQXtWs4vLJ-R,34755,[SPARK-37502][SQL] Support cast aware output partitioning and required if it can up cast,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-11-30T08:53:27Z,2021-11-30T14:23:08Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Enhance semantic equals at `Partitioning` to support cast aware output partitioning and required if it can up cast.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
If a `Cast` is up cast then it should be without any truncating or precision lose or possible runtime failures. So the output partitioning should be same with/without `Cast` if the `Cast` is up cast.

Let's say we have a query:
```sql
-- v1: c1 int
-- v2: c2 long

SELECT * FROM v2 JOIN (SELECT c1, count(*) FROM v1 GROUP BY c1) v1 ON v1.c1 = v2.c2
```

The executed plan contains three shuffle nodes which looks like:
```sql
SortMergeJoin
  Exchange(cast(c1 as bigint))
    HashAggregate
      Exchange(c1)
        Scan v1
  Exchange(c2)
    Scan v2
```

We can simplify the plan using two shuffle nodes:
```sql
SortMergeJoin
  HashAggregate
    Exchange(c1)
      Scan v1
  Exchange(c2)
    Scan v2
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, the plan may be changed

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Add test in:
- org.apache.spark.sql.catalyst.DistributionSuite
- org.apache.spark.sql.SQLQuerySuite",https://api.github.com/repos/apache/spark/issues/34755/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34755,https://github.com/apache/spark/pull/34755,https://github.com/apache/spark/pull/34755.diff,https://github.com/apache/spark/pull/34755.patch,,https://api.github.com/repos/apache/spark/issues/34755/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
139,https://api.github.com/repos/apache/spark/issues/34752,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34752/labels{/name},https://api.github.com/repos/apache/spark/issues/34752/comments,https://api.github.com/repos/apache/spark/issues/34752/events,https://github.com/apache/spark/pull/34752,1066855178,PR_kwDOAQXtWs4vK0gX,34752,[SPARK-37515][STREAMING] minRatePerPartition should be multiplied with secsPerBatch,[],open,False,,[],,2,2021-11-30T06:43:11Z,2021-12-09T06:06:01Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

`maxRatePerPartition` means ""max messages per partition per second"".
But minRatePerPartition does not. (""max messages per partition per a batch""). This is a bug.

minRatePerPartition should be multiplied with secsPerBatch


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

minRatePerPartition will work properly as described docs
 - https://github.com/apache/spark/blob/f97de309792f382ae823894e978f7e54f34f1a29/docs/configuration.md?plain=1#L2878-L2886

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

tested with `org.apache.spark.streaming.kafka010.DirectKafkaStreamSuite`

",https://api.github.com/repos/apache/spark/issues/34752/timeline,,spark,apache,sungpeo,13159599,MDQ6VXNlcjEzMTU5NTk5,https://avatars.githubusercontent.com/u/13159599?v=4,,https://api.github.com/users/sungpeo,https://github.com/sungpeo,https://api.github.com/users/sungpeo/followers,https://api.github.com/users/sungpeo/following{/other_user},https://api.github.com/users/sungpeo/gists{/gist_id},https://api.github.com/users/sungpeo/starred{/owner}{/repo},https://api.github.com/users/sungpeo/subscriptions,https://api.github.com/users/sungpeo/orgs,https://api.github.com/users/sungpeo/repos,https://api.github.com/users/sungpeo/events{/privacy},https://api.github.com/users/sungpeo/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34752,https://github.com/apache/spark/pull/34752,https://github.com/apache/spark/pull/34752.diff,https://github.com/apache/spark/pull/34752.patch,,https://api.github.com/repos/apache/spark/issues/34752/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
140,https://api.github.com/repos/apache/spark/issues/34741,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34741/labels{/name},https://api.github.com/repos/apache/spark/issues/34741/comments,https://api.github.com/repos/apache/spark/issues/34741/events,https://github.com/apache/spark/pull/34741,1065952187,PR_kwDOAQXtWs4vH6i7,34741,[SPARK-37463][SQL] Read/Write Timestamp ntz from/to Orc uses UTC time zone,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-11-29T11:59:41Z,2021-12-28T08:04:39Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
#33588 (comment) show Spark cannot read/write timestamp ntz and ltz correctly. Based on the discussion https://github.com/apache/spark/pull/34712#issuecomment-981402675, we just to fix read/write timestamp ntz to Orc uses UTC timestamp.

The root cause is Orc write/read timestamp with local timezone in default. The local timezone will be changed.
If the Orc writer write timestamp with local timezone(e.g. America/Los_Angeles), when the Orc reader reading the timestamp with other local timezone(e.g. Europe/Amsterdam), the value of timestamp will be different.

If we let the Orc writer write timestamp with UTC timezone, when the Orc reader reading the timestamp with UTC timezone too, the value of timestamp will be correct.

The related Orc source:
https://github.com/apache/orc/blob/3f1e57cf1cebe58027c1bd48c09eef4e9717a9e3/java/core/src/java/org/apache/orc/impl/WriterImpl.java#L525

https://github.com/apache/orc/blob/1f68ac0c7f2ae804b374500dcf1b4d7abe30ffeb/java/core/src/java/org/apache/orc/impl/TreeReaderFactory.java#L1184

### Why are the changes needed?
Fix the bug about read/write timestamp ntz from/to Orc with different times zone.


### Does this PR introduce _any_ user-facing change?
No. Orc timestamp ntz is a new feature not release yet.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34741/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34741,https://github.com/apache/spark/pull/34741,https://github.com/apache/spark/pull/34741.diff,https://github.com/apache/spark/pull/34741.patch,,https://api.github.com/repos/apache/spark/issues/34741/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
141,https://api.github.com/repos/apache/spark/issues/34729,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34729/labels{/name},https://api.github.com/repos/apache/spark/issues/34729/comments,https://api.github.com/repos/apache/spark/issues/34729/events,https://github.com/apache/spark/pull/34729,1065132547,PR_kwDOAQXtWs4vFlnQ,34729,[SPARK-37475][SQL] Add scale parameter to floor and ceil functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-27T22:31:29Z,2022-01-30T20:37:32Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Adds `scale` parameter to `floor`/`ceil` functions in order to allow users to control the rounding position. This feature is proposed in the PR: https://github.com/apache/spark/pull/34593

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently we support Decimal RoundingModes : HALF_UP (round) and HALF_EVEN (bround). But we have use cases that needs RoundingMode.UP and RoundingMode.DOWN.

Floor and Ceil functions helps to do this but it doesn't support the position of the rounding. Adding scale parameter to the functions would help us control the rounding positions. 

Snowflake supports `scale` parameter to `floor`/`ceil` :
` FLOOR( <input_expr> [, <scale_expr> ] )`

REF:
https://docs.snowflake.com/en/sql-reference/functions/floor.html

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Now users can pass `scale` parameter to the `floor` and `ceil` functions.
 ```
     > SELECT floor(-0.1);
       -1.0
      > SELECT floor(5);
       5
      > SELECT floor(3.1411, 3);
       3.141
      > SELECT floor(3.1411, -3);
       1000.0

      > SELECT ceil(-0.1);
       0.0
      > SELECT ceil(5);
       5
      > SELECT ceil(3.1411, 3);
       3.142
      > SELECT ceil(3.1411, -3);
       1000.0

```
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
This patch was tested locally using unit test and git workflow.",https://api.github.com/repos/apache/spark/issues/34729/timeline,,spark,apache,sathiyapk,5880194,MDQ6VXNlcjU4ODAxOTQ=,https://avatars.githubusercontent.com/u/5880194?v=4,,https://api.github.com/users/sathiyapk,https://github.com/sathiyapk,https://api.github.com/users/sathiyapk/followers,https://api.github.com/users/sathiyapk/following{/other_user},https://api.github.com/users/sathiyapk/gists{/gist_id},https://api.github.com/users/sathiyapk/starred{/owner}{/repo},https://api.github.com/users/sathiyapk/subscriptions,https://api.github.com/users/sathiyapk/orgs,https://api.github.com/users/sathiyapk/repos,https://api.github.com/users/sathiyapk/events{/privacy},https://api.github.com/users/sathiyapk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34729,https://github.com/apache/spark/pull/34729,https://github.com/apache/spark/pull/34729.diff,https://github.com/apache/spark/pull/34729.patch,,https://api.github.com/repos/apache/spark/issues/34729/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
142,https://api.github.com/repos/apache/spark/issues/34727,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34727/labels{/name},https://api.github.com/repos/apache/spark/issues/34727/comments,https://api.github.com/repos/apache/spark/issues/34727/events,https://github.com/apache/spark/pull/34727,1065066376,PR_kwDOAQXtWs4vFauv,34727,[SPARK-37467][SQL] Consolidate whole stage and non whole stage subexpression elimination,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-11-27T15:25:13Z,2022-01-28T20:57:02Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR consolidates the code paths for subexpression elimination in whole stage and non-whole stage codegen. Whole stage codegen seemed to be mostly a superset of the non-whole stage subexpression elimination, just with whole stage not using the codegen context to track subexpressions. Since subexpression values are replaced with empty blocks when evaluated, the context should be able to track the subexpressions across multiple operators. Not sure if there's corner cases I'm missing though.

It shouldn't result in any functionality changes, but there are slight differences in the generated code as a result of this:
- Subexpressions in whole stage always use mutable state for results instead of inlining results to support code splitting in non-whole stage
- Non-whole stage now supports the same inlining subexpressions if small enough as whole stage codegen
- Subexpressions are tracked across multiple physical operators in whole stage. They are still only calculated in each operator, but if you happen to have an expression in a later operator that was a subexpression in a previous operator, it will be used in the later operator.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently, there are different code paths to handle subexpression elimination in whole stage and non-whole stage codegen. This makes it harder to add new capabilities to subexpression elimination having to deal with independent code paths.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, just slight changes in generated code.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Existing unit tests.",https://api.github.com/repos/apache/spark/issues/34727/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34727,https://github.com/apache/spark/pull/34727,https://github.com/apache/spark/pull/34727.diff,https://github.com/apache/spark/pull/34727.patch,,https://api.github.com/repos/apache/spark/issues/34727/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
143,https://api.github.com/repos/apache/spark/issues/34725,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34725/labels{/name},https://api.github.com/repos/apache/spark/issues/34725/comments,https://api.github.com/repos/apache/spark/issues/34725/events,https://github.com/apache/spark/pull/34725,1064925232,PR_kwDOAQXtWs4vFLrc,34725,[SPARK-37473][CORE] BypassMergeSortShuffleWriter may loss data when disk is missing however catagory is present,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-27T07:11:55Z,2021-12-29T01:54:31Z,,NONE,,False,"### What changes were proposed in this pull request?
We think it has no data when the segment file not exists when all segment files produced by `BypassMergeSortShuffleWriter` is merging;

However, `file.exists()` may rerurn `false` when then the disk which segment file in on is missing and the root catagory exists; the missing disk only lead `file.exists()` return `false` but no exception. The task will run in peace without current segment file written.

The segment data will be ignored  and leading shuffle data loss.


### Why are the changes needed?
Shuffle may loss partition data leading corretness problems in final data.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
 the task finished as normal without exception when I delete  a partition file before segment files merge， and the final data is incorrect;
The task will retry when apply this patch
",https://api.github.com/repos/apache/spark/issues/34725/timeline,,spark,apache,seayoun,45163307,MDQ6VXNlcjQ1MTYzMzA3,https://avatars.githubusercontent.com/u/45163307?v=4,,https://api.github.com/users/seayoun,https://github.com/seayoun,https://api.github.com/users/seayoun/followers,https://api.github.com/users/seayoun/following{/other_user},https://api.github.com/users/seayoun/gists{/gist_id},https://api.github.com/users/seayoun/starred{/owner}{/repo},https://api.github.com/users/seayoun/subscriptions,https://api.github.com/users/seayoun/orgs,https://api.github.com/users/seayoun/repos,https://api.github.com/users/seayoun/events{/privacy},https://api.github.com/users/seayoun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34725,https://github.com/apache/spark/pull/34725,https://github.com/apache/spark/pull/34725.diff,https://github.com/apache/spark/pull/34725.patch,,https://api.github.com/repos/apache/spark/issues/34725/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
144,https://api.github.com/repos/apache/spark/issues/34709,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34709/labels{/name},https://api.github.com/repos/apache/spark/issues/34709/comments,https://api.github.com/repos/apache/spark/issues/34709/events,https://github.com/apache/spark/pull/34709,1063340697,PR_kwDOAQXtWs4vAeU6,34709,[WIP][SPARK-37259] Add option to unwrap query to support CTE for MSSQL JDBC driver,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-25T09:25:23Z,2021-11-29T15:19:00Z,,NONE,,True,"### What changes were proposed in this pull request?
This PR adds the boolean option 'useRawQuery' to unwrap the query from 'select' statement, used to get schema, as it causes problems with CTE when using MSSQL JDBC driver. When set to 'true', the user has to also provide a schema of result in 'customSchema' option. The obvious downside of this approach is that the user is obligated to fetch schema beforehand when running the query. The advantage is that we are avoiding running query twice just to get schema, and users can run query without modification, unlike the solution described in https://github.com/apache/spark/pull/34693


### Why are the changes needed?
These changes are needed to support of CTE while using MSSQL JDBC


### Does this PR introduce _any_ user-facing change?
Added optiont 'useRawQuery' to JDBC options. Example:
```
JdbcMsSqlDF = (
    spark.read.format(""jdbc"")
    .option(""url"", f""jdbc:sqlserver://{server}:{port};databaseName={database};"")
    .option(""user"", user)
    .option(""password"", password)
    .option(""useRawQuery"", ""true"")  //<----------- Do not wrap the query
    .option(""customSchema"", schema)
    .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"")
    .option(""query"", query)
    .load()
)
```


### How was this patch tested?
The patch was tested manually, unit tests  are pending, it's still WIP.
",https://api.github.com/repos/apache/spark/issues/34709/timeline,,spark,apache,akhalymon-cv,30291478,MDQ6VXNlcjMwMjkxNDc4,https://avatars.githubusercontent.com/u/30291478?v=4,,https://api.github.com/users/akhalymon-cv,https://github.com/akhalymon-cv,https://api.github.com/users/akhalymon-cv/followers,https://api.github.com/users/akhalymon-cv/following{/other_user},https://api.github.com/users/akhalymon-cv/gists{/gist_id},https://api.github.com/users/akhalymon-cv/starred{/owner}{/repo},https://api.github.com/users/akhalymon-cv/subscriptions,https://api.github.com/users/akhalymon-cv/orgs,https://api.github.com/users/akhalymon-cv/repos,https://api.github.com/users/akhalymon-cv/events{/privacy},https://api.github.com/users/akhalymon-cv/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34709,https://github.com/apache/spark/pull/34709,https://github.com/apache/spark/pull/34709.diff,https://github.com/apache/spark/pull/34709.patch,,https://api.github.com/repos/apache/spark/issues/34709/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
145,https://api.github.com/repos/apache/spark/issues/34695,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34695/labels{/name},https://api.github.com/repos/apache/spark/issues/34695/comments,https://api.github.com/repos/apache/spark/issues/34695/events,https://github.com/apache/spark/pull/34695,1062019180,PR_kwDOAQXtWs4u8eC6,34695,[SPARK-32446][CORE] Add percentile distribution REST API & UI of peak memory metrics for all executors ,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,61,2021-11-24T05:28:51Z,2022-01-26T01:52:12Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This pr continue the work of https://github.com/apache/spark/pull/29247 since origin author didn't reply for a long time.
Will add as co-author.

For the whole process of application, user may want to know each executor's peak memory usage to see the Resource utilization. The distribution of all executor's peak memory metrics usage  can help users know whether or not there is a skew/bottleneck among executor resource utilization in a given stage.

We define `activeOnly` and `quantiles` query parameter in the REST API for all executors peak memory metrics distribution:
```
applications/<application_id>/<application_attempt/executorPeakMemoryMetricsDistribution?activeOnly=[true (default) | false]&quantiles=0.05,0.25,0.5,0.75,0.95
```
1. withSummaries: default is false, define whether to show current stage's taskMetricsDistribution and executorMetricsDistribution
2. quantiles: default is `0.0,0.25,0.5,0.75,1.0` only effect when `withSummaries=true`, it define the quantiles we use when calculating metrics distributions.

When withSummaries=true, both task metrics in percentile distribution and executor metrics in percentile distribution are included in the REST API output.  The default value of withSummaries is false, i.e. no metrics percentile distribution will be included in the REST API output.

 

### Why are the changes needed?
Always user care about  executor peak usage distribution, this pr help users understand executor peak memory metrics distributions.


### Does this PR introduce _any_ user-facing change?
User can use below restful API to get all executor's peak memory metrics distribution:
```
applications/<application_id>/<application_attempt>/executorPeakMemoryMetricsDistribution
```

### How was this patch tested?
<img width=""944"" alt=""截屏2022-01-07 下午2 09 09"" src=""https://user-images.githubusercontent.com/46485123/148500063-d8a5d3e8-fac8-4f7a-a6d8-1f917abd651f.png"">


![image](https://user-images.githubusercontent.com/46485123/145008985-cfecba4d-f08f-4d5d-9a2d-1ce184eb1253.png)
",https://api.github.com/repos/apache/spark/issues/34695/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34695,https://github.com/apache/spark/pull/34695,https://github.com/apache/spark/pull/34695.diff,https://github.com/apache/spark/pull/34695.patch,,https://api.github.com/repos/apache/spark/issues/34695/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
146,https://api.github.com/repos/apache/spark/issues/34693,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34693/labels{/name},https://api.github.com/repos/apache/spark/issues/34693/comments,https://api.github.com/repos/apache/spark/issues/34693/events,https://github.com/apache/spark/pull/34693,1061197484,PR_kwDOAQXtWs4u51Ij,34693,[SPARK-37259][SQL] Support CTE and TempTable queries with MSSQL JDBC,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-11-23T12:09:33Z,2021-12-02T18:22:03Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Currently CTE queries from Spark are not supported with MSSQL server via JDBC. This is because MSSQL server doesn't support the nested CTE syntax that Spark builds from the original query (`options.tableOrQuery`) in `JDBCRDD.resolveTable()` and in `JDBCRDD.compute()`.
Unfortunately, it is non-trivial to split an arbitrary query it into ""with"" and ""regular"" clauses in `MsSqlServerDialect`. So instead, I'm proposing a new general JDBC option ""withClause"" that users can use if they have complex queries with CTE:
```
val withClause = ""WITH t AS (SELECT x, y FROM tbl)""
val query = ""SELECT * FROM t WHERE x > 10""
val df = spark.read.format(""jdbc"")
  .option(""url"", jdbcUrl)
  .option(""withClause"", withClause)
  .option(""query"", query)
  .load()
```
This change also works with MSSQL's temp table syntax:
```
val withClause = ""(SELECT * INTO #TempTable FROM (SELECT * FROM tbl WHERE x > 10) t)""
val query = ""SELECT * FROM #TempTable""
val df = spark.read.format(""jdbc"")
  .option(""url"", jdbcUrl)
  .option(""withClause"", withClause)
  .option(""query"", query)
  .load()
```

### Why are the changes needed?
To support CTE queries with MSSQL.

### Does this PR introduce _any_ user-facing change?
Yes, CTE queries are supported form now.

### How was this patch tested?
Added new integration UTs.
",https://api.github.com/repos/apache/spark/issues/34693/timeline,,spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34693,https://github.com/apache/spark/pull/34693,https://github.com/apache/spark/pull/34693.diff,https://github.com/apache/spark/pull/34693.patch,,https://api.github.com/repos/apache/spark/issues/34693/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
147,https://api.github.com/repos/apache/spark/issues/34683,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34683/labels{/name},https://api.github.com/repos/apache/spark/issues/34683/comments,https://api.github.com/repos/apache/spark/issues/34683/events,https://github.com/apache/spark/pull/34683,1060204926,PR_kwDOAQXtWs4u2szv,34683,[SPARK-37283][SQL][FOLLOWUP] Avoid trying to store a table which contains timestamp_ntz types in Hive compatible format,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-22T14:12:12Z,2021-11-24T04:43:36Z,,MEMBER,,False,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is PR is to avoid trying to store a table which contains `timestamp_ntz` types in Hive compatible format.
In the current master, Spark tries to store such a table in Hive compatible format first, but it doesn't support `timestamp_ntz`.
As a result warning is logged like as follows though it's finally stored in Spark specific format.
```
CREATE TABLE tbl1(a TIMESTAMP_NTZ) USING Parquet
...
21/11/22 21:57:18 WARN HiveExternalCatalog: Could not persist `default`.`tbl1` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz' but 'timestamp_ntz' is found.
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)
        at org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)
        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:304)
        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:235)
        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:234)
        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:284)
        at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)
        at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:506)
        at org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:404)
        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:273)
...
```
### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To fix the confusing behavior.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Modified the test added in #34551",https://api.github.com/repos/apache/spark/issues/34683/timeline,,spark,apache,sarutak,4736016,MDQ6VXNlcjQ3MzYwMTY=,https://avatars.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34683,https://github.com/apache/spark/pull/34683,https://github.com/apache/spark/pull/34683.diff,https://github.com/apache/spark/pull/34683.patch,,https://api.github.com/repos/apache/spark/issues/34683/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
148,https://api.github.com/repos/apache/spark/issues/34680,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34680/labels{/name},https://api.github.com/repos/apache/spark/issues/34680/comments,https://api.github.com/repos/apache/spark/issues/34680/events,https://github.com/apache/spark/pull/34680,1059772347,PR_kwDOAQXtWs4u1SIC,34680,[SPARK-37421][PYTHON] Inline type hints for python/pyspark/mllib/evaluation.py,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-22T07:09:51Z,2021-12-30T07:00:40Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for evaluation.py in python/pyspark/mllib/
### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.
### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34680/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34680,https://github.com/apache/spark/pull/34680,https://github.com/apache/spark/pull/34680.diff,https://github.com/apache/spark/pull/34680.patch,,https://api.github.com/repos/apache/spark/issues/34680/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
149,https://api.github.com/repos/apache/spark/issues/34675,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34675/labels{/name},https://api.github.com/repos/apache/spark/issues/34675/comments,https://api.github.com/repos/apache/spark/issues/34675/events,https://github.com/apache/spark/pull/34675,1059522641,PR_kwDOAQXtWs4u0cwy,34675,[SPARK-37433][SQL] Uses TimeZone.getDefault when timeZoneId is None for ZoneAwareExpression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-21T23:36:59Z,2021-11-29T20:42:58Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Calling `timeZoneId.get` on Option[String] leads to `NoSuchElementException: None.get`. This PR matches the value of Option[String] and uses `TimeZone.getDefault.toZoneId` when zoneId is None, this avoid unexpected exceptions to users. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Calling `.get` on Option variable never been a good idea. We can either use a default value or choose to throw a meaningful exception. In this case, TimeZone.getDefault will be a good fit for a default value.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Tested Locally and via Unit Test.",https://api.github.com/repos/apache/spark/issues/34675/timeline,,spark,apache,sathiyapk,5880194,MDQ6VXNlcjU4ODAxOTQ=,https://avatars.githubusercontent.com/u/5880194?v=4,,https://api.github.com/users/sathiyapk,https://github.com/sathiyapk,https://api.github.com/users/sathiyapk/followers,https://api.github.com/users/sathiyapk/following{/other_user},https://api.github.com/users/sathiyapk/gists{/gist_id},https://api.github.com/users/sathiyapk/starred{/owner}{/repo},https://api.github.com/users/sathiyapk/subscriptions,https://api.github.com/users/sathiyapk/orgs,https://api.github.com/users/sathiyapk/repos,https://api.github.com/users/sathiyapk/events{/privacy},https://api.github.com/users/sathiyapk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34675,https://github.com/apache/spark/pull/34675,https://github.com/apache/spark/pull/34675.diff,https://github.com/apache/spark/pull/34675.patch,,https://api.github.com/repos/apache/spark/issues/34675/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
150,https://api.github.com/repos/apache/spark/issues/34672,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34672/labels{/name},https://api.github.com/repos/apache/spark/issues/34672/comments,https://api.github.com/repos/apache/spark/issues/34672/events,https://github.com/apache/spark/pull/34672,1058983769,PR_kwDOAQXtWs4uy3FB,34672,[SPARK-37394][CORE] Skip registering with ESS if a customized shuffle manager is configured,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,28,2021-11-19T23:06:57Z,2022-01-21T19:35:11Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Propose to skip registering with ESS if a customized shuffle manager (Remote Shuffle Service) is configured. Otherwise, when the dynamic allocation is enabled without an external shuffle service in place, the Spark executor still tries to connect to the external shuffle service which gets to a connection refused exception.


### Why are the changes needed?
To get dynamic allocation works with a 3rd party remote shuffle service.


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Test locally on K8s with docker-desktop, DA enabled, no external shuffle service, running Uber's RSS locally.

With the default setting, when I run spark job and it will fail with the following error:

> 21/11/30 07:35:24 INFO BlockManager: Registering executor with local external shuffle service.                                                                                              
21/11/30 07:35:24 ERROR BlockManager: Failed to connect to external shuffle server, will retry 2 more times after waiting 5 seconds...
java.io.IOException: Failed to connect to /10.1.2.201:7337

Then apply this patch to Spark, rebuild Spark, and make the changes like the following in the ShuffleManager implementation, i.e `RssShuffleManager.scala`

```
diff --git a/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala b/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala
index 4b6e825..aeffd7d 100644
--- a/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala
+++ b/src/main/scala/org/apache/spark/shuffle/RssShuffleManager.scala
@@ -442,5 +442,8 @@ class RssShuffleManager(conf: SparkConf) extends ShuffleManager with Logging {
       }
     new ServerConnectionCacheUpdateRefresher(serverConnectionResolver, MultiServerHeartbeatClient.getServerCache)
   }
+
+  override def supportExternalShuffleService: Boolean = false
+
```
restart RSS, and rerun the Spark job, the same job can be successfully completed.
",https://api.github.com/repos/apache/spark/issues/34672/timeline,,spark,apache,yangwwei,14214570,MDQ6VXNlcjE0MjE0NTcw,https://avatars.githubusercontent.com/u/14214570?v=4,,https://api.github.com/users/yangwwei,https://github.com/yangwwei,https://api.github.com/users/yangwwei/followers,https://api.github.com/users/yangwwei/following{/other_user},https://api.github.com/users/yangwwei/gists{/gist_id},https://api.github.com/users/yangwwei/starred{/owner}{/repo},https://api.github.com/users/yangwwei/subscriptions,https://api.github.com/users/yangwwei/orgs,https://api.github.com/users/yangwwei/repos,https://api.github.com/users/yangwwei/events{/privacy},https://api.github.com/users/yangwwei/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34672,https://github.com/apache/spark/pull/34672,https://github.com/apache/spark/pull/34672.diff,https://github.com/apache/spark/pull/34672.patch,,https://api.github.com/repos/apache/spark/issues/34672/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
151,https://api.github.com/repos/apache/spark/issues/34665,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34665/labels{/name},https://api.github.com/repos/apache/spark/issues/34665/comments,https://api.github.com/repos/apache/spark/issues/34665/events,https://github.com/apache/spark/pull/34665,1058208995,PR_kwDOAQXtWs4uwXRW,34665,[SPARK-37383][SQL]Print the parsing time for each phase of a SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-19T07:05:27Z,2021-11-19T10:43:31Z,,NONE,,False,"### What changes were proposed in this pull request?
The PR is proposed to:
print out the timeSpent for each phase of a SQL and the total timeSpent of the SQL for parsing

### Why are the changes needed?
the time spent for each parsing phase of a SQL is counted and recorded in QueryPlanningTracker ,  but it is not yet shown anywhere.
when SQL parsing is suspected to be slow, we cannot confirm which phase is slow，therefore, it is necessary to print out the SQL parsing time.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests.
",https://api.github.com/repos/apache/spark/issues/34665/timeline,,spark,apache,caican00,94670132,U_kgDOBaSNNA,https://avatars.githubusercontent.com/u/94670132?v=4,,https://api.github.com/users/caican00,https://github.com/caican00,https://api.github.com/users/caican00/followers,https://api.github.com/users/caican00/following{/other_user},https://api.github.com/users/caican00/gists{/gist_id},https://api.github.com/users/caican00/starred{/owner}{/repo},https://api.github.com/users/caican00/subscriptions,https://api.github.com/users/caican00/orgs,https://api.github.com/users/caican00/repos,https://api.github.com/users/caican00/events{/privacy},https://api.github.com/users/caican00/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34665,https://github.com/apache/spark/pull/34665,https://github.com/apache/spark/pull/34665.diff,https://github.com/apache/spark/pull/34665.patch,,https://api.github.com/repos/apache/spark/issues/34665/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
152,https://api.github.com/repos/apache/spark/issues/34659,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34659/labels{/name},https://api.github.com/repos/apache/spark/issues/34659/comments,https://api.github.com/repos/apache/spark/issues/34659/events,https://github.com/apache/spark/pull/34659,1058061822,PR_kwDOAQXtWs4uv4ZW,34659,[SPARK-34863][SQL] Support complex types for Parquet vectorized reader,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2021-11-19T02:30:35Z,2022-01-25T19:24:26Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR adds support for complex types (e.g., list, map, array) for Spark's vectorized Parquet reader. In particular, this introduces the following changes:
1. Added a new class `ParquetColumnVector` which encapsulates all the necessary information needed when reading a Parquet column, including the `ParquetColumn` for the Parquet column, the repetition & definition levels (only allocated for a leaf-node of a complex type), as well as the reader for the column. In addition, it also contains logic for assembling nested columnar batches, via interpreting Parquet repetition & definition levels. 
2. Changes are made in `VectorizedParquetRecordReader` to initialize a list of `ParquetColumnVector` for the columns read.
3. `VectorizedColumnReader` now also creates a reader for repetition column. Depending on whether maximum repetition level is 0, the batch read is now split into two code paths, e.g., `readBatch` versus `readBatchNested`.
4. Added logic to handle complex type in `VectorizedRleValuesReader`. For data types involving only struct or primitive types, it still goes with the old `readBatch` method which now also saves definition levels into a vector for later assembly. Otherwise, for data types involving array or map, a separate code path `readBatchNested` is introduced to handle repetition levels.
5. Modified `WritableColumnVector` to better support null structs. Currently it requires populating null entries to all child vectors when there is a null struct, however this will waste space and also doesn't work well with Parquet scan. This adds an extra field `structOffsets` which records the mapping from a row ID to the position of the row in the child vector, so that child vectors will only need to store real null elements.
This PR also introduced a new flag `spark.sql.parquet.enableNestedColumnVectorizedReader` which turns the feature on or off. By default it is on to facilitates all the Parquet related test coverage.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

Whenever read schema containing complex types, at the moment Spark will fallback to the row-based reader in parquet-mr, which is much slower. As benchmark shows, by adding support into the vectorized reader, we can get ~15x on average speed up on reading struct fields, and ~1.5x when reading array of struct and map.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

With the PR Spark should now support reading complex types in its vectorized Parquet reader. A new config `spark.sql.parquet.enableNestedColumnVectorizedReader` is introduced to turn the feature on or off.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Added new unit tests.",https://api.github.com/repos/apache/spark/issues/34659/timeline,,spark,apache,sunchao,506679,MDQ6VXNlcjUwNjY3OQ==,https://avatars.githubusercontent.com/u/506679?v=4,,https://api.github.com/users/sunchao,https://github.com/sunchao,https://api.github.com/users/sunchao/followers,https://api.github.com/users/sunchao/following{/other_user},https://api.github.com/users/sunchao/gists{/gist_id},https://api.github.com/users/sunchao/starred{/owner}{/repo},https://api.github.com/users/sunchao/subscriptions,https://api.github.com/users/sunchao/orgs,https://api.github.com/users/sunchao/repos,https://api.github.com/users/sunchao/events{/privacy},https://api.github.com/users/sunchao/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34659,https://github.com/apache/spark/pull/34659,https://github.com/apache/spark/pull/34659.diff,https://github.com/apache/spark/pull/34659.patch,,https://api.github.com/repos/apache/spark/issues/34659/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
153,https://api.github.com/repos/apache/spark/issues/34647,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34647/labels{/name},https://api.github.com/repos/apache/spark/issues/34647/comments,https://api.github.com/repos/apache/spark/issues/34647/events,https://github.com/apache/spark/pull/34647,1057351837,PR_kwDOAQXtWs4utujo,34647,[SPARK-36180][SQL] Support TimestampNTZ type in Hive,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2021-11-18T13:22:12Z,2022-01-19T02:46:05Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Spark not supports TimestampNTZ type in Hive.

```
[info] Caused by: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz:timestamp' but 'timestamp_ntz' is found.[info] Caused by: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'timestamp_ntz:timestamp' but 'timestamp_ntz' is found.
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:372)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:355)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseType(TypeInfoUtils.java:416)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.parseTypeInfos(TypeInfoUtils.java:329)
[info]  at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.getTypeInfosFromTypeString(TypeInfoUtils.java:814)
[info]  at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.extractColumnInfo(LazySerDeParameters.java:162)[info]  at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.<init>(LazySerDeParameters.java:91)
[info]  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initialize(LazySimpleSerDe.java:116)
[info]  at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54)
[info]  at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)
[info]  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:453)
[info]  at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:440)
[info]  at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)
[info]  at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:199)
[info]  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:842)
...
```

Hive only providers the timestamp type, so Spark should write both timestamp_ltz and timestamp_ntz as Hive' timestamp.
When Spark read schema form Hive, We should restore the timestamp_ntz type.

### Why are the changes needed?
The hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.
FYI, In hive 3.0, the will be a timestamp with local timezone added.


### Does this PR introduce _any_ user-facing change?
'No'. timestamp_ntz is new and not public yet


### How was this patch tested?
New test
",https://api.github.com/repos/apache/spark/issues/34647/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34647,https://github.com/apache/spark/pull/34647,https://github.com/apache/spark/pull/34647.diff,https://github.com/apache/spark/pull/34647.patch,,https://api.github.com/repos/apache/spark/issues/34647/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
154,https://api.github.com/repos/apache/spark/issues/34640,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34640/labels{/name},https://api.github.com/repos/apache/spark/issues/34640/comments,https://api.github.com/repos/apache/spark/issues/34640/events,https://github.com/apache/spark/pull/34640,1056864144,PR_kwDOAQXtWs4usHyb,34640,[SPARK-31585][SQL] Introduce Z-order expression,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-11-18T02:27:33Z,2021-12-01T06:44:58Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR is to introduce a new expression in Spark - `ZOrder`. The motivation is Z-order enables to sort tuples in a way, to allow efficiently data skipping for columnar file format (Parquet and ORC).

For query with filter on combination of multiple columns, example:

```sql
SELECT *
FROM table
WHERE x = 0 OR y = 0
```

Parquet/ORC cannot skip file/row-groups efficiently when reading, even though the table is sorted (locally or globally) on any columns. However when table is Z-order sorted on multiple columns, Parquet/ORC can skip file/row-groups efficiently when reading. We should add the feature in Spark to allow OSS Spark users benefitted in running these queries.

With this PR, user can do Z-order sort when writing the table with followed syntax:

```sql
INSERT INTO t
SELECT ...
FROM ...
SORT BY ZORDER(x, y, ...)
```

or

```sql
INSERT INTO t
SELECT ...
FROM ...
ORDER BY ZORDER(x, y, ...)
```

Then when reading the table with filter on `x` and `y`, the performance can be improved by skipping more files and row-groups. More details below for micro benchmark.

This PR adds the support for Z-order on integer types (byte, short, int, and long). For other data types such as float and string will be added as followup. Code-gen support for expression will be also added later.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To improve the query performance when filtering on multiple columns. Seeing 1x-6x run-time improvement in micro benchmark below.

```scala
override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {
    def prepareTable(dir: File, numRows: Int): Unit = {
      import spark.implicits._
      val df = spark.range(numRows).map(_ => (Random.nextLong, Random.nextLong))
        .toDF(""x"", ""y"")
      val zorderedDf = df.sort(Column(ZOrder(Seq($""x"".expr, $""y"".expr))))

      saveAsTable(df, dir, """")
      saveAsTable(zorderedDf, dir, ""ZOrder"")
    }

    def saveAsTable(df: DataFrame, dir: File, suffix: String): Unit = {
      val blockSize = org.apache.parquet.hadoop.ParquetWriter.DEFAULT_PAGE_SIZE
      val orcPath = dir.getCanonicalPath + ""/orc"" + suffix
      val parquetPath = dir.getCanonicalPath + ""/parquet"" + suffix

      df.write.mode(""overwrite"")
        .option(""orc.dictionary.key.threshold"", 0.8)
        .option(""orc.compress.size"", blockSize)
        .option(""orc.stripe.size"", blockSize).orc(orcPath)
      spark.read.orc(orcPath).createOrReplaceTempView(""orcTable"" + suffix)

      df.write.mode(""overwrite"")
        .option(""parquet.block.size"", blockSize).parquet(parquetPath)
      spark.read.parquet(parquetPath).createOrReplaceTempView(""parquetTable"" + suffix)
    }

    def withTempTable(tableNames: String*)(f: => Unit): Unit = {
      try f finally tableNames.foreach(spark.catalog.dropTempView)
    }

    runBenchmark(s""ZOrder"") {
      withTempPath { dir =>
        withTempTable(""orcTable"", ""parquetTable"", ""orcTableZOrder"", ""parquetTableZOrder"") {
          prepareTable(dir, 1024 * 1024 * 15)
          val benchmark = new Benchmark(""zorder"", 1024 * 1024 * 15,
            minNumIters = 5, output = output)

          benchmark.addCase(""Parquet no sort"") { _ =>
            spark.sql(s""SELECT * FROM parquetTable WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.addCase(""Parquet z-order sort on (x, y)"") { _ =>
            spark.sql(s""SELECT * FROM parquetTableZOrder WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.addCase(""ORC no sort"") { _ =>
            spark.sql(s""SELECT * FROM orcTable WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.addCase(""ORC z-order sort on (x, y)"") { _ =>
            spark.sql(s""SELECT * FROM orcTableZOrder WHERE x = 0 OR y = 0"").noop()
          }

          benchmark.run()
        }
      }
    }
  }
```

* Compare the performance between reading table having no sort, and table having local Z-order sort on `(x, y)`.
Seeing 6x run-time improvement for Parquet, and 1x for ORC:

```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Mac OS X 10.16
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
zorder:                                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Parquet no sort                                     274            287          11         57.5          17.4       1.0X
Parquet z-order sort on (x, y)                       37             41           2        420.2           2.4       7.3X
ORC no sort                                         674            754          47         23.3          42.8       0.4X
ORC z-order sort on (x, y)                          262            282          11         59.9          16.7       1.0X
```

* Compare the performance between reading table having no sort, and table having local sort on `(x, y)`.
No performance improvement as expected.

```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Mac OS X 10.16
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
zorder:                                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Parquet no sort                                     285            319          29         55.1          18.1       1.0X
Parquet sort on (x, y)                              278            290          10         56.5          17.7       1.0X
ORC no sort                                         823            842          21         19.1          52.4       0.3X
ORC sort on (x, y)                                  748            760          16         21.0          47.6       0.4X
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. The added expression can be used by user - `zorder`.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added unit test in `ZOrderExpressionSuite.scala`.",https://api.github.com/repos/apache/spark/issues/34640/timeline,,spark,apache,c21,4629931,MDQ6VXNlcjQ2Mjk5MzE=,https://avatars.githubusercontent.com/u/4629931?v=4,,https://api.github.com/users/c21,https://github.com/c21,https://api.github.com/users/c21/followers,https://api.github.com/users/c21/following{/other_user},https://api.github.com/users/c21/gists{/gist_id},https://api.github.com/users/c21/starred{/owner}{/repo},https://api.github.com/users/c21/subscriptions,https://api.github.com/users/c21/orgs,https://api.github.com/users/c21/repos,https://api.github.com/users/c21/events{/privacy},https://api.github.com/users/c21/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34640,https://github.com/apache/spark/pull/34640,https://github.com/apache/spark/pull/34640.diff,https://github.com/apache/spark/pull/34640.patch,,https://api.github.com/repos/apache/spark/issues/34640/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
155,https://api.github.com/repos/apache/spark/issues/34637,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34637/labels{/name},https://api.github.com/repos/apache/spark/issues/34637/comments,https://api.github.com/repos/apache/spark/issues/34637/events,https://github.com/apache/spark/pull/34637,1056786558,PR_kwDOAQXtWs4ur3s3,34637,Spark-37349 add SQL Rest API parsing logic,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-11-18T00:27:25Z,2022-01-02T04:31:28Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Following up on https://issues.apache.org/jira/browse/SPARK-31440, values like
`""value"" : ""total (min, med, max (stageId: taskId))\n177.0 B (59.0 B, 59.0 B, 59.0 B (stage 1.0: task 5))""` are currently shown from Rest API calls which are not easily digested in its current form.New processing logic of the values is introduced along with the creation of the following class in the SQL Rest API to organize the metric values: 
```
case class Value private[spark] (stageId: Option[String] = None, taskId: Option[String] = None,
                                 amount: Option[String] = None, min: Option[String] = None,
                                 med: Option[String] = None, max: Option[String] = None)
```
Which after processing, would make the output look like 
`{
      ""value"" : {
        ""stageId"" : ""1.0"",
        ""taskId"" : ""5"",
        ""amount"" : ""177.0 B"",
        ""min"" : ""59.0 B"",
        ""med"" : ""59.0 B"",
        ""max"" : ""59.0 B""
      }`

Currently not in the PR but could be added if there is interest is the normalization of metrics for aggregation purposes such as the following:
- The conversion of hour, minute and second time units to milliseconds.
- PB,TB, GB, MB, KB units are converted to Bytes.
- Comma is removed from Comma formatted Long values (e.g: 8389632)

<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To organize and process new metric fields in a more user friendly manner.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes, see output below which are gathered from `Check Sql Rest Api Endpoints` Unit Test in SqlResourceWithActualMetricsSuite.scala with AQE set to true.
Before Changes:
[BeforeSpark37349UT.txt](https://github.com/apache/spark/files/7566623/BeforeSpark37349UT.txt)
After changes:
[AfterSpark37349UT.txt](https://github.com/apache/spark/files/7566624/AfterSpark37349UT.txt)

Backward Compatibility:
API Changes were made in `sql/core/src/main/scala/org/apache/spark/status/api/v1/sql/api.scala`.
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Added new Unit Test, manual testing locally.",https://api.github.com/repos/apache/spark/issues/34637/timeline,,spark,apache,yliou,16739760,MDQ6VXNlcjE2NzM5NzYw,https://avatars.githubusercontent.com/u/16739760?v=4,,https://api.github.com/users/yliou,https://github.com/yliou,https://api.github.com/users/yliou/followers,https://api.github.com/users/yliou/following{/other_user},https://api.github.com/users/yliou/gists{/gist_id},https://api.github.com/users/yliou/starred{/owner}{/repo},https://api.github.com/users/yliou/subscriptions,https://api.github.com/users/yliou/orgs,https://api.github.com/users/yliou/repos,https://api.github.com/users/yliou/events{/privacy},https://api.github.com/users/yliou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34637,https://github.com/apache/spark/pull/34637,https://github.com/apache/spark/pull/34637.diff,https://github.com/apache/spark/pull/34637.patch,,https://api.github.com/repos/apache/spark/issues/34637/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
156,https://api.github.com/repos/apache/spark/issues/34636,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34636/labels{/name},https://api.github.com/repos/apache/spark/issues/34636/comments,https://api.github.com/repos/apache/spark/issues/34636/events,https://github.com/apache/spark/pull/34636,1056619938,PR_kwDOAQXtWs4urT7N,34636,[WIP][SPARK-37359][K8S] Cleanup the Spark Kubernetes Integration tests,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,22,2021-11-17T20:50:29Z,2021-12-20T09:59:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Drop remove reason stats check to make the test less flaky & disable Spark R test [hasn't passed for a long time]
Try and avoid PV/PVC delete/create race condition

### Why are the changes needed?

Or K8s test suite is broken so people ignore it. This is not good.

Listener bus message is not always delivered and printed
SparkR tests have been broken for a long time and I don't see any interest in fixing them
PV/PVC creation/deletion can have a race condition during integration tests.


### Does this PR introduce _any_ user-facing change?

Test only change

### How was this patch tested?

WIP (waiting on CI for k8s int).",https://api.github.com/repos/apache/spark/issues/34636/timeline,,spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34636,https://github.com/apache/spark/pull/34636,https://github.com/apache/spark/pull/34636.diff,https://github.com/apache/spark/pull/34636.patch,,https://api.github.com/repos/apache/spark/issues/34636/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
157,https://api.github.com/repos/apache/spark/issues/34629,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34629/labels{/name},https://api.github.com/repos/apache/spark/issues/34629/comments,https://api.github.com/repos/apache/spark/issues/34629/events,https://github.com/apache/spark/pull/34629,1055872510,PR_kwDOAQXtWs4upAwz,34629,[SPARK-37355][CORE]Avoid Block Manager registrations when Executor is shutting down,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-11-17T09:00:01Z,2021-12-23T05:41:07Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Avoid BlockManager registrations when executor is shutting down.

### Why are the changes needed?

The block manager should not do re-register if the executor is shutting down by driver.


### Does this PR introduce _any_ user-facing change?

No

### How was this patch tested?

Existing tests.
",https://api.github.com/repos/apache/spark/issues/34629/timeline,,spark,apache,wankunde,3626747,MDQ6VXNlcjM2MjY3NDc=,https://avatars.githubusercontent.com/u/3626747?v=4,,https://api.github.com/users/wankunde,https://github.com/wankunde,https://api.github.com/users/wankunde/followers,https://api.github.com/users/wankunde/following{/other_user},https://api.github.com/users/wankunde/gists{/gist_id},https://api.github.com/users/wankunde/starred{/owner}{/repo},https://api.github.com/users/wankunde/subscriptions,https://api.github.com/users/wankunde/orgs,https://api.github.com/users/wankunde/repos,https://api.github.com/users/wankunde/events{/privacy},https://api.github.com/users/wankunde/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34629,https://github.com/apache/spark/pull/34629,https://github.com/apache/spark/pull/34629.diff,https://github.com/apache/spark/pull/34629.patch,,https://api.github.com/repos/apache/spark/issues/34629/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
158,https://api.github.com/repos/apache/spark/issues/34623,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34623/labels{/name},https://api.github.com/repos/apache/spark/issues/34623/comments,https://api.github.com/repos/apache/spark/issues/34623/events,https://github.com/apache/spark/pull/34623,1055660235,PR_kwDOAQXtWs4uoUJn,34623,[SPARK-37347][SQL] Spark Thrift Server (STS) driver fullFC becourse of timeoutExecutor not shutdown correctly,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-17T03:22:26Z,2021-11-19T08:51:50Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add timeoutExecutor shutdown method in SparkExecuteStatementOperation and shut down when statement finished or error.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
When spark.sql.thriftServer.queryTimeout or java.sql.Statement.setQueryTimeout is setted >0 , SparkExecuteStatementOperation add timeoutExecutor to kill time-consumed query in SPARK-26533. But timeoutExecutor is not shutdown correctly when statement is finished, it can only be shutdown when timeout. When we set timeout to a long time for example 1 hour, the long-running STS driver will FullGC and the application is not available for a long time.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
No",https://api.github.com/repos/apache/spark/issues/34623/timeline,,spark,apache,lk246,39769581,MDQ6VXNlcjM5NzY5NTgx,https://avatars.githubusercontent.com/u/39769581?v=4,,https://api.github.com/users/lk246,https://github.com/lk246,https://api.github.com/users/lk246/followers,https://api.github.com/users/lk246/following{/other_user},https://api.github.com/users/lk246/gists{/gist_id},https://api.github.com/users/lk246/starred{/owner}{/repo},https://api.github.com/users/lk246/subscriptions,https://api.github.com/users/lk246/orgs,https://api.github.com/users/lk246/repos,https://api.github.com/users/lk246/events{/privacy},https://api.github.com/users/lk246/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34623,https://github.com/apache/spark/pull/34623,https://github.com/apache/spark/pull/34623.diff,https://github.com/apache/spark/pull/34623.patch,,https://api.github.com/repos/apache/spark/issues/34623/reactions,2,0,0,0,0,0,2,0,0,,,,,,,,,,,,,,,,,,
159,https://api.github.com/repos/apache/spark/issues/34622,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34622/labels{/name},https://api.github.com/repos/apache/spark/issues/34622/comments,https://api.github.com/repos/apache/spark/issues/34622/events,https://github.com/apache/spark/pull/34622,1055256977,PR_kwDOAQXtWs4unH1x,34622,[SPARK-37340][UI] Display StageIds in Operators for SQL UI,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-11-16T19:45:10Z,2022-01-02T04:33:49Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Add explicit stageId to operator mapping in the Spark UI that is a more general version of https://issues.apache.org/jira/browse/SPARK-30209, where a stageId-> operator mapping is done with the following algorithm.
 1. Read SparkGraph to get every Node's name and respective AccumulatorIDs.
 2. Gets each stage's AccumulatorIDs.
 3. Maps Operators to stages by checking for non-zero intersection of Step 1 and 2's AccumulatorIDs.
 4. Connect SparkGraphNodes to respective StageIDs for rendering in SQL UI.
As a result, some operators without max metrics values will also have stageIds in the UI. In some cases, there is no operator->StageID mapping made because no stageIds have accumulatorIds that are a part of the Operator's accumulatorIds. URL links at the top to go to the succeeded jobs and completed stages that were executed as a part of the selected query are also provided.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Makes for easier and quicker debugging and navigation.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
Yes, `Succeeded Jobs:` and `Completed Stages:`listed at the top of the UI, along with `Stages:` in some of the operators.
<img width=""697"" alt=""Screen Shot 2021-11-16 at 11 35 51 AM"" src=""https://user-images.githubusercontent.com/16739760/142054791-8229d142-41cd-4706-a53e-7abb51e5901c.png"">

<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
Manual test locally in SQL UI.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
",https://api.github.com/repos/apache/spark/issues/34622/timeline,,spark,apache,yliou,16739760,MDQ6VXNlcjE2NzM5NzYw,https://avatars.githubusercontent.com/u/16739760?v=4,,https://api.github.com/users/yliou,https://github.com/yliou,https://api.github.com/users/yliou/followers,https://api.github.com/users/yliou/following{/other_user},https://api.github.com/users/yliou/gists{/gist_id},https://api.github.com/users/yliou/starred{/owner}{/repo},https://api.github.com/users/yliou/subscriptions,https://api.github.com/users/yliou/orgs,https://api.github.com/users/yliou/repos,https://api.github.com/users/yliou/events{/privacy},https://api.github.com/users/yliou/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34622,https://github.com/apache/spark/pull/34622,https://github.com/apache/spark/pull/34622.diff,https://github.com/apache/spark/pull/34622.patch,,https://api.github.com/repos/apache/spark/issues/34622/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
160,https://api.github.com/repos/apache/spark/issues/34616,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34616/labels{/name},https://api.github.com/repos/apache/spark/issues/34616/comments,https://api.github.com/repos/apache/spark/issues/34616/events,https://github.com/apache/spark/pull/34616,1054717965,PR_kwDOAQXtWs4ulWaO,34616,[SPARK-37344][SQL][DOC] spark-sql cli will keep `\` when match `\;` after spark3,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-11-16T10:38:46Z,2021-12-20T03:31:01Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Before Spark 3, if we pass a SQL like `select split('Spark SQL', '\;')`  to spark-sql, after process, it will actually execute  `select split('Spark SQL', ';')`.

spark-sql with verbose log:
```
[info]   2021-11-16 18:05:34.86 - stdout> spark-sql> select split('dawdawdawd','\;');
[info]   2021-11-16 18:05:34.875 - stdout> select split('dawdawdawd',';')
```

But after Spark 3 It will execute  `select split('Spark SQL', '\;')`
spark-sql with verbose log:
```
[info]   2021-11-16 18:05:34.86 - stdout> spark-sql> select split('dawdawdawd','\;');
[info]   2021-11-16 18:05:34.875 - stdout> select split('dawdawdawd','\;')
```


In this PR we doc this change.

This change is caused by hive commit : https://github.com/apache/hive/commit/65a65826a0d351a3d918bdb98595bdd106d37adb#diff-79277c3cfeb5bc38066fbbe2b90dcee5c870100b8b1e106d53ed0d56817bd0ee

Related JIRA ID : https://issues.apache.org/jira/browse/HIVE-15297 


### Why are the changes needed?
Update migration guide


### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?

Not need

![image](https://user-images.githubusercontent.com/46485123/142100077-a3c9151d-4a8d-4817-874a-c28dd03131ff.png)
",https://api.github.com/repos/apache/spark/issues/34616/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34616,https://github.com/apache/spark/pull/34616,https://github.com/apache/spark/pull/34616.diff,https://github.com/apache/spark/pull/34616.patch,,https://api.github.com/repos/apache/spark/issues/34616/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
161,https://api.github.com/repos/apache/spark/issues/34604,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34604/labels{/name},https://api.github.com/repos/apache/spark/issues/34604/comments,https://api.github.com/repos/apache/spark/issues/34604/events,https://github.com/apache/spark/pull/34604,1053756296,PR_kwDOAQXtWs4uiZ84,34604,[SPARK-37329][YARN] File system delegation tokens are leaked,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-15T14:54:15Z,2022-01-06T09:57:02Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Explicitly cancel the delegation token that's not taken care of by YARN.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Leaking file system delegation tokens create burden for the file system components (for example, KMS), and in the worst case, cause performance regression or even making FS inaccessible.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Manually tested on a small cluster, verify the kms delegation tokens are created and cancelled properly.",https://api.github.com/repos/apache/spark/issues/34604/timeline,,spark,apache,jojochuang,2691807,MDQ6VXNlcjI2OTE4MDc=,https://avatars.githubusercontent.com/u/2691807?v=4,,https://api.github.com/users/jojochuang,https://github.com/jojochuang,https://api.github.com/users/jojochuang/followers,https://api.github.com/users/jojochuang/following{/other_user},https://api.github.com/users/jojochuang/gists{/gist_id},https://api.github.com/users/jojochuang/starred{/owner}{/repo},https://api.github.com/users/jojochuang/subscriptions,https://api.github.com/users/jojochuang/orgs,https://api.github.com/users/jojochuang/repos,https://api.github.com/users/jojochuang/events{/privacy},https://api.github.com/users/jojochuang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34604,https://github.com/apache/spark/pull/34604,https://github.com/apache/spark/pull/34604.diff,https://github.com/apache/spark/pull/34604.patch,,https://api.github.com/repos/apache/spark/issues/34604/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
162,https://api.github.com/repos/apache/spark/issues/34602,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34602/labels{/name},https://api.github.com/repos/apache/spark/issues/34602/comments,https://api.github.com/repos/apache/spark/issues/34602/events,https://github.com/apache/spark/pull/34602,1053617296,PR_kwDOAQXtWs4uh86l,34602,[SPARK-37328][SQL] Fix bug that OptimizeSkewedJoin may not work after it was moved from queryStageOptimizerRules to queryStagePreparationRules.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-11-15T12:35:58Z,2021-12-21T14:43:16Z,,NONE,,False,"
### What changes were proposed in this pull request?
Fix the issue that OptimizeSkewedJoin may not work.
Since OptimizeSkewedJoin was moved from `queryStageOptimizerRules` to `queryStagePreparationRules,` the position OptimizeSkewedJoin was applied has been moved from `newQueryStage()` to `reOptimize()`. The plan OptimizeSkewedJoin applied on changed from plan of new stage which is about to submit to whole spark plan.
In the cases where skewedJoin is not last stage, OptimizeSkewedJoin may not work because the number of collected shuffleStages is more than 2.


### Why are the changes needed?
Bug fix.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
New test.
",https://api.github.com/repos/apache/spark/issues/34602/timeline,,spark,apache,Liulietong,20419086,MDQ6VXNlcjIwNDE5MDg2,https://avatars.githubusercontent.com/u/20419086?v=4,,https://api.github.com/users/Liulietong,https://github.com/Liulietong,https://api.github.com/users/Liulietong/followers,https://api.github.com/users/Liulietong/following{/other_user},https://api.github.com/users/Liulietong/gists{/gist_id},https://api.github.com/users/Liulietong/starred{/owner}{/repo},https://api.github.com/users/Liulietong/subscriptions,https://api.github.com/users/Liulietong/orgs,https://api.github.com/users/Liulietong/repos,https://api.github.com/users/Liulietong/events{/privacy},https://api.github.com/users/Liulietong/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34602,https://github.com/apache/spark/pull/34602,https://github.com/apache/spark/pull/34602.diff,https://github.com/apache/spark/pull/34602.patch,,https://api.github.com/repos/apache/spark/issues/34602/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
163,https://api.github.com/repos/apache/spark/issues/34593,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34593/labels{/name},https://api.github.com/repos/apache/spark/issues/34593/comments,https://api.github.com/repos/apache/spark/issues/34593/events,https://github.com/apache/spark/pull/34593,1053066585,PR_kwDOAQXtWs4ugKGk,34593,"[SPARK-37324][SQL] Adds support for decimal rounding mode up, down, half_down","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,51,2021-11-14T23:07:33Z,2021-11-26T02:55:04Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?

1. Adds support for Decimal RoundingMode.UP, DOWN and HALF_DOWN by letting users to pass the rounding mode as argument to round function.
2. bround function calls round function with the argument ""half_even""


### Why are the changes needed?

Currently there's no easier and straight forward way to round decimals with the mode UP, DOWN and HALF_DOWN. People need to use UDF or complex operations to do the same.

Opening support for the other rounding modes might interest a lot of use cases.

**SAP Hana Sql ROUND function does it :** 

`ROUND(<number> [, <position> [, <rounding_mode>]])`

REF : https://help.sap.com/viewer/7c78579ce9b14a669c1f3295b0d8ca16/Cloud/en-US/20e6a27575191014bd54a07fd86c585d.html


**Sql Server does something similar to this :**

`ROUND ( numeric_expression , length [ ,function ] )`

REF : https://docs.microsoft.com/en-us/sql/t-sql/functions/round-transact-sql?view=sql-server-ver15 


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Now, users can specify the rounding mode while calling round function for round modes up, down, half_down. Calling round function without rounding mode will default to half_up.

```
> SELECT round(3.145, 2)
3.15

>SELECT round(3.145, 2, 'down')
3.14
```

```
df.withColumn(""value_rounded"", round('value, 0)

df.withColumn(""value_rounded"", round('value, 0, ""down"")
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
This patch was tested locally using unit test and git workflow.",https://api.github.com/repos/apache/spark/issues/34593/timeline,,spark,apache,sathiyapk,5880194,MDQ6VXNlcjU4ODAxOTQ=,https://avatars.githubusercontent.com/u/5880194?v=4,,https://api.github.com/users/sathiyapk,https://github.com/sathiyapk,https://api.github.com/users/sathiyapk/followers,https://api.github.com/users/sathiyapk/following{/other_user},https://api.github.com/users/sathiyapk/gists{/gist_id},https://api.github.com/users/sathiyapk/starred{/owner}{/repo},https://api.github.com/users/sathiyapk/subscriptions,https://api.github.com/users/sathiyapk/orgs,https://api.github.com/users/sathiyapk/repos,https://api.github.com/users/sathiyapk/events{/privacy},https://api.github.com/users/sathiyapk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34593,https://github.com/apache/spark/pull/34593,https://github.com/apache/spark/pull/34593.diff,https://github.com/apache/spark/pull/34593.patch,,https://api.github.com/repos/apache/spark/issues/34593/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
164,https://api.github.com/repos/apache/spark/issues/34569,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34569/labels{/name},https://api.github.com/repos/apache/spark/issues/34569/comments,https://api.github.com/repos/apache/spark/issues/34569/events,https://github.com/apache/spark/pull/34569,1051688614,PR_kwDOAQXtWs4ucUd8,34569,[SPARK-37301][CORE] ConcurrentModificationException caused by CollectionAccumulator serialization in the heartbeat thread,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-12T08:08:52Z,2021-11-15T09:56:08Z,,NONE,,False,"### What changes were proposed in this pull request?

In our production environment, you can use the following code to reproduce the problem:

```scala
val acc = sc.collectionAccumulator[String](""test_acc"")
    
sc.parallelize(Array(0)).foreach(_ => {
  var i = 0
  var stop = false
  val start = System.currentTimeMillis()
  while (!stop) {
    acc.add(i.toString)
    if (i % 10000 == 0) {
      acc.reset()
      if ((System.currentTimeMillis() - start) / 1000 > 120) {
        stop = true
      }
    }
    i = i + 1
  }
})
sc.stop()
```

This code can make the executor fail to send heartbeats, even more than the default 60 times, and then the executor exits.

```tex
21/11/11 21:00:23 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1007)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayList.writeObject(ArrayList.java:766)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.rpc.netty.RequestMessage.serialize(NettyRpcEnv.scala:601)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:244)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)
	... 12 more
21/11/11 21:00:23 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times
```

The reason is that when the heartbeat thread serializes the Collection Accumulator, the task thread may modify the Collection Accumulator


### Why are the changes needed?

Avoid heartbeat reporting failure, which may cause application failure


### Does this PR introduce _any_ user-facing change?

No


### How was this patch tested?
Existing tests and manual tests
",https://api.github.com/repos/apache/spark/issues/34569/timeline,,spark,apache,mcdull-zhang,63445864,MDQ6VXNlcjYzNDQ1ODY0,https://avatars.githubusercontent.com/u/63445864?v=4,,https://api.github.com/users/mcdull-zhang,https://github.com/mcdull-zhang,https://api.github.com/users/mcdull-zhang/followers,https://api.github.com/users/mcdull-zhang/following{/other_user},https://api.github.com/users/mcdull-zhang/gists{/gist_id},https://api.github.com/users/mcdull-zhang/starred{/owner}{/repo},https://api.github.com/users/mcdull-zhang/subscriptions,https://api.github.com/users/mcdull-zhang/orgs,https://api.github.com/users/mcdull-zhang/repos,https://api.github.com/users/mcdull-zhang/events{/privacy},https://api.github.com/users/mcdull-zhang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34569,https://github.com/apache/spark/pull/34569,https://github.com/apache/spark/pull/34569.diff,https://github.com/apache/spark/pull/34569.patch,,https://api.github.com/repos/apache/spark/issues/34569/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
165,https://api.github.com/repos/apache/spark/issues/34568,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34568/labels{/name},https://api.github.com/repos/apache/spark/issues/34568/comments,https://api.github.com/repos/apache/spark/issues/34568/events,https://github.com/apache/spark/pull/34568,1051668730,PR_kwDOAQXtWs4ucQZf,34568,[SPARK-37287][SQL] Pull out dynamic partition and bucket sort from FileFormatWriter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-11-12T07:38:19Z,2022-01-10T12:26:33Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

- Add a new trait `V1Write` to hold some sort infos of v1 write. e.g., partition columns, bucket spec.
- Then let the following writing command extend the `V1Write`, includes both datasource and hive
  - InsertIntoHadoopFsRelationCommand
  - CreateDataSourceTableAsSelectCommand
  - InsertIntoHiveTable
  - CreateHiveTableAsSelectBase
- Add a new rule `V1Writes` to decide if we should add a `Sort` operator based its `V1Write.requiredOrdering`.  This rule should be similar with `V2Writes`.
- So now we can remove the `SortExec` in `FileFormatWriter.write`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
`FileFormatWriter.write` now is used by all V1 write which includes datasource and hive table. However it contains a sort which is based on dynamic partition and bucket columns that can not be seen in plan directly.

V2 write has a better approach that it satisfies the order or even distribution by using rule `V2Writes`.

V1 write should do the similar thing with V2 write.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
this is a code refactor, so it should pass CI",https://api.github.com/repos/apache/spark/issues/34568/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34568,https://github.com/apache/spark/pull/34568,https://github.com/apache/spark/pull/34568.diff,https://github.com/apache/spark/pull/34568.patch,,https://api.github.com/repos/apache/spark/issues/34568/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
166,https://api.github.com/repos/apache/spark/issues/34558,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34558/labels{/name},https://api.github.com/repos/apache/spark/issues/34558/comments,https://api.github.com/repos/apache/spark/issues/34558/events,https://github.com/apache/spark/pull/34558,1051004592,PR_kwDOAQXtWs4uaHfU,34558,[SPARK-37019][SQL] Add codegen support to array higher-order functions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,2,2021-11-11T13:55:00Z,2022-01-28T20:56:39Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This PR adds codegen support to array based higher order functions except ArraySort. This is my first time playing around with codegen, so definitely looking for any feedback.

A few notes:
- Disabled subexpression elimination for lambda functions (this already was the case because it was CodegenFallback). I plan to explore supprting subexpression elimination inside lambda functions later on, as it will require special handling.
- I set the AtomicReference for all lambda values as well in case a child expression reverts to interpreted evaluation for any reason (CodegenFallback or otherwise)

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To improve performance of array higher-order function operations, letting the children be codegen'd and participate in WholeStageCodegen

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, only performance improvements.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing unit tests, let me know if there's other codegen-specific unit tests I should add.",https://api.github.com/repos/apache/spark/issues/34558/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34558,https://github.com/apache/spark/pull/34558,https://github.com/apache/spark/pull/34558.diff,https://github.com/apache/spark/pull/34558.patch,,https://api.github.com/repos/apache/spark/issues/34558/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
167,https://api.github.com/repos/apache/spark/issues/34553,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34553/labels{/name},https://api.github.com/repos/apache/spark/issues/34553/comments,https://api.github.com/repos/apache/spark/issues/34553/events,https://github.com/apache/spark/pull/34553,1050751053,PR_kwDOAQXtWs4uZSif,34553,[SPARK-37285] [ML] Add Weight of Evidence and Information value to ml.feature,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-11-11T09:04:48Z,2021-11-12T08:19:42Z,,NONE,,False,"JIRA Issue: https://issues.apache.org/jira/browse/SPARK-37285

The following shows the differences with this [PR-10803] :
1. More accurate algorithm to reduce the variance when the classification data skewed (all 1 or 0 in the bin of a feature) .
2. Programming based on RDD and better performance.
3. Separate IV and WOE modules to avoid coupling.",https://api.github.com/repos/apache/spark/issues/34553/timeline,,spark,apache,taosiyuan163,24226312,MDQ6VXNlcjI0MjI2MzEy,https://avatars.githubusercontent.com/u/24226312?v=4,,https://api.github.com/users/taosiyuan163,https://github.com/taosiyuan163,https://api.github.com/users/taosiyuan163/followers,https://api.github.com/users/taosiyuan163/following{/other_user},https://api.github.com/users/taosiyuan163/gists{/gist_id},https://api.github.com/users/taosiyuan163/starred{/owner}{/repo},https://api.github.com/users/taosiyuan163/subscriptions,https://api.github.com/users/taosiyuan163/orgs,https://api.github.com/users/taosiyuan163/repos,https://api.github.com/users/taosiyuan163/events{/privacy},https://api.github.com/users/taosiyuan163/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34553,https://github.com/apache/spark/pull/34553,https://github.com/apache/spark/pull/34553.diff,https://github.com/apache/spark/pull/34553.patch,,https://api.github.com/repos/apache/spark/issues/34553/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
168,https://api.github.com/repos/apache/spark/issues/34535,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34535/labels{/name},https://api.github.com/repos/apache/spark/issues/34535/comments,https://api.github.com/repos/apache/spark/issues/34535/events,https://github.com/apache/spark/pull/34535,1048679938,PR_kwDOAQXtWs4uSg_W,34535,[SPARK-37201][SQL] GeneratorNestedColumnAliasing support Generate with Filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,5,2021-11-09T14:34:53Z,2021-11-15T06:05:50Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In current ` GeneratorNestedColumnAliasing`, spark only support  push down case with Project as below
```
Project [v1#225, el#226]
   +- Project [struct#220.v1 AS v1#225, el#226, struct#220]
      +- Generate explode(array#221), false, [el#226]
         +- SubqueryAlias spark_catalog.default.table
            +- Relation default.table[struct#220,array#221] parquet
```

In this pr we support push dow with Project and Filter as below
```
Project [v1#225, el#226]
 +- Project [struct#220.v1 AS v1#225, el#226, struct#220]
    +- Filter ((el#226 = cx1) AND (struct#220.v2 = v3))
      +- Generate explode(array#221), false, [el#226]
         +- SubqueryAlias spark_catalog.default.table
            +- Relation default.table[struct#220,array#221] parquet
```

### Why are the changes needed?
Improve GeneratorNestedColumnAliasing to support more case


### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add UT
",https://api.github.com/repos/apache/spark/issues/34535/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34535,https://github.com/apache/spark/pull/34535,https://github.com/apache/spark/pull/34535.diff,https://github.com/apache/spark/pull/34535.patch,,https://api.github.com/repos/apache/spark/issues/34535/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
169,https://api.github.com/repos/apache/spark/issues/34513,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34513/labels{/name},https://api.github.com/repos/apache/spark/issues/34513/comments,https://api.github.com/repos/apache/spark/issues/34513/events,https://github.com/apache/spark/pull/34513,1047078487,PR_kwDOAQXtWs4uNTNT,34513,[SPARK-37234][PYTHON] Inline type hints for python/pyspark/mllib/stat/_statistics.py,"[{'id': 1405803268, 'node_id': 'MDU6TGFiZWwxNDA1ODAzMjY4', 'url': 'https://api.github.com/repos/apache/spark/labels/MLLIB', 'name': 'MLLIB', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2021-11-08T06:48:26Z,2021-11-26T04:06:08Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/mllib/stat/_statistics.py
### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.
### Does this PR introduce _any_ user-facing change?
No
### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34513/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34513,https://github.com/apache/spark/pull/34513,https://github.com/apache/spark/pull/34513.diff,https://github.com/apache/spark/pull/34513.patch,,https://api.github.com/repos/apache/spark/issues/34513/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
170,https://api.github.com/repos/apache/spark/issues/34504,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34504/labels{/name},https://api.github.com/repos/apache/spark/issues/34504/comments,https://api.github.com/repos/apache/spark/issues/34504/events,https://github.com/apache/spark/pull/34504,1046540120,PR_kwDOAQXtWs4uLsuz,34504,[SPARK-37226][SQL] Filter push down through window if partitionSpec isEmpty,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2021-11-06T16:03:09Z,2021-12-01T08:50:07Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr enhance `PushPredicateThroughNonJoin` to support filter push down through window if window partition is empty. For example:
```scala
spark.sql(""CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b FROM range(1000)"")
spark.sql(""SELECT * FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY a) AS rn FROM t1) t WHERE rn > 100 and rn <= 200"").explain(true)
```
After this pr:
```
== Optimized Logical Plan ==
Filter ((rn#3 > 100) AND (rn#3 <= 200))
+- Window [row_number() windowspecdefinition(a#5L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#3], [a#5L ASC NULLS FIRST]
   +- GlobalLimit 200
      +- LocalLimit 200
         +- Sort [a#5L ASC NULLS FIRST], true
            +- Relation default.t1[a#5L,b#6L] parquet
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 100
spark.sql(s""CREATE TABLE t1 using parquet AS SELECT id as a, id as b FROM range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark filter push down through window"", numRows, minNumIters = 5)

Seq(1, 1000).foreach { threshold =>
  val name = s""Filter push down through window ${if (threshold > 1) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.execution.topKSortFallbackThreshold"" -> s""$threshold"") {
      spark.sql(""SELECT * FROM (SELECT *, ROW_NUMBER() OVER(ORDER BY a) AS rn FROM t1) t WHERE rn > 100 and rn <= 200"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
Benchmark result:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark filter push down through window:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
--------------------------------------------------------------------------------------------------------------------------
Filter push down through window (Disabled)          79219          87062         NaN          1.3         755.5       1.0X
Filter push down through window (Enabled)            5339           5821         425         19.6          50.9      14.8X

```

Production benchmark:

Before this PR | After this PR
--- | ---
![image](https://user-images.githubusercontent.com/5399861/141800881-c0721682-69b3-4861-80fa-0b0ee324aeeb.png)  | ![image](https://user-images.githubusercontent.com/5399861/141878272-48252c7f-108b-4834-9d9b-e8a54eb05e75.png)
",https://api.github.com/repos/apache/spark/issues/34504/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34504,https://github.com/apache/spark/pull/34504,https://github.com/apache/spark/pull/34504.diff,https://github.com/apache/spark/pull/34504.patch,,https://api.github.com/repos/apache/spark/issues/34504/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
171,https://api.github.com/repos/apache/spark/issues/34500,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34500/labels{/name},https://api.github.com/repos/apache/spark/issues/34500/comments,https://api.github.com/repos/apache/spark/issues/34500/events,https://github.com/apache/spark/pull/34500,1046424098,PR_kwDOAQXtWs4uLXaZ,34500,[WIP][SPARK-33574][CORE] Improve locality for push-based shuffle especially for join-like operations,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-11-06T05:31:02Z,2022-01-31T22:34:23Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Reuse merger locations in the case where multiple stages output is consumed by a single stage (e.g.: Joins).

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For sibling shuffle map stages, i.e. ones that share common child stages, reusing merger locations between them can help to further increase shuffle locality ratio when push based shuffle is enabled. Examples include Joins where the reduce stage needs to fetch shuffle data from multiple parent shuffle map stages.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. This PR introduces a client-side config for push-based shuffle `spark.shuffle.push.reuse.merger.locations`. If push-based shuffle is turned-off then users will not see any change.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
Passed the added UTs.",https://api.github.com/repos/apache/spark/issues/34500/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34500,https://github.com/apache/spark/pull/34500,https://github.com/apache/spark/pull/34500.diff,https://github.com/apache/spark/pull/34500.patch,,https://api.github.com/repos/apache/spark/issues/34500/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
172,https://api.github.com/repos/apache/spark/issues/34491,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34491/labels{/name},https://api.github.com/repos/apache/spark/issues/34491/comments,https://api.github.com/repos/apache/spark/issues/34491/events,https://github.com/apache/spark/pull/34491,1045631417,PR_kwDOAQXtWs4uIzDo,34491,[SPARK-37215][YARN] Support Application Timeouts on YARN,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,"[{'login': 'yaooqinn', 'id': 8326978, 'node_id': 'MDQ6VXNlcjgzMjY5Nzg=', 'avatar_url': 'https://avatars.githubusercontent.com/u/8326978?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/yaooqinn', 'html_url': 'https://github.com/yaooqinn', 'followers_url': 'https://api.github.com/users/yaooqinn/followers', 'following_url': 'https://api.github.com/users/yaooqinn/following{/other_user}', 'gists_url': 'https://api.github.com/users/yaooqinn/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/yaooqinn/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/yaooqinn/subscriptions', 'organizations_url': 'https://api.github.com/users/yaooqinn/orgs', 'repos_url': 'https://api.github.com/users/yaooqinn/repos', 'events_url': 'https://api.github.com/users/yaooqinn/events{/privacy}', 'received_events_url': 'https://api.github.com/users/yaooqinn/received_events', 'type': 'User', 'site_admin': False}]",,15,2021-11-05T09:38:48Z,2021-11-15T21:47:38Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Since #YARN-3813/2.9.0/3.0.0, YARN supports ApplicationTimeouts. It helps enforce lifetime application SLAs. Currently, lifetime indicates the overall time spent by an application in YARN. It is calculated from its submit time to finish time, including running time and the waiting time for resource allocation.

YARN allows admins to set lifetime of an application at leaf-queue. It also allows users to set it programmatically. During application submission, user can set it in  `ApplicationSubmissionContext#setApplicationTimeouts(Map<ApplicationTimeoutType, Long> applicationTimeouts)`.

So far, YARN supports for one timeout type - LIFETIME.

In this PR, we `setApplicationTimeouts` when the YARN dependency is available, e.g. the default Hadoop 3.3 or user-specified Hadoop 2.9+ when Hadoop is provided at compile phase.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

This can enforce the application SLAs.
For example, when the YARN queue is hit its limits of app concurrency or cpu/mem, an app will be pending for a very long time or even get stuck in  `ACCEPTED` state forever and do nothing. 

Sometimes, users also may want their app to succeed or timeout/failed with proper time constraints.

This is necessary for end-users use spark througth serverless spark platform like apache kyuubi(incubating) to prevent issue like https://github.com/apache/incubator-kyuubi/issues/1039, https://github.com/apache/incubator-kyuubi/issues/278 and so on.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

yes, we add a new conf but do not change the current behavior

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

new tests added
",https://api.github.com/repos/apache/spark/issues/34491/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34491,https://github.com/apache/spark/pull/34491,https://github.com/apache/spark/pull/34491.diff,https://github.com/apache/spark/pull/34491.patch,,https://api.github.com/repos/apache/spark/issues/34491/reactions,0,0,0,0,0,0,0,0,0,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False
173,https://api.github.com/repos/apache/spark/issues/34489,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34489/labels{/name},https://api.github.com/repos/apache/spark/issues/34489/comments,https://api.github.com/repos/apache/spark/issues/34489/events,https://github.com/apache/spark/pull/34489,1045534130,PR_kwDOAQXtWs4uIey8,34489,[SPARK-37210][SQL] Write to static partition in dynamic write mode,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,6,2021-11-05T07:47:05Z,2021-12-01T07:38:41Z,,MEMBER,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
When using static partition writing, dynamicPartitionOverwrite should also be set to true. See [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for details

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
An error occurred while concurrently writing to different static partitions.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
See [SPARK-37210](https://issues.apache.org/jira/browse/SPARK-37210) for specific test.",https://api.github.com/repos/apache/spark/issues/34489/timeline,,spark,apache,wForget,17894939,MDQ6VXNlcjE3ODk0OTM5,https://avatars.githubusercontent.com/u/17894939?v=4,,https://api.github.com/users/wForget,https://github.com/wForget,https://api.github.com/users/wForget/followers,https://api.github.com/users/wForget/following{/other_user},https://api.github.com/users/wForget/gists{/gist_id},https://api.github.com/users/wForget/starred{/owner}{/repo},https://api.github.com/users/wForget/subscriptions,https://api.github.com/users/wForget/orgs,https://api.github.com/users/wForget/repos,https://api.github.com/users/wForget/events{/privacy},https://api.github.com/users/wForget/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34489,https://github.com/apache/spark/pull/34489,https://github.com/apache/spark/pull/34489.diff,https://github.com/apache/spark/pull/34489.patch,,https://api.github.com/repos/apache/spark/issues/34489/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
174,https://api.github.com/repos/apache/spark/issues/34468,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34468/labels{/name},https://api.github.com/repos/apache/spark/issues/34468/comments,https://api.github.com/repos/apache/spark/issues/34468/events,https://github.com/apache/spark/pull/34468,1042074184,PR_kwDOAQXtWs4t-EtD,34468,[SPARK-37194][SQL] Avoid unnecessary sort in FileFormatWriter if it's not dynamic partition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-11-02T09:32:15Z,2021-11-02T17:33:32Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Pass a new parameter `dynamicPartition` to `FileFormatWriter.write` so that we can distinguish if we need local sort or not.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Avoid unnecessary sort in FileFormatWriter if it's not dynamic partition

`FileFormatWriter.write` will sort the partition and bucket column before writing. I think this code path assumed the input `partitionColumns` are dynamic but actually it's not. It now is used by three code path:
- `FileStreamSink`; it should be always dynamic partition
- `SaveAsHiveFile`; it followed the assuming that `InsertIntoHiveTable` has removed the static partition and `InsertIntoHiveDirCommand` has no partition
- `InsertIntoHadoopFsRelationCommand`; it passed `partitionColumns` into `FileFormatWriter.write` without removing static partition because we need it to generate the partition path in `DynamicPartitionDataWriter`

It shows that the unnecessary sort only affected the `InsertIntoHadoopFsRelationCommand` if we write data with static partition.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
It should not affect the existed behavior, just improve perf. And for perf number, I did a simple benchmak:

```sql

CREATE TABLE test (id long) USING PARQUET PARTITIONED BY (d string);

-- before this PR, it tooks 1.82  seconds
-- after this PR,  it tooks 1.072 seconds
INSERT OVERWRITE TABLE test PARTITION(d='a') SELECT id FROM range(10000000);
```

",https://api.github.com/repos/apache/spark/issues/34468/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34468,https://github.com/apache/spark/pull/34468,https://github.com/apache/spark/pull/34468.diff,https://github.com/apache/spark/pull/34468.patch,,https://api.github.com/repos/apache/spark/issues/34468/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
175,https://api.github.com/repos/apache/spark/issues/34457,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34457/labels{/name},https://api.github.com/repos/apache/spark/issues/34457/comments,https://api.github.com/repos/apache/spark/issues/34457/events,https://github.com/apache/spark/pull/34457,1041189443,PR_kwDOAQXtWs4t7XzP,34457,[SPARK-37178][ML] Add Target Encoding to ml.feature,"[{'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-11-01T13:45:19Z,2021-11-02T08:12:14Z,,NONE,,False,JIRA Issue: https://issues.apache.org/jira/browse/SPARK-37178,https://api.github.com/repos/apache/spark/issues/34457/timeline,,spark,apache,taosiyuan163,24226312,MDQ6VXNlcjI0MjI2MzEy,https://avatars.githubusercontent.com/u/24226312?v=4,,https://api.github.com/users/taosiyuan163,https://github.com/taosiyuan163,https://api.github.com/users/taosiyuan163/followers,https://api.github.com/users/taosiyuan163/following{/other_user},https://api.github.com/users/taosiyuan163/gists{/gist_id},https://api.github.com/users/taosiyuan163/starred{/owner}{/repo},https://api.github.com/users/taosiyuan163/subscriptions,https://api.github.com/users/taosiyuan163/orgs,https://api.github.com/users/taosiyuan163/repos,https://api.github.com/users/taosiyuan163/events{/privacy},https://api.github.com/users/taosiyuan163/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34457,https://github.com/apache/spark/pull/34457,https://github.com/apache/spark/pull/34457.diff,https://github.com/apache/spark/pull/34457.patch,,https://api.github.com/repos/apache/spark/issues/34457/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
176,https://api.github.com/repos/apache/spark/issues/34453,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34453/labels{/name},https://api.github.com/repos/apache/spark/issues/34453/comments,https://api.github.com/repos/apache/spark/issues/34453/events,https://github.com/apache/spark/pull/34453,1040721196,PR_kwDOAQXtWs4t502o,34453,[SPARK-37173][SQL] SparkGetFunctionOperation return builtin function only once,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2021-11-01T03:29:59Z,2022-01-27T09:28:44Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
According to https://github.com/apache/spark/pull/25252/files#r738489764, if we use wild pattern, it will return too much rows.

In this pr we return common builtin functions only once

### Why are the changes needed?
Improve performance

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
WIP
",https://api.github.com/repos/apache/spark/issues/34453/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34453,https://github.com/apache/spark/pull/34453,https://github.com/apache/spark/pull/34453.diff,https://github.com/apache/spark/pull/34453.patch,,https://api.github.com/repos/apache/spark/issues/34453/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
177,https://api.github.com/repos/apache/spark/issues/34407,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34407/labels{/name},https://api.github.com/repos/apache/spark/issues/34407/comments,https://api.github.com/repos/apache/spark/issues/34407/events,https://github.com/apache/spark/pull/34407,1037961916,PR_kwDOAQXtWs4txNtD,34407,[SPARK-37047][SQL][FOLLOWUP] lpad/rpad should work in non-ANSI mode if parameters str and pad are different types,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-10-28T00:14:09Z,2021-10-28T02:11:07Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This is a followup of #34154 and #34370 . Now `lpad`/`rpad` allow the `str` and `pad` parameters to be of different types in non-ANSI mode. The result type in this case is a character string.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

The changes in this PR restore the behavior (in non-ANSI mode) to that prior to #34154 and #34370 when the `lpad` and `rpad` functions take an `str` and `pad` argument and these arguments are of different types.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No. The overloads for the `lpad` and `rpad` functions have not been released yet.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->

Existing tests are enough (and have been updated appropriately).
",https://api.github.com/repos/apache/spark/issues/34407/timeline,,spark,apache,mkaravel,6397014,MDQ6VXNlcjYzOTcwMTQ=,https://avatars.githubusercontent.com/u/6397014?v=4,,https://api.github.com/users/mkaravel,https://github.com/mkaravel,https://api.github.com/users/mkaravel/followers,https://api.github.com/users/mkaravel/following{/other_user},https://api.github.com/users/mkaravel/gists{/gist_id},https://api.github.com/users/mkaravel/starred{/owner}{/repo},https://api.github.com/users/mkaravel/subscriptions,https://api.github.com/users/mkaravel/orgs,https://api.github.com/users/mkaravel/repos,https://api.github.com/users/mkaravel/events{/privacy},https://api.github.com/users/mkaravel/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34407,https://github.com/apache/spark/pull/34407,https://github.com/apache/spark/pull/34407.diff,https://github.com/apache/spark/pull/34407.patch,,https://api.github.com/repos/apache/spark/issues/34407/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
178,https://api.github.com/repos/apache/spark/issues/34406,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34406/labels{/name},https://api.github.com/repos/apache/spark/issues/34406/comments,https://api.github.com/repos/apache/spark/issues/34406/events,https://github.com/apache/spark/pull/34406,1037798773,PR_kwDOAQXtWs4twruD,34406,Minor fix to docs for read_csv,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-27T19:46:40Z,2022-01-18T06:50:22Z,,NONE,,False,"Fixed documentation of the escapechar parameter of read_csv function
(docs incorrectly say this is for escaping the delimiter - it is for escaping a quotechar)

<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Minor update to function documentation.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently the documentation for the escapechar parameter is incorrect.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Fixes the documentation

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.
-->
No testing needed",https://api.github.com/repos/apache/spark/issues/34406/timeline,,spark,apache,tnixon,1181780,MDQ6VXNlcjExODE3ODA=,https://avatars.githubusercontent.com/u/1181780?v=4,,https://api.github.com/users/tnixon,https://github.com/tnixon,https://api.github.com/users/tnixon/followers,https://api.github.com/users/tnixon/following{/other_user},https://api.github.com/users/tnixon/gists{/gist_id},https://api.github.com/users/tnixon/starred{/owner}{/repo},https://api.github.com/users/tnixon/subscriptions,https://api.github.com/users/tnixon/orgs,https://api.github.com/users/tnixon/repos,https://api.github.com/users/tnixon/events{/privacy},https://api.github.com/users/tnixon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34406,https://github.com/apache/spark/pull/34406,https://github.com/apache/spark/pull/34406.diff,https://github.com/apache/spark/pull/34406.patch,,https://api.github.com/repos/apache/spark/issues/34406/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
179,https://api.github.com/repos/apache/spark/issues/34404,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34404/labels{/name},https://api.github.com/repos/apache/spark/issues/34404/comments,https://api.github.com/repos/apache/spark/issues/34404/events,https://github.com/apache/spark/pull/34404,1037629184,PR_kwDOAQXtWs4twI_P,34404,[DO NOT MERGE][PYTHON] testing lint-python,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-10-27T16:35:01Z,2021-10-27T21:11:05Z,,CONTRIBUTOR,,False,DO NOT MERGE TYVM,https://api.github.com/repos/apache/spark/issues/34404/timeline,,spark,apache,shaneknapp,1606572,MDQ6VXNlcjE2MDY1NzI=,https://avatars.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34404,https://github.com/apache/spark/pull/34404,https://github.com/apache/spark/pull/34404.diff,https://github.com/apache/spark/pull/34404.patch,,https://api.github.com/repos/apache/spark/issues/34404/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
180,https://api.github.com/repos/apache/spark/issues/34402,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34402/labels{/name},https://api.github.com/repos/apache/spark/issues/34402/comments,https://api.github.com/repos/apache/spark/issues/34402/events,https://github.com/apache/spark/pull/34402,1037518916,PR_kwDOAQXtWs4tvxz2,34402,[SPARK-30220] Enable using Exists/In subqueries outside of the Filter node,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-10-27T14:49:47Z,2022-01-04T06:50:50Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Enable using Exists/In subqueries in other nodes besides `Filter`: `Aggregate`, `Project` and `Window`.
This is allready mostly supported, but it was blocked in the `Analyzer`. Only requires a small tweak in the `Optimizer`.

### Why are the changes needed?
One of the last open feature parities between PostgreSQL and Spark: [SPARK-30374](https://issues.apache.org/jira/browse/SPARK-30374)

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing and new UTs
",https://api.github.com/repos/apache/spark/issues/34402/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34402,https://github.com/apache/spark/pull/34402,https://github.com/apache/spark/pull/34402.diff,https://github.com/apache/spark/pull/34402.patch,,https://api.github.com/repos/apache/spark/issues/34402/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
181,https://api.github.com/repos/apache/spark/issues/34396,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34396/labels{/name},https://api.github.com/repos/apache/spark/issues/34396/comments,https://api.github.com/repos/apache/spark/issues/34396/events,https://github.com/apache/spark/pull/34396,1036868643,PR_kwDOAQXtWs4ttsS6,34396,[SPARK-37124][SQL] Support RowToColumnarExec with Arrow format,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-10-27T01:00:10Z,2021-11-23T02:00:21Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This Jira is aim to support Arrow format in RowToColumnarExec.

### Why are the changes needed?
Current ArrowColumnVector is not fully equivalent to OnHeap/OffHeapColumnVector in spark, so RowToColumnarExec doesn't support write to Arrow format so far.

since Arrow API is now being more stable, and using pandas udf will perform much better than python udf.

### What has been done in this pull request?
I am  proposing to support RowToColumnarExec with Arrow.

What I did in this PR is to add a load api in ArrowColumnVector to load arrowRecordBatch to ArrowColumnVector, then called inside RowToColumnarExec doExecute.

### How was this patch tested?
UTs are also added to test this new API and RowToColumnarExec with ArrowFormat.

### Does this PR introduce _any_ user-facing change?
NO

Signed-off-by: Chendi Xue <chendi.xue@intel.com>",https://api.github.com/repos/apache/spark/issues/34396/timeline,,spark,apache,xuechendi,4355494,MDQ6VXNlcjQzNTU0OTQ=,https://avatars.githubusercontent.com/u/4355494?v=4,,https://api.github.com/users/xuechendi,https://github.com/xuechendi,https://api.github.com/users/xuechendi/followers,https://api.github.com/users/xuechendi/following{/other_user},https://api.github.com/users/xuechendi/gists{/gist_id},https://api.github.com/users/xuechendi/starred{/owner}{/repo},https://api.github.com/users/xuechendi/subscriptions,https://api.github.com/users/xuechendi/orgs,https://api.github.com/users/xuechendi/repos,https://api.github.com/users/xuechendi/events{/privacy},https://api.github.com/users/xuechendi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34396,https://github.com/apache/spark/pull/34396,https://github.com/apache/spark/pull/34396.diff,https://github.com/apache/spark/pull/34396.patch,,https://api.github.com/repos/apache/spark/issues/34396/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
182,https://api.github.com/repos/apache/spark/issues/34386,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34386/labels{/name},https://api.github.com/repos/apache/spark/issues/34386/comments,https://api.github.com/repos/apache/spark/issues/34386/events,https://github.com/apache/spark/pull/34386,1035682757,PR_kwDOAQXtWs4tp0U0,34386,[WIP] - Changes to PySpark doc homepage and User Guide,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,15,2021-10-26T00:12:19Z,2022-01-17T14:19:57Z,,NONE,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
WIP - fixes to PySpark documentation front page and User guide 
-->


### Why are the changes needed?
<!--
Make the content on the documentation page more coherent
-->


### Does this PR introduce _any_ user-facing change?
<!--
The docs are user-facing and will be the front page of PySpark home page and the User guide
-->


### How was this patch tested?
<!--
The docs were built and tested using sphinx and Github pages
-->
",https://api.github.com/repos/apache/spark/issues/34386/timeline,,spark,apache,srijith-rajamohan,78916489,MDQ6VXNlcjc4OTE2NDg5,https://avatars.githubusercontent.com/u/78916489?v=4,,https://api.github.com/users/srijith-rajamohan,https://github.com/srijith-rajamohan,https://api.github.com/users/srijith-rajamohan/followers,https://api.github.com/users/srijith-rajamohan/following{/other_user},https://api.github.com/users/srijith-rajamohan/gists{/gist_id},https://api.github.com/users/srijith-rajamohan/starred{/owner}{/repo},https://api.github.com/users/srijith-rajamohan/subscriptions,https://api.github.com/users/srijith-rajamohan/orgs,https://api.github.com/users/srijith-rajamohan/repos,https://api.github.com/users/srijith-rajamohan/events{/privacy},https://api.github.com/users/srijith-rajamohan/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34386,https://github.com/apache/spark/pull/34386,https://github.com/apache/spark/pull/34386.diff,https://github.com/apache/spark/pull/34386.patch,,https://api.github.com/repos/apache/spark/issues/34386/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
183,https://api.github.com/repos/apache/spark/issues/34378,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34378/labels{/name},https://api.github.com/repos/apache/spark/issues/34378/comments,https://api.github.com/repos/apache/spark/issues/34378/events,https://github.com/apache/spark/pull/34378,1034800338,PR_kwDOAQXtWs4tm9Ax,34378,spark executor pod anti affinity when run on k8s,[],open,False,,[],,5,2021-10-25T07:57:08Z,2021-11-04T05:21:18Z,,NONE,,False,I have realized the executor pod anti affinity. This prevents too many pods from starting on one node. ,https://api.github.com/repos/apache/spark/issues/34378/timeline,,spark,apache,wfxxh,22764286,MDQ6VXNlcjIyNzY0Mjg2,https://avatars.githubusercontent.com/u/22764286?v=4,,https://api.github.com/users/wfxxh,https://github.com/wfxxh,https://api.github.com/users/wfxxh/followers,https://api.github.com/users/wfxxh/following{/other_user},https://api.github.com/users/wfxxh/gists{/gist_id},https://api.github.com/users/wfxxh/starred{/owner}{/repo},https://api.github.com/users/wfxxh/subscriptions,https://api.github.com/users/wfxxh/orgs,https://api.github.com/users/wfxxh/repos,https://api.github.com/users/wfxxh/events{/privacy},https://api.github.com/users/wfxxh/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34378,https://github.com/apache/spark/pull/34378,https://github.com/apache/spark/pull/34378.diff,https://github.com/apache/spark/pull/34378.patch,,https://api.github.com/repos/apache/spark/issues/34378/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
184,https://api.github.com/repos/apache/spark/issues/34367,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34367/labels{/name},https://api.github.com/repos/apache/spark/issues/34367/comments,https://api.github.com/repos/apache/spark/issues/34367/events,https://github.com/apache/spark/pull/34367,1033447009,PR_kwDOAQXtWs4tiv1X,34367,[SPARK-37099][SQL] Impl a rank-based filter to optimize top-k computation,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,62,2021-10-22T10:51:01Z,2021-12-31T07:07:34Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
add a new node `RankLimit` to filter out uncessary rows based on rank computed on partial dataset.

it supports this pattern:

```
 select (... (row_number|rank|dense_rank)() over ( [partition by ...] order by ... ) as rn)
    where rn (==|<|<=) k and other conditions
```

For these three rank functions (row_number|rank|dense_rank), the rank of a key computed on partitial dataset  always  <=  its final rank computed on the whole dataset.

so we can safely discard rows with partitial rank > `k`, anywhere.



### Why are the changes needed?
1, reduce the shuffle write;
2, solve skewed-window problem, a practical case was optimized from 2.5h to 26min


### Does this PR introduce _any_ user-facing change?
a new config is added


### How was this patch tested?

1, added testsuits, practical cases on our production system

2, 10TiB TPC-DS - q67:

Before this PR | After this PR
--- | ---
Job Duration=58min|Job Duration=11min
Stage Duration=50min|Stage Duration=3sec
Stage Shuffle=58.0 GiB|Stage Shuffle=9.9 MiB
![image](https://user-images.githubusercontent.com/7322292/147652153-80890751-1c6d-4c54-8baf-1b036e829ca9.png)|![image](https://user-images.githubusercontent.com/7322292/147652272-128d3013-c2d0-4676-ab79-050d3349d0b2.png)
![image](https://user-images.githubusercontent.com/7322292/147808906-ed68e493-d0a3-4134-964a-a037721f4fbb.png)|![image](https://user-images.githubusercontent.com/7322292/147808939-a605f85a-bb31-49fa-9dd9-a9af23ec5df0.png)


3, added benchmark:

```

[info] Java HotSpot(TM) 64-Bit Server VM 1.8.0_301-b09 on Linux 5.11.0-41-generic
[info] Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
[info] Benchmark Top-K:                                      Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------------------
[info] ROW_NUMBER WITHOUT PARTITION                                  10688          11377         664          2.0         509.6       1.0X
[info] ROW_NUMBER WITHOUT PARTITION (RANKLIMIT Sorting)               2678           2962         137          7.8         127.7       4.0X
[info] ROW_NUMBER WITHOUT PARTITION (RANKLIMIT TakeOrdered)           1585           1611          19         13.2          75.6       6.7X
[info] RANK WITHOUT PARTITION                                        11504          12056         406          1.8         548.6       0.9X
[info] RANK WITHOUT PARTITION (RANKLIMIT)                             3020           3148          89          6.9         144.0       3.5X
[info] DENSE_RANK WITHOUT PARTITION                                  11728          11915         216          1.8         559.3       0.9X
[info] DENSE_RANK WITHOUT PARTITION (RANKLIMIT)                       2632           2906         182          8.0         125.5       4.1X
[info] ROW_NUMBER WITH PARTITION                                     23139          24025         500          0.9        1103.4       0.5X
[info] ROW_NUMBER WITH PARTITION (RANKLIMIT Sorting)                  7034           7575         361          3.0         335.4       1.5X
[info] ROW_NUMBER WITH PARTITION (RANKLIMIT TakeOrdered)              5958           6391         311          3.5         284.1       1.8X
[info] RANK WITH PARTITION                                           24942          26005         795          0.8        1189.4       0.4X
[info] RANK WITH PARTITION (RANKLIMIT)                                7217           7517         219          2.9         344.1       1.5X
[info] DENSE_RANK WITH PARTITION                                     24843          26726         221          0.8        1184.6       0.4X
[info] DENSE_RANK WITH PARTITION (RANKLIMIT)                          7455           7978         560          2.8         355.5       1.4X
```",https://api.github.com/repos/apache/spark/issues/34367/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34367,https://github.com/apache/spark/pull/34367,https://github.com/apache/spark/pull/34367.diff,https://github.com/apache/spark/pull/34367.patch,,https://api.github.com/repos/apache/spark/issues/34367/reactions,2,1,0,0,0,0,0,0,1,,,,,,,,,,,,,,,,,,
185,https://api.github.com/repos/apache/spark/issues/34366,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34366/labels{/name},https://api.github.com/repos/apache/spark/issues/34366/comments,https://api.github.com/repos/apache/spark/issues/34366/events,https://github.com/apache/spark/pull/34366,1033381850,PR_kwDOAQXtWs4tiibR,34366,[SPARK-37097][YARN] yarn-cluster mode don't need to retry when AM container exit code 0 but application failed.,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-10-22T09:37:33Z,2021-11-10T15:17:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Some yarn-cluster application meet such exception.
```
21/10/20 03:31:55 ERROR Client: Application diagnostics message: Application application_1632999510150_2163647 failed 1 times (global limit =8; local limit is =1) due to AM Container for appattempt_1632999510150_2163647_000001 exited with  exitCode: 0
Failing this attempt.Diagnostics: For more detailed output, check the application tracking page: http://ip-xx-xx-xx-xx.idata-server.shopee.io:8088/cluster/app/application_1632999510150_2163647 Then click on links to logs of each attempt.
. Failing the application.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1632999510150_2163647 finished with failed status
```

It's caused by below situation:
1. yarn-cluster mode application usr code finished, AM shutdown hook triggered
2. AM call unregister from RM but timeout, since AM shutdown hook have try catch, won't throw exception, so AM container exit with code 0(application user code running success).
3. Since RM lose connection with AM, then treat this container as failed final status.
4. Then client side got application report as final status failed but am container exit code 0. client treat it as failed, then retry.


it's a unnecessary retry. we can avoid it.


### Why are the changes needed?
Avoid unnecessary retry

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?

Manual tested",https://api.github.com/repos/apache/spark/issues/34366/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34366,https://github.com/apache/spark/pull/34366,https://github.com/apache/spark/pull/34366.diff,https://github.com/apache/spark/pull/34366.patch,,https://api.github.com/repos/apache/spark/issues/34366/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
186,https://api.github.com/repos/apache/spark/issues/34362,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34362/labels{/name},https://api.github.com/repos/apache/spark/issues/34362/comments,https://api.github.com/repos/apache/spark/issues/34362/events,https://github.com/apache/spark/pull/34362,1033077696,PR_kwDOAQXtWs4thlEd,34362,[SPARK-37090][BUILD] Upgrade `libthrift` to 0.16.0 to avoid security vulnerabilities,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,13,2021-10-22T01:07:03Z,2022-01-30T04:07:36Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr backport HIVE-21498 to upgrade libthrift to 0.13.0.

### Why are the changes needed?

To addresses CVEs:

Component Name | Component Version Name | Vulnerability | Fixed version
-- | -- | -- | --
Apache Thrift | 0.11.0-4. | [CVE-2019-0205](https://nvd.nist.gov/vuln/detail/CVE-2019-0205) | 0.13.0
Apache Thrift | 0.11.0-4. | CVE-2019-0210 | 0.13.0

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Existing test.",https://api.github.com/repos/apache/spark/issues/34362/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34362,https://github.com/apache/spark/pull/34362,https://github.com/apache/spark/pull/34362.diff,https://github.com/apache/spark/pull/34362.patch,,https://api.github.com/repos/apache/spark/issues/34362/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
187,https://api.github.com/repos/apache/spark/issues/34359,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34359/labels{/name},https://api.github.com/repos/apache/spark/issues/34359/comments,https://api.github.com/repos/apache/spark/issues/34359/events,https://github.com/apache/spark/pull/34359,1032748808,PR_kwDOAQXtWs4tgfti,34359,[SPARK-36986][SQL] Improving external schema management flexibility on DataSet and StructType,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-10-21T17:28:13Z,2021-12-14T09:58:51Z,,NONE,,False,"### What changes were proposed in this pull request?
These are the following proposed improvements:
1 - ability to retrieve from StructType, the field's name and schema in one single call, requesting to return a tupple by index. 
2 - Allowing for a dataset to be created from a schema, and passing the corresponding internal rows which the internal types map with the schema already defined externally. 


### Why are the changes needed?
Explanations provided for the respective changes mentioned earlier
1- Avoids two client calls/loops to obtain consolidated field info, when looping through a schema and updating field values on a genericRow data type.
2 - This allows to create Spark fields based on any data structure, without depending on Spark's internal conversions (in particular for Json parsing), and improves performance by skipping the CatalystConverts job of converting native Java types into Spark types.



### Does this PR introduce _any_ user-facing change?
This PR augments the current DataSet API and the StructType object manipulation. 


### How was this patch tested?
Unit tests were added (included in the PR), and bench tests were done locally to validate the performance of the new dataset API call. 
",https://api.github.com/repos/apache/spark/issues/34359/timeline,,spark,apache,risinga,3072864,MDQ6VXNlcjMwNzI4NjQ=,https://avatars.githubusercontent.com/u/3072864?v=4,,https://api.github.com/users/risinga,https://github.com/risinga,https://api.github.com/users/risinga/followers,https://api.github.com/users/risinga/following{/other_user},https://api.github.com/users/risinga/gists{/gist_id},https://api.github.com/users/risinga/starred{/owner}{/repo},https://api.github.com/users/risinga/subscriptions,https://api.github.com/users/risinga/orgs,https://api.github.com/users/risinga/repos,https://api.github.com/users/risinga/events{/privacy},https://api.github.com/users/risinga/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34359,https://github.com/apache/spark/pull/34359,https://github.com/apache/spark/pull/34359.diff,https://github.com/apache/spark/pull/34359.patch,,https://api.github.com/repos/apache/spark/issues/34359/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
188,https://api.github.com/repos/apache/spark/issues/34352,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34352/labels{/name},https://api.github.com/repos/apache/spark/issues/34352/comments,https://api.github.com/repos/apache/spark/issues/34352/events,https://github.com/apache/spark/pull/34352,1032186107,PR_kwDOAQXtWs4tepwm,34352,[SPARK-37018][SQL] Spark SQL should support create function with Aggregator,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2021-10-21T08:06:29Z,2021-12-27T00:59:28Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Spark SQL not supports to create function of `Aggregator` yet and deprecated `UserDefinedAggregateFunction`.
If we want remove `UserDefinedAggregateFunction`, Spark SQL should provide a new option.
Note: This PR replaces https://github.com/apache/spark/pull/34303.


### Why are the changes needed?
We need to provide a new way to create user defined aggregate function so as remove `UserDefinedAggregateFunction` in future.


### Does this PR introduce _any_ user-facing change?
Yes. Users will create user defined aggregate function by implement `Aggregator`.


### How was this patch tested?
New tests.
",https://api.github.com/repos/apache/spark/issues/34352/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34352,https://github.com/apache/spark/pull/34352,https://github.com/apache/spark/pull/34352.diff,https://github.com/apache/spark/pull/34352.patch,,https://api.github.com/repos/apache/spark/issues/34352/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
189,https://api.github.com/repos/apache/spark/issues/34334,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34334/labels{/name},https://api.github.com/repos/apache/spark/issues/34334/comments,https://api.github.com/repos/apache/spark/issues/34334/events,https://github.com/apache/spark/pull/34334,1030889485,PR_kwDOAQXtWs4tadZi,34334,[SPARK-36763][SQL] Pull out complex sorting expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,8,2021-10-20T01:56:29Z,2021-12-09T14:04:09Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This pr pull out complex sorting expressions if it is global sorting to reduce the evaluation of complex expressions. For example:
```sql
SELECT id AS a, id AS b FROM range(10) ORDER BY a - b
```
Before this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Sort [(a#0L - b#1L) ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning((a#0L - b#1L) ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#12]
      +- Project [id#2L AS a#0L, id#2L AS b#1L]
         +- Range (0, 10, step=1, splits=2)
```
After this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#0L, b#1L]
   +- Sort [_sortingexpression#5L ASC NULLS FIRST], true, 0
      +- Exchange rangepartitioning(_sortingexpression#5L ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#16]
         +- Project [id#2L AS a#0L, id#2L AS b#1L, (id#2L - id#2L) AS _sortingexpression#5L]
            +- Range (0, 10, step=1, splits=2)
```

### Why are the changes needed?

Improve order performance if the sorting expressions contains complex expressions.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 10
spark.sql(s""CREATE TABLE t1 using parquet AS select id AS a, id AS b FROM range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark pull out ordering expressions"", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s""Pull out ordering expressions ${if (pullOutEnabled) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.pullOutOrderingExpressions"" -> s""$pullOutEnabled"") {
      spark.sql(""SELECT t1.* FROM t1 ORDER BY translate(t1.a, '123', 'abc')"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out ordering expressions:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out ordering expressions (Disabled)           9232           9753         867          1.1         880.4       1.0X
Pull out ordering expressions (Enabled)            7084           7462         370          1.5         675.5       1.3X
```",https://api.github.com/repos/apache/spark/issues/34334/timeline,,spark,apache,RabbidHY,90840965,MDQ6VXNlcjkwODQwOTY1,https://avatars.githubusercontent.com/u/90840965?v=4,,https://api.github.com/users/RabbidHY,https://github.com/RabbidHY,https://api.github.com/users/RabbidHY/followers,https://api.github.com/users/RabbidHY/following{/other_user},https://api.github.com/users/RabbidHY/gists{/gist_id},https://api.github.com/users/RabbidHY/starred{/owner}{/repo},https://api.github.com/users/RabbidHY/subscriptions,https://api.github.com/users/RabbidHY/orgs,https://api.github.com/users/RabbidHY/repos,https://api.github.com/users/RabbidHY/events{/privacy},https://api.github.com/users/RabbidHY/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34334,https://github.com/apache/spark/pull/34334,https://github.com/apache/spark/pull/34334.diff,https://github.com/apache/spark/pull/34334.patch,,https://api.github.com/repos/apache/spark/issues/34334/reactions,3,0,0,3,0,0,0,0,0,,,,,,,,,,,,,,,,,,
190,https://api.github.com/repos/apache/spark/issues/34326,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34326/labels{/name},https://api.github.com/repos/apache/spark/issues/34326/comments,https://api.github.com/repos/apache/spark/issues/34326/events,https://github.com/apache/spark/pull/34326,1029902759,PR_kwDOAQXtWs4tXXNr,34326,[SPARK-37053][CORE] Add metrics to SparkHistoryServer,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,58,2021-10-19T06:17:05Z,2022-02-01T22:32:38Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Add metrics system to history server.

|  Metrics  | Desc  |
|  ----  | ----  |
| HistoryServer.applications  | The size of all applications loaded in history server  |
| HistoryServer. incompleted  | The size of incompleted applications loaded in history server |
| HistoryServer. under.process  | The size of under process event log applications loaded in history server |
| HistorySerevr.diskManager.free | The size of free app info space about HistoryServerDiskManager |
| HistorySerevr.diskManager.committed | The size of committed app info about HistoryServerDiskManager |
| HistorySerevr.memoryManager.free | The size of free app info space in HistoryServerMemoryManager |
| HistorySerevr.memoryManager.current | The size of current app info  in HistoryServerMemoryManager |
| HistorySerevr.diskManager.diskAndMemorymanager.total | The total size of app info  in HistoryServerDiskManager and HistoryServerMemoryManger |
| HistoryServer. check.logs.timer  | The cost time of check event log |
| HistoryServer. clean.logs.timer  | The cost time of clean event log |
| HistoryServer. clean.driver.logs.timer  | The cost time of clean driver log |
| HistoryServer. compact.timer  | The cost time of compact event log |
| HistoryServer. loadStore.timer  | The cost time of load store log |
| ApplicationCache.history.cache.eviction.count | Application eviction count |
| ApplicationCache.history.cache.load.count | Application load cache count|
| ApplicationCache.history.cache.lookup.count | Application cache lookup count |
| ApplicationCache.history.cache.lookup.failure.count | Application look up failed count|
|  ApplicationCache.history.cache.load.timer | Application load time  | 


A demo metrics json:
```
{
  ""version"" : ""4.0.0"",
  ""gauges"" : {
    ""HistoryServer.applications"" : {
      ""value"" : 14
    },
    ""HistoryServer. incompleted"" : {
      ""value"" : 0
    },
    ""HistoryServer.under.process"" : {
      ""value"" : 0
    }
  },
  ""counters"" : {
    ""ApplicationCache.history.cache.eviction.count"" : {
      ""count"" : 0
    },
    ""ApplicationCache.history.cache.load.count"" : {
      ""count"" : 0
    },
    ""ApplicationCache.history.cache.lookup.count"" : {
      ""count"" : 0
    },
    ""ApplicationCache.history.cache.lookup.failure.count"" : {
      ""count"" : 0
    }
  },
  ""meters"" : { },
  ""timers"" : {
    ""ApplicationCache.history.cache.load.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.check.logs.timer"" : {
      ""count"" : 1,
      ""max"" : 735.628517,
      ""mean"" : 735.628517,
      ""min"" : 735.628517,
      ""p50"" : 735.628517,
      ""p75"" : 735.628517,
      ""p95"" : 735.628517,
      ""p98"" : 735.628517,
      ""p99"" : 735.628517,
      ""p999"" : 735.628517,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.4121821730552311,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.clean.driver.logs.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.clean.logs.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.compact.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    },
    ""HistoryServer.load.store.timer"" : {
      ""count"" : 0,
      ""max"" : 0.0,
      ""mean"" : 0.0,
      ""min"" : 0.0,
      ""p50"" : 0.0,
      ""p75"" : 0.0,
      ""p95"" : 0.0,
      ""p98"" : 0.0,
      ""p99"" : 0.0,
      ""p999"" : 0.0,
      ""stddev"" : 0.0,
      ""m15_rate"" : 0.0,
      ""m1_rate"" : 0.0,
      ""m5_rate"" : 0.0,
      ""mean_rate"" : 0.0,
      ""duration_units"" : ""milliseconds"",
      ""rate_units"" : ""calls/second""
    }
  }
}

```


### Why are the changes needed?
Add metrics to History Server, with this patch we can plugin these metrics to any Dropwizard supported monitoring infra and get insights into the performance of SHS.


### Does this PR introduce _any_ user-facing change?
User can use history server monitor data

### How was this patch tested?
Added UT


<img width=""1119"" alt=""截屏2022-01-08 下午7 45 38"" src=""https://user-images.githubusercontent.com/46485123/148642906-5d91d255-9e19-483c-909f-f7acabe81a94.png"">
",https://api.github.com/repos/apache/spark/issues/34326/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34326,https://github.com/apache/spark/pull/34326,https://github.com/apache/spark/pull/34326.diff,https://github.com/apache/spark/pull/34326.patch,,https://api.github.com/repos/apache/spark/issues/34326/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
191,https://api.github.com/repos/apache/spark/issues/34324,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34324/labels{/name},https://api.github.com/repos/apache/spark/issues/34324/comments,https://api.github.com/repos/apache/spark/issues/34324/events,https://github.com/apache/spark/pull/34324,1029888542,PR_kwDOAQXtWs4tXUbT,34324,[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-10-19T05:53:39Z,2022-01-26T10:38:15Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/streaming/dstream.py

### Why are the changes needed?
We can take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing tests",https://api.github.com/repos/apache/spark/issues/34324/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34324,https://github.com/apache/spark/pull/34324,https://github.com/apache/spark/pull/34324.diff,https://github.com/apache/spark/pull/34324.patch,,https://api.github.com/repos/apache/spark/issues/34324/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
192,https://api.github.com/repos/apache/spark/issues/34320,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34320/labels{/name},https://api.github.com/repos/apache/spark/issues/34320/comments,https://api.github.com/repos/apache/spark/issues/34320/events,https://github.com/apache/spark/pull/34320,1029793022,PR_kwDOAQXtWs4tXBxP,34320,[SPARK-18621][PYTHON] make sql type reprs eval-able,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1405801475, 'node_id': 'MDU6TGFiZWwxNDA1ODAxNDc1', 'url': 'https://api.github.com/repos/apache/spark/labels/ML', 'name': 'ML', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,24,2021-10-19T02:42:38Z,2021-12-29T18:25:18Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
These changes update the `__repr__` methods of type classes in `pyspark.sql.types` to print string representations which are `eval`-able. In other words, any instance of a `DataType` will produce a repr which can be passed to `eval()` to create an identical instance.

Similar changes previously submitted: https://github.com/apache/spark/pull/25495

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This [bug](https://issues.apache.org/jira/browse/SPARK-18621) has been around for a while. The current implementation returns a string representation which is valid in scala rather than python. These changes fix the repr to be valid with python.

The [motivation](https://docs.python.org/3/library/functions.html#repr) is ""to return a string that would yield an object with the same value when passed to eval()"".

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Example:

Current implementation:

```python
from pyspark.sql.types import *

struct = StructType([StructField('f1', StringType(), True)])
repr(struct)
# StructType(List(StructField(f1,StringType,true)))
new_struct = eval(repr(struct))
# Traceback (most recent call last):
#   File ""<input>"", line 1, in <module>
#   File ""<string>"", line 1, in <module>
# NameError: name 'List' is not defined

struct_field = StructField('f1', StringType(), True)
repr(struct_field)
# StructField(f1,StringType,true)
new_struct_field = eval(repr(struct_field))
# Traceback (most recent call last):
#   File ""<input>"", line 1, in <module>
#   File ""<string>"", line 1, in <module>
# NameError: name 'f1' is not defined
```

With changes:

```python
from pyspark.sql.types import *

struct = StructType([StructField('f1', StringType(), True)])
repr(struct)
# StructType([StructField('f1', StringType(), True)])
new_struct = eval(repr(struct))
struct == new_struct
# True

struct_field = StructField('f1', StringType(), True)
repr(struct_field)
# StructField('f1', StringType(), True)
new_struct_field = eval(repr(struct_field))
struct_field == new_struct_field
# True
```

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
The changes include a test which asserts that an instance of each type is equal to the `eval` of its `repr`, as in the above example.
",https://api.github.com/repos/apache/spark/issues/34320/timeline,,spark,apache,crflynn,16403734,MDQ6VXNlcjE2NDAzNzM0,https://avatars.githubusercontent.com/u/16403734?v=4,,https://api.github.com/users/crflynn,https://github.com/crflynn,https://api.github.com/users/crflynn/followers,https://api.github.com/users/crflynn/following{/other_user},https://api.github.com/users/crflynn/gists{/gist_id},https://api.github.com/users/crflynn/starred{/owner}{/repo},https://api.github.com/users/crflynn/subscriptions,https://api.github.com/users/crflynn/orgs,https://api.github.com/users/crflynn/repos,https://api.github.com/users/crflynn/events{/privacy},https://api.github.com/users/crflynn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34320,https://github.com/apache/spark/pull/34320,https://github.com/apache/spark/pull/34320.diff,https://github.com/apache/spark/pull/34320.patch,,https://api.github.com/repos/apache/spark/issues/34320/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
193,https://api.github.com/repos/apache/spark/issues/34316,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34316/labels{/name},https://api.github.com/repos/apache/spark/issues/34316/comments,https://api.github.com/repos/apache/spark/issues/34316/events,https://github.com/apache/spark/pull/34316,1029034299,PR_kwDOAQXtWs4tUsSZ,34316,[SPARK-37043][SQL] Cancel all running job after AQE plan finished,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-18T12:13:21Z,2021-11-30T12:08:56Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Cancel running job after AQE plan finished, so this PR add a `runningStages` in `AdaptiveExecutionContext` to record the running stages.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
We see stage was still running after AQE plan finished. This is because the plan which contains a join with one empty side has been converted to `LocalTableScanExec` during `AQEOptimizer`, but the other side of this join is still running (shuffle map stage).

It's no meaning to keep running the stage, so It's better to cancel the running stage after AQE plan finished in case wasting the task resource.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
no

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
add test.",https://api.github.com/repos/apache/spark/issues/34316/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34316,https://github.com/apache/spark/pull/34316,https://github.com/apache/spark/pull/34316.diff,https://github.com/apache/spark/pull/34316.patch,,https://api.github.com/repos/apache/spark/issues/34316/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
194,https://api.github.com/repos/apache/spark/issues/34293,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34293/labels{/name},https://api.github.com/repos/apache/spark/issues/34293/comments,https://api.github.com/repos/apache/spark/issues/34293/events,https://github.com/apache/spark/pull/34293,1027193162,PR_kwDOAQXtWs4tPWpA,34293,[SPARK-37014][PYTHON] Inline type hints for python/pyspark/streaming/context.py,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1984378631, 'node_id': 'MDU6TGFiZWwxOTg0Mzc4NjMx', 'url': 'https://api.github.com/repos/apache/spark/labels/DSTREAM', 'name': 'DSTREAM', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-10-15T08:11:00Z,2022-01-19T23:18:17Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Inline type hints for python/pyspark/streaming/context.py from Inline type hints for python/pyspark/streaming/context.pyi.

### Why are the changes needed?
Currently, there is type hint stub files python/pyspark/streaming/context.pyi to show the expected types for functions, but we can also take advantage of static type checking within the functions by inlining the type hints.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Existing test.",https://api.github.com/repos/apache/spark/issues/34293/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34293,https://github.com/apache/spark/pull/34293,https://github.com/apache/spark/pull/34293.diff,https://github.com/apache/spark/pull/34293.patch,,https://api.github.com/repos/apache/spark/issues/34293/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
195,https://api.github.com/repos/apache/spark/issues/34264,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34264/labels{/name},https://api.github.com/repos/apache/spark/issues/34264/comments,https://api.github.com/repos/apache/spark/issues/34264/events,https://github.com/apache/spark/pull/34264,1024195363,PR_kwDOAQXtWs4tF2j7,34264,[SPARK-36462][K8S] Add the ability to selectively disable watching or polling,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-10-12T19:17:23Z,2021-12-01T07:35:32Z,,CONTRIBUTOR,,False,"
### What changes were proposed in this pull request?

Add the ability to selectively disable watching or polling

### Why are the changes needed?

Watching or polling for pod status on Kubernetes can place additional load on etcd, with a large number of executors and large number of jobs this can have negative impacts and executors register themselves with the driver under normal operations anyways.

### Does this PR introduce _any_ user-facing change?

Two new config flags.


### How was this patch tested?

New unit tests + manually tested a forked version of this on an internal cluster with both watching and polling disabled.",https://api.github.com/repos/apache/spark/issues/34264/timeline,,spark,apache,holdenk,59893,MDQ6VXNlcjU5ODkz,https://avatars.githubusercontent.com/u/59893?v=4,,https://api.github.com/users/holdenk,https://github.com/holdenk,https://api.github.com/users/holdenk/followers,https://api.github.com/users/holdenk/following{/other_user},https://api.github.com/users/holdenk/gists{/gist_id},https://api.github.com/users/holdenk/starred{/owner}{/repo},https://api.github.com/users/holdenk/subscriptions,https://api.github.com/users/holdenk/orgs,https://api.github.com/users/holdenk/repos,https://api.github.com/users/holdenk/events{/privacy},https://api.github.com/users/holdenk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34264,https://github.com/apache/spark/pull/34264,https://github.com/apache/spark/pull/34264.diff,https://github.com/apache/spark/pull/34264.patch,,https://api.github.com/repos/apache/spark/issues/34264/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
196,https://api.github.com/repos/apache/spark/issues/34231,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34231/labels{/name},https://api.github.com/repos/apache/spark/issues/34231/comments,https://api.github.com/repos/apache/spark/issues/34231/events,https://github.com/apache/spark/pull/34231,1021635960,PR_kwDOAQXtWs4s-Dvi,34231,[SPARK-36964][CORE][YARN] Share cached dnsToSwitchMapping for yarn locality container requests,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,7,2021-10-09T07:41:22Z,2021-12-03T13:43:56Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

since SPARK-13704, spark re-implemented RackResolver and created a separate dnsToSwitchMapping instance. Here change to use RackResolver's internal dnsToSwitchMapping to reduce the time taken by YarnAllocator to add container requests.

### Why are the changes needed?

If submits a stage with abundant tasks, rack resolving takes a long time when YarnAllocator add requests with locality preference, which is caused by a large number of loops to execute the rack parsing script, eventually causing ExecutorAllocationManager request total Executors rpc  timeout.

### How was this patch tested?
UT +  manually testing on a 5w+ node cluster.",https://api.github.com/repos/apache/spark/issues/34231/timeline,,spark,apache,gaoyajun02,81629032,MDQ6VXNlcjgxNjI5MDMy,https://avatars.githubusercontent.com/u/81629032?v=4,,https://api.github.com/users/gaoyajun02,https://github.com/gaoyajun02,https://api.github.com/users/gaoyajun02/followers,https://api.github.com/users/gaoyajun02/following{/other_user},https://api.github.com/users/gaoyajun02/gists{/gist_id},https://api.github.com/users/gaoyajun02/starred{/owner}{/repo},https://api.github.com/users/gaoyajun02/subscriptions,https://api.github.com/users/gaoyajun02/orgs,https://api.github.com/users/gaoyajun02/repos,https://api.github.com/users/gaoyajun02/events{/privacy},https://api.github.com/users/gaoyajun02/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34231,https://github.com/apache/spark/pull/34231,https://github.com/apache/spark/pull/34231.diff,https://github.com/apache/spark/pull/34231.patch,,https://api.github.com/repos/apache/spark/issues/34231/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
197,https://api.github.com/repos/apache/spark/issues/34212,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34212/labels{/name},https://api.github.com/repos/apache/spark/issues/34212/comments,https://api.github.com/repos/apache/spark/issues/34212/events,https://github.com/apache/spark/pull/34212,1019923504,PR_kwDOAQXtWs4s4xo1,34212,[SPARK-36402][PYTHON] Implement Series.combine,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-10-07T11:17:52Z,2022-01-29T04:09:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Implement Series.combine

### Why are the changes needed?

Increase pandas API coverage in PySpark
### Does this PR introduce _any_ user-facing change?

User can use

```python
>>> s1 = ps.Series({'falcon': 330.0, 'eagle': 160.0})
>>> s1
falcon    330.0
eagle     160.0
dtype: float64

>>> s2 = ps.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})
>>> s2
falcon    345.0
eagle     200.0
duck       30.0
dtype: float64

>>> s1.combine(s2, max)
duck        NaN
eagle     200.0
falcon    345.0
dtype: float64
```
### How was this patch tested?

unit tests and docstest",https://api.github.com/repos/apache/spark/issues/34212/timeline,,spark,apache,dchvn,91461145,MDQ6VXNlcjkxNDYxMTQ1,https://avatars.githubusercontent.com/u/91461145?v=4,,https://api.github.com/users/dchvn,https://github.com/dchvn,https://api.github.com/users/dchvn/followers,https://api.github.com/users/dchvn/following{/other_user},https://api.github.com/users/dchvn/gists{/gist_id},https://api.github.com/users/dchvn/starred{/owner}{/repo},https://api.github.com/users/dchvn/subscriptions,https://api.github.com/users/dchvn/orgs,https://api.github.com/users/dchvn/repos,https://api.github.com/users/dchvn/events{/privacy},https://api.github.com/users/dchvn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34212,https://github.com/apache/spark/pull/34212,https://github.com/apache/spark/pull/34212.diff,https://github.com/apache/spark/pull/34212.patch,,https://api.github.com/repos/apache/spark/issues/34212/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
198,https://api.github.com/repos/apache/spark/issues/34122,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34122/labels{/name},https://api.github.com/repos/apache/spark/issues/34122/comments,https://api.github.com/repos/apache/spark/issues/34122/events,https://github.com/apache/spark/pull/34122,1008746689,PR_kwDOAQXtWs4sV-pu,34122,[SPARK-34826][SHUFFLE] Adaptively fetch shuffle mergers for push based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-09-27T22:46:32Z,2022-02-01T19:32:07Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Currently shuffle mergers are fetched before the start of the ShuffleMapStage. But for initial stages this can be problematic as shuffle mergers are nothing but unique hosts with shuffle services running which could be very few based on executors and this can cause merge ratio to be low. 

With this approach, `ShuffleMapTask` query for merger locations if not available and if available and start using this for pushing the blocks. Since partitions are mapped uniquely to a merger location, it should be fine to not push for the earlier set of tasks. This should improve the merge ratio for even initial stages.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Performance improvement. No new APIs change.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit tests and also has been working in our internal production environment for a while now.",https://api.github.com/repos/apache/spark/issues/34122/timeline,,spark,apache,venkata91,8871522,MDQ6VXNlcjg4NzE1MjI=,https://avatars.githubusercontent.com/u/8871522?v=4,,https://api.github.com/users/venkata91,https://github.com/venkata91,https://api.github.com/users/venkata91/followers,https://api.github.com/users/venkata91/following{/other_user},https://api.github.com/users/venkata91/gists{/gist_id},https://api.github.com/users/venkata91/starred{/owner}{/repo},https://api.github.com/users/venkata91/subscriptions,https://api.github.com/users/venkata91/orgs,https://api.github.com/users/venkata91/repos,https://api.github.com/users/venkata91/events{/privacy},https://api.github.com/users/venkata91/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34122,https://github.com/apache/spark/pull/34122,https://github.com/apache/spark/pull/34122.diff,https://github.com/apache/spark/pull/34122.patch,,https://api.github.com/repos/apache/spark/issues/34122/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
199,https://api.github.com/repos/apache/spark/issues/34098,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34098/labels{/name},https://api.github.com/repos/apache/spark/issues/34098/comments,https://api.github.com/repos/apache/spark/issues/34098/events,https://github.com/apache/spark/pull/34098,1006345272,PR_kwDOAQXtWs4sO97D,34098,[SPARK-36842][Core] TaskSchedulerImpl - stop TaskResultGetter properly,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-09-24T11:02:51Z,2021-10-31T13:58:10Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Catch exception during TaskSchedulerImpl.stop() so that all components can be stopped properly

### Why are the changes needed?
Otherwise some threads won't be stopped during spark session restart

### Does this PR introduce _any_ user-facing change?
NO

### How was this patch tested?
It's tested by 
1. create a new spark session in yarn-client mode
2. kill the spark application on yarn
3. check that the spark context is stopped and create a new spark session
4. do the above steps multiple times and verify that no task-result-getter threads number doesn't increase
",https://api.github.com/repos/apache/spark/issues/34098/timeline,,spark,apache,lxian,3442641,MDQ6VXNlcjM0NDI2NDE=,https://avatars.githubusercontent.com/u/3442641?v=4,,https://api.github.com/users/lxian,https://github.com/lxian,https://api.github.com/users/lxian/followers,https://api.github.com/users/lxian/following{/other_user},https://api.github.com/users/lxian/gists{/gist_id},https://api.github.com/users/lxian/starred{/owner}{/repo},https://api.github.com/users/lxian/subscriptions,https://api.github.com/users/lxian/orgs,https://api.github.com/users/lxian/repos,https://api.github.com/users/lxian/events{/privacy},https://api.github.com/users/lxian/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34098,https://github.com/apache/spark/pull/34098,https://github.com/apache/spark/pull/34098.diff,https://github.com/apache/spark/pull/34098.patch,,https://api.github.com/repos/apache/spark/issues/34098/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
200,https://api.github.com/repos/apache/spark/issues/34089,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34089/labels{/name},https://api.github.com/repos/apache/spark/issues/34089/comments,https://api.github.com/repos/apache/spark/issues/34089/events,https://github.com/apache/spark/pull/34089,1006016259,PR_kwDOAQXtWs4sN_AE,34089,[SPARK-36837][BUILD] Upgrade Kafka to 3.1.0,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,11,2021-09-24T02:34:04Z,2022-01-27T05:08:17Z,,MEMBER,,True,"### What changes were proposed in this pull request?

This PR aims to upgrade Kafka client library from 2.8.1 to 3.1.0.

### Why are the changes needed?

Kafka 3.1.0 has the following improvements and bug fixes including client side.
- https://downloads.apache.org/kafka/3.1.0/RELEASE_NOTES.html
- https://downloads.apache.org/kafka/3.0.0/RELEASE_NOTES.html

The following is the notable breaking changes.
- KAFKA-12554: Refactor Log layer
  - KIP-405: Log layer refactor https://docs.google.com/document/d/1dQJL4MCwqQJSPmZkVmVzshFZKuFy_bCPtubav4wBfHQ/edit#
- KAFKA-12945: Remove port, host.name and related configs in 3.0

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Pass the CIs.",https://api.github.com/repos/apache/spark/issues/34089/timeline,,spark,apache,dongjoon-hyun,9700541,MDQ6VXNlcjk3MDA1NDE=,https://avatars.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34089,https://github.com/apache/spark/pull/34089,https://github.com/apache/spark/pull/34089.diff,https://github.com/apache/spark/pull/34089.patch,,https://api.github.com/repos/apache/spark/issues/34089/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
201,https://api.github.com/repos/apache/spark/issues/34074,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34074/labels{/name},https://api.github.com/repos/apache/spark/issues/34074/comments,https://api.github.com/repos/apache/spark/issues/34074/events,https://github.com/apache/spark/pull/34074,1004856465,PR_kwDOAQXtWs4sKbtm,34074,[SPARK-33573][SHUFFLE][YARN] Shuffle server side metrics for Push-based shuffle,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-09-22T23:19:50Z,2022-01-05T22:14:12Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is one of the patches for SPARK-33235: Push-based Shuffle Improvement Tasks.
Added a class `PushMergeMetrics`, to collect below metrics from shuffle server side for Push-based shuffle:
- no opportunity responses
- too late responses
- pushed bytes written
- cached block bytes

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
This helps to understand the push based shuffle metrics from shuffle server side.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added a method `verifyMetrics` to verify those metrics in existing unit tests.

Lead-authored by: Chandni Singh chsingh@linkedin.com
Co-authored by: Min Shen mshen@linkedin.com
Co-authored by: Minchu Yang minyang@linkedin.com",https://api.github.com/repos/apache/spark/issues/34074/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34074,https://github.com/apache/spark/pull/34074,https://github.com/apache/spark/pull/34074.diff,https://github.com/apache/spark/pull/34074.patch,,https://api.github.com/repos/apache/spark/issues/34074/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
202,https://api.github.com/repos/apache/spark/issues/34070,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34070/labels{/name},https://api.github.com/repos/apache/spark/issues/34070/comments,https://api.github.com/repos/apache/spark/issues/34070/events,https://github.com/apache/spark/pull/34070,1003981877,PR_kwDOAQXtWs4sHqww,34070,[SPARK-36840][SQL] Support DPP if there is no selective predicate on the filtering side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-09-22T08:11:17Z,2021-11-23T13:56:09Z,,MEMBER,,False,"### What changes were proposed in this pull request?

This pr makes it insert a DPP if there is no selective predicate on the filtering side and it still has benefits even makes the filter ratio much smaller.


### Why are the changes needed?

In some cases, it may pruning a lot of data even if there is no selective predicate on the filtering side.






Before this PR | After this PR
--- | ---
![image](https://user-images.githubusercontent.com/5399861/134630846-fbec8def-a12d-4c77-bd82-084a04ab89c0.png)  | ![image](https://user-images.githubusercontent.com/5399861/134631736-d170c194-22e0-4ae0-a592-ef9e635866f2.png)


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test.",https://api.github.com/repos/apache/spark/issues/34070/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34070,https://github.com/apache/spark/pull/34070,https://github.com/apache/spark/pull/34070.diff,https://github.com/apache/spark/pull/34070.patch,,https://api.github.com/repos/apache/spark/issues/34070/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
203,https://api.github.com/repos/apache/spark/issues/34069,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34069/labels{/name},https://api.github.com/repos/apache/spark/issues/34069/comments,https://api.github.com/repos/apache/spark/issues/34069/events,https://github.com/apache/spark/pull/34069,1003887215,PR_kwDOAQXtWs4sHXUM,34069,[SPARK-36823][SQL] Support broadcast nested loop join hint for equi-join,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,25,2021-09-22T06:42:15Z,2021-10-27T17:11:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Add a new hint `BROADCAST_NL`

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For the join if one side is small and other side is large, the shuffle overhead is also very big. Due to the
bhj limitation, we can only broadcast right side for left join and left side for right join. So for the other case, we can try to use `BroadcastNestedLoopJoin` as the join strategy.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
yes, new hint

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Add new test in `JoinHintSuite`",https://api.github.com/repos/apache/spark/issues/34069/timeline,,spark,apache,ulysses-you,12025282,MDQ6VXNlcjEyMDI1Mjgy,https://avatars.githubusercontent.com/u/12025282?v=4,,https://api.github.com/users/ulysses-you,https://github.com/ulysses-you,https://api.github.com/users/ulysses-you/followers,https://api.github.com/users/ulysses-you/following{/other_user},https://api.github.com/users/ulysses-you/gists{/gist_id},https://api.github.com/users/ulysses-you/starred{/owner}{/repo},https://api.github.com/users/ulysses-you/subscriptions,https://api.github.com/users/ulysses-you/orgs,https://api.github.com/users/ulysses-you/repos,https://api.github.com/users/ulysses-you/events{/privacy},https://api.github.com/users/ulysses-you/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34069,https://github.com/apache/spark/pull/34069,https://github.com/apache/spark/pull/34069.diff,https://github.com/apache/spark/pull/34069.patch,,https://api.github.com/repos/apache/spark/issues/34069/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
204,https://api.github.com/repos/apache/spark/issues/34062,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34062/labels{/name},https://api.github.com/repos/apache/spark/issues/34062/comments,https://api.github.com/repos/apache/spark/issues/34062/events,https://github.com/apache/spark/pull/34062,1003178471,PR_kwDOAQXtWs4sE9i-,34062,[SPARK-36819][SQL] Don't insert redundant filters in case static partition pruning can be done,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,27,2021-09-21T19:58:51Z,2022-01-07T17:08:18Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Don't insert dynamic partition pruning filters in case the filters already added statically. In case the filtering predicate on dimension table is on joinKey, no need to insert DPP filter in that case.

Sample query:
```
SELECT f.date_id, f.pid, f.sid FROM
(select date_id, product_id as pid, store_id as sid from fact_stats) as f
JOIN dim_stats s
ON f.sid = s.store_id WHERE s.store_id = 3
```

Without this PR DPP filter is inserted for above query despite of `store_id#4551 = 3` on fact_stats
```
  == Physical Plan ==
  *(2) Project [date_id#4548, pid#4631, sid#4632]
  +- *(2) BroadcastHashJoin [sid#4632], [store_id#4552], Inner, BuildRight, false
     :- *(2) Project [date_id#4548, product_id#4549 AS pid#4631, store_id#4551 AS sid#4632]
     :  +- *(2) ColumnarToRow
     :     +- FileScan parquet default.fact_stats[date_id#4548,product_id#4549,store_id#4551] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/workspace/spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [(store_id#4551 = 3), isnotnull(store_id#4551), **dynamicpruningexpression(store_id#4551 IN dynamic...,** PushedFilters: [], ReadSchema: struct<date_id:int,product_id:int>
     :           +- SubqueryBroadcast dynamicpruning#4636, 0, [store_id#4552], [id=#2461]
     :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2460]
     :                 +- *(1) Filter (isnotnull(store_id#4552) AND (store_id#4552 = 3))
     :                    +- *(1) ColumnarToRow
     :                       +- FileScan parquet default.dim_stats[store_id#4552] Batched: true, DataFilters: [isnotnull(store_id#4552), (store_id#4552 = 3)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/workspace/spark/sql/core/spark-warehouse/org.apache.spark.sql..., PartitionFilters: [], PushedFilters: [IsNotNull(store_id), EqualTo(store_id,3)], ReadSchema: struct<store_id:int>
     +- ReusedExchange [store_id#4552], BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#2460]
```



### Why are the changes needed?
Having redundant dynamic filters can have  unnecessary overheads: extra filtering overhead + in case of DPP subquery case, subquery execution overhead.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Existing UTs pass. added new test case.",https://api.github.com/repos/apache/spark/issues/34062/timeline,,spark,apache,Swinky,5418286,MDQ6VXNlcjU0MTgyODY=,https://avatars.githubusercontent.com/u/5418286?v=4,,https://api.github.com/users/Swinky,https://github.com/Swinky,https://api.github.com/users/Swinky/followers,https://api.github.com/users/Swinky/following{/other_user},https://api.github.com/users/Swinky/gists{/gist_id},https://api.github.com/users/Swinky/starred{/owner}{/repo},https://api.github.com/users/Swinky/subscriptions,https://api.github.com/users/Swinky/orgs,https://api.github.com/users/Swinky/repos,https://api.github.com/users/Swinky/events{/privacy},https://api.github.com/users/Swinky/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34062,https://github.com/apache/spark/pull/34062,https://github.com/apache/spark/pull/34062.diff,https://github.com/apache/spark/pull/34062.patch,,https://api.github.com/repos/apache/spark/issues/34062/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
205,https://api.github.com/repos/apache/spark/issues/34056,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34056/labels{/name},https://api.github.com/repos/apache/spark/issues/34056/comments,https://api.github.com/repos/apache/spark/issues/34056/events,https://github.com/apache/spark/pull/34056,1001956323,PR_kwDOAQXtWs4sA024,34056,"[SPARK-36811][SQL] Add SQL functions for the BINARY data type for AND, OR, XOR, and NOT","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,44,2021-09-21T06:52:48Z,2021-11-18T12:50:36Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?

This PR introduces four new SQL functions operating on the BINARY data type.
  1. `bitand`: Takes as input two binary strings and optionally a string argument, and returns their bitwise AND.
  2. `bitor`: Takes as input two binary strings and optionally a string argument, and returns their bitwise OR.
  3. `bitxor`: Takes as input two binary strings and optionally a string argument, and returns their bitwise XOR.
  4. `bitnot`: Takes as input a binary string and returns its bitwise NOT.

In more detail, the first three functions behave as follows:
* If the input is only two binary sequences, they are expected to be of the same byte length, otherwise an exception is thrown. In this case the result is the bitwise operation that the function supports, and the byte length of the resulting byte sequence is equal to the common byte length of the two inputs.
* In the three-argument case, the third argument determines how the operation is performed in the case of byte sequences with unequal lengths. The third argument must be equal to either `lpad` or `rpad` (equality here is case insensitive), otherwise an exception is thrown. More precisely, if the third argument is equal to `lpad`, the shorter byte sequence is conceptually left-padded with zeros to match the length of the longer byte sequence, whereas if the third argument is `rpad`, the shorter byte sequence is conceptually right-padded with zeros to match the length of the longer sequence. Obviously, if the two input byte sequences are of the same length the result is the same for both values of the third argument, and equal to what the two-argument version of the function would have produced. In all cases, the byte length of the result is equal to the maximum of the byte lengths of the two input byte sequences.

#### Examples
* Equal length inputs, two argument overload, `bitand` function:
```scala
scala> sql(""select bitand(unhex('aabb'), unhex('03e3'))"").show
+--------------------------------------+
|bitand(unhex(aabb), unhex(03e3), lpad)|
+--------------------------------------+
|                               [02 A3]|
+--------------------------------------+
```
* Equal length inputs, three argument overload, `bitand` function:
```scala
scala> sql(""select bitand(unhex('aabb'), unhex('03e3'), 'lpad')"").show
+--------------------------------------+
|bitand(unhex(aabb), unhex(03e3), lpad)|
+--------------------------------------+
|                               [02 A3]|
+--------------------------------------+

scala> sql(""select bitand(unhex('aabb'), unhex('03e3'), 'rpad')"").show
+--------------------------------------+
|bitand(unhex(aabb), unhex(03e3), rpad)|
+--------------------------------------+
|                               [02 A3]|
+--------------------------------------+
```
* Unequal length inputs, three argument overload, `bitand` function:
```scala
scala> sql(""select bitand(unhex('aabb'), unhex('13e3ff'), 'lpad')"").show
+----------------------------------------+
|bitand(unhex(aabb), unhex(13e3ff), lpad)|
+----------------------------------------+
|                              [00 A2 BB]|
+----------------------------------------+

scala> sql(""select bitand(unhex('aabb'), unhex('13e3ff'), 'rpad')"").show
+----------------------------------------+
|bitand(unhex(aabb), unhex(13e3ff), rpad)|
+----------------------------------------+
|                              [02 A3 00]|
+----------------------------------------+
```
* Unequal length inputs, three argument overload, `bitor` function:
```scala
scala> sql(""select bitor(unhex('aabb'), unhex('13e3ff'), 'lpad')"").show
+---------------------------------------+
|bitor(unhex(aabb), unhex(13e3ff), lpad)|
+---------------------------------------+
|                             [13 EB FF]|
+---------------------------------------+

scala> sql(""select bitor(unhex('aabb'), unhex('13e3ff'), 'rpad')"").show
+---------------------------------------+
|bitor(unhex(aabb), unhex(13e3ff), rpad)|
+---------------------------------------+
|                             [BB FB FF]|
+---------------------------------------+
```
* Unequal length inputs, three argument overload, `bitxor` function:
```scala
scala> sql(""select bitxor(unhex('aabb'), unhex('13e3ff'), 'lpad')"").show
+----------------------------------------+
|bitxor(unhex(aabb), unhex(13e3ff), lpad)|
+----------------------------------------+
|                              [13 49 44]|
+----------------------------------------+


scala> sql(""select bitxor(unhex('aabb'), unhex('13e3ff'), 'rpad')"").show
+----------------------------------------+
|bitxor(unhex(aabb), unhex(13e3ff), rpad)|
+----------------------------------------+
|                              [B9 58 FF]|
+----------------------------------------+
```

### Why are the changes needed?

These functions are useful for performing bitwise operations on `BINARY` values, seen as bit sets.

Other databases offer similar or the same functionality. In more detail:
* Teradata supports them as [functions](https://docs.teradata.com/r/kmuOwjp1zEYg98JsB8fu_A/zNvtiufFYVtAA~wqxccYKA).
* Postgres supports them as [operators](https://www.postgresql.org/docs/9.4/functions-bitstring.html) over the `BIT` data type with the restriction that the inputs must be of equal length.
* MySQL supports them as [operators](https://dev.mysql.com/doc/refman/8.0/en/bit-functions.html). Arguments must have the same length.
* SQL Server supports them as [operators](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/bitwise-operators-transact-sql?view=sql-server-ver15), but only one of the two arguments can be a binary string.

Oracle, Snowflake, Hive SQL, Big Query do not support the proposed functions for binary strings, but rather only for integral types.

### Does this PR introduce _any_ user-facing change?

Yes. Four new SQL functions.

### How was this patch tested?

Unit tests.
",https://api.github.com/repos/apache/spark/issues/34056/timeline,,spark,apache,mkaravel,6397014,MDQ6VXNlcjYzOTcwMTQ=,https://avatars.githubusercontent.com/u/6397014?v=4,,https://api.github.com/users/mkaravel,https://github.com/mkaravel,https://api.github.com/users/mkaravel/followers,https://api.github.com/users/mkaravel/following{/other_user},https://api.github.com/users/mkaravel/gists{/gist_id},https://api.github.com/users/mkaravel/starred{/owner}{/repo},https://api.github.com/users/mkaravel/subscriptions,https://api.github.com/users/mkaravel/orgs,https://api.github.com/users/mkaravel/repos,https://api.github.com/users/mkaravel/events{/privacy},https://api.github.com/users/mkaravel/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34056,https://github.com/apache/spark/pull/34056,https://github.com/apache/spark/pull/34056.diff,https://github.com/apache/spark/pull/34056.patch,,https://api.github.com/repos/apache/spark/issues/34056/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
206,https://api.github.com/repos/apache/spark/issues/34010,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34010/labels{/name},https://api.github.com/repos/apache/spark/issues/34010/comments,https://api.github.com/repos/apache/spark/issues/34010/events,https://github.com/apache/spark/pull/34010,997566830,PR_kwDOAQXtWs4rzp_v,34010,"[SPARK-36770][SQL] Replace Unbounded Following window functions with Unbounded Preceding window function (First, Last)","[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,1,2021-09-15T22:19:22Z,2021-12-13T23:21:39Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
**Context**

The **UnboundedFollowingWindowFunctionFrame** has the time complexity of O(N^2), N is the number of rows in the current partition, more specific the complexity is O(N* (N - 1)/2).

**What happens internally in UnboundedFollowingWindowFunctionFrame?**
In the window frame, while processing each incoming row, it will go through current row till the end of partition to do re-calculation. This process will be repeated on each incoming row, which causes the high run-time complexity.

But UnboundedPrecedingWindowFunctionFrame has much better time complexity O(N), N is the number of rows in the current partition.

 
**What is the idea of the improvement?**

Give the big time complexity difference between UnboundedFollowingWindowFunctionFrame and UnboundedPrecedingWindowFunctionFrame, we can do following conversions to improve the time complexity of first() and last() from O(N^2) to O(N)

```
case 1:
first() OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING) 
converts to 
last()  OVER(PARTITION BY colA ORDER BY colB DEAC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)

case 2:
last()  OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING) 
converts to 
first() OVER(PARTITION BY colA ORDER BY colB DESC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)
```

**Summary**

Replace ""UNBOUNDED FOLLOWING"" with ""UNBOUNDED PRECEDING"", and flip the ORDER BY for the window functions first() and last() for ROWS.


### Why are the changes needed?
Improve the run-time performance for window function fist() and last() against ROWS UNBOUNDED FOLLOWING.


### Does this PR introduce _any_ user-facing change?
NO


### How was this patch tested?
Added unit test: sql/core/src/test/scala/org/apache/spark/sql/WindowFunctionOptimizerTestSuite.scala

",https://api.github.com/repos/apache/spark/issues/34010/timeline,,spark,apache,guibin,289256,MDQ6VXNlcjI4OTI1Ng==,https://avatars.githubusercontent.com/u/289256?v=4,,https://api.github.com/users/guibin,https://github.com/guibin,https://api.github.com/users/guibin/followers,https://api.github.com/users/guibin/following{/other_user},https://api.github.com/users/guibin/gists{/gist_id},https://api.github.com/users/guibin/starred{/owner}{/repo},https://api.github.com/users/guibin/subscriptions,https://api.github.com/users/guibin/orgs,https://api.github.com/users/guibin/repos,https://api.github.com/users/guibin/events{/privacy},https://api.github.com/users/guibin/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34010,https://github.com/apache/spark/pull/34010,https://github.com/apache/spark/pull/34010.diff,https://github.com/apache/spark/pull/34010.patch,,https://api.github.com/repos/apache/spark/issues/34010/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
207,https://api.github.com/repos/apache/spark/issues/34009,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34009/labels{/name},https://api.github.com/repos/apache/spark/issues/34009/comments,https://api.github.com/repos/apache/spark/issues/34009/events,https://github.com/apache/spark/pull/34009,997562866,PR_kwDOAQXtWs4rzpOV,34009,[SPARK-34378][SQL][AVRO] Enhance AvroSerializer validation to allow extra nullable Avro fields,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1988040187, 'node_id': 'MDU6TGFiZWwxOTg4MDQwMTg3', 'url': 'https://api.github.com/repos/apache/spark/labels/AVRO', 'name': 'AVRO', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,33,2021-09-15T22:12:39Z,2022-01-28T21:14:48Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Loosen the schema validation logic in `AvroSerializer` to accommodate the situation where a user has provided an explicit schema (via `avroSchema`) and this schema has extra fields which are not present in the Catalyst schema (the DF being written). Specifically, extra _nullable_ fields will be allowed and populated as null. _Required_ fields (non-null) will still be checked for existence.

### Why are the changes needed?
It's common for Avro schemas to evolve in a _compatible_ way (as discussed in Confluent's documentation on [Schema Evolution and Compatibility](https://docs.confluent.io/platform/current/schema-registry/avro.html); here I refer to `FULL` compatibility). Under such a scenario, new _optional_ fields are added to a schema. Producers are free to include the new field if they so choose, and consumers are free to read the new field if they so choose. It is optional on both sides.

Consider the following code:
```
val outputSchema = getOutputSchema()
df.write.format(""avro"").option(""avroSchema"", outputSchema).save(...)
```
If you have a situation where schemas are managed in some centralized repository (e.g. a [schema registry](https://docs.confluent.io/platform/current/schema-registry/index.html)), `outputSchema` may update at some point to add a new optional field, without you necessarily initiating any action on your side as a data producer. With the current code, this would cause the producer job to break, because validation would complain that the newly added field is not present in the DataFrame. Really, the producer should be able to continue producing data as normal even without adding the new field to the DataFrame it is writing out, because the field is optional.

### Does this PR introduce _any_ user-facing change?
Yes, when using the `avroSchema` option on the Avro data source during writes, validation is less strict, and allows for (compatible) schema evolution to be handled more gracefully.

### How was this patch tested?
New unit tests added. We've also been employing this logic internally for a few years, though the implementation was quite different due to recent changes in this area of the code.",https://api.github.com/repos/apache/spark/issues/34009/timeline,,spark,apache,xkrogen,6570401,MDQ6VXNlcjY1NzA0MDE=,https://avatars.githubusercontent.com/u/6570401?v=4,,https://api.github.com/users/xkrogen,https://github.com/xkrogen,https://api.github.com/users/xkrogen/followers,https://api.github.com/users/xkrogen/following{/other_user},https://api.github.com/users/xkrogen/gists{/gist_id},https://api.github.com/users/xkrogen/starred{/owner}{/repo},https://api.github.com/users/xkrogen/subscriptions,https://api.github.com/users/xkrogen/orgs,https://api.github.com/users/xkrogen/repos,https://api.github.com/users/xkrogen/events{/privacy},https://api.github.com/users/xkrogen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34009,https://github.com/apache/spark/pull/34009,https://github.com/apache/spark/pull/34009.diff,https://github.com/apache/spark/pull/34009.patch,,https://api.github.com/repos/apache/spark/issues/34009/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
208,https://api.github.com/repos/apache/spark/issues/34000,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/34000/labels{/name},https://api.github.com/repos/apache/spark/issues/34000/comments,https://api.github.com/repos/apache/spark/issues/34000/events,https://github.com/apache/spark/pull/34000,996465213,PR_kwDOAQXtWs4rwNS7,34000,[SPARK-36620][SHUFFLE] Add client side push based shuffle metrics,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-09-14T21:42:22Z,2021-12-16T11:48:04Z,,CONTRIBUTOR,,False," ### What changes were proposed in this pull request?
This adds the following push based shuffle metrics like :
- Merger count and magnet enabled/disabled for stage
- Time spent on results finalization
- Counting actual number of blocks other than chunks
- Corrupt shuffle blocks chunks and fallback

 ### Why are the changes needed?
These changes help to understand the push based shuffle metrics of the application

 ### Does this PR introduce _any_ user-facing change?
Changes to API responses by SHS (eg: /stages)

 ### How was this patch tested?
Modified existing unit tests and also tested API response on event log files in SHS

Lead-authored by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>
Co-authored by: Chandni Singh <chsingh@linkedin.com>
Co-authored by: Thejdeep Gudivada <tgudivada@linkedin.com>",https://api.github.com/repos/apache/spark/issues/34000/timeline,,spark,apache,thejdeep,1708757,MDQ6VXNlcjE3MDg3NTc=,https://avatars.githubusercontent.com/u/1708757?v=4,,https://api.github.com/users/thejdeep,https://github.com/thejdeep,https://api.github.com/users/thejdeep/followers,https://api.github.com/users/thejdeep/following{/other_user},https://api.github.com/users/thejdeep/gists{/gist_id},https://api.github.com/users/thejdeep/starred{/owner}{/repo},https://api.github.com/users/thejdeep/subscriptions,https://api.github.com/users/thejdeep/orgs,https://api.github.com/users/thejdeep/repos,https://api.github.com/users/thejdeep/events{/privacy},https://api.github.com/users/thejdeep/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/34000,https://github.com/apache/spark/pull/34000,https://github.com/apache/spark/pull/34000.diff,https://github.com/apache/spark/pull/34000.patch,,https://api.github.com/repos/apache/spark/issues/34000/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
209,https://api.github.com/repos/apache/spark/issues/33986,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33986/labels{/name},https://api.github.com/repos/apache/spark/issues/33986/comments,https://api.github.com/repos/apache/spark/issues/33986/events,https://github.com/apache/spark/pull/33986,995496899,PR_kwDOAQXtWs4rtG9r,33986,[SPARK-36727][SQL]Support sql overwrite a path that is also being read from when partit…,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-14T01:49:05Z,2021-10-31T09:05:47Z,,NONE,,False,"### What changes were proposed in this pull request?
```
// non-partitioned table overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET;
INSERT OVERWRITE TABLE tbl SELECT 0,1;
INSERT OVERWRITE TABLE tbl SELECT * FROM tbl;

// partitioned table static overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET PARTITIONED BY (pt1 INT);
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT 0 AS col1,1 AS col2;
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT col1, col2 FROM WHERE p1=2021;
```
When we run the above query, an error will be throwed ""Cannot overwrite a path that is also being read from""
We need to support this operation when the spark.sql.sources.partitionOverwriteMode is dynamic

### How was this patch tested?
Unit tests -> InsertSuite.scala
",https://api.github.com/repos/apache/spark/issues/33986/timeline,,spark,apache,TongWei1105,68682646,MDQ6VXNlcjY4NjgyNjQ2,https://avatars.githubusercontent.com/u/68682646?v=4,,https://api.github.com/users/TongWei1105,https://github.com/TongWei1105,https://api.github.com/users/TongWei1105/followers,https://api.github.com/users/TongWei1105/following{/other_user},https://api.github.com/users/TongWei1105/gists{/gist_id},https://api.github.com/users/TongWei1105/starred{/owner}{/repo},https://api.github.com/users/TongWei1105/subscriptions,https://api.github.com/users/TongWei1105/orgs,https://api.github.com/users/TongWei1105/repos,https://api.github.com/users/TongWei1105/events{/privacy},https://api.github.com/users/TongWei1105/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33986,https://github.com/apache/spark/pull/33986,https://github.com/apache/spark/pull/33986.diff,https://github.com/apache/spark/pull/33986.patch,,https://api.github.com/repos/apache/spark/issues/33986/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
210,https://api.github.com/repos/apache/spark/issues/33980,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33980/labels{/name},https://api.github.com/repos/apache/spark/issues/33980/comments,https://api.github.com/repos/apache/spark/issues/33980/events,https://github.com/apache/spark/pull/33980,994925162,MDExOlB1bGxSZXF1ZXN0NzMyNzc1NTk2,33980,[SPARK-32285][PYTHON] Add PySpark support for nested timestamps with arrow,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,14,2021-09-13T13:57:23Z,2021-11-11T12:20:25Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Added nested timestamp support for Pyspark with Arrow
Following code will run through this PR


```
from pyspark.sql.types import StructType, TimestampType, StructField, ArrayType, LongType
spark.conf.set(""spark.sql.execution.arrow.pyspark.fallback.enabled"", ""False"")
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""True"")

import datetime
import pandas as pd
origin = pd.DataFrame({""a"": [[datetime.datetime(2012, 2, 2, 2, 2, 2)]]})
df = spark.createDataFrame(origin, schema=StructType([StructField(""a"", ArrayType(TimestampType()), True)]))
ts = datetime.datetime(2015, 11, 1, 0, 30)

schema = StructType([StructField(""a"", ArrayType(TimestampType()), True)])

df = spark.createDataFrame([([ts, ts],)], schema=schema)

df.toPandas()
```


### Why are the changes needed?
This change is required to convert ArrayType(TimeStamp) to pandas via arrow. 


### Does this PR introduce any user-facing change?
Yes user will be able to convert DF which contain Arraytype(Timestamp) to pandas

### How was this patch tested?
unit tests",https://api.github.com/repos/apache/spark/issues/33980/timeline,,spark,apache,pralabhkumar,16147255,MDQ6VXNlcjE2MTQ3MjU1,https://avatars.githubusercontent.com/u/16147255?v=4,,https://api.github.com/users/pralabhkumar,https://github.com/pralabhkumar,https://api.github.com/users/pralabhkumar/followers,https://api.github.com/users/pralabhkumar/following{/other_user},https://api.github.com/users/pralabhkumar/gists{/gist_id},https://api.github.com/users/pralabhkumar/starred{/owner}{/repo},https://api.github.com/users/pralabhkumar/subscriptions,https://api.github.com/users/pralabhkumar/orgs,https://api.github.com/users/pralabhkumar/repos,https://api.github.com/users/pralabhkumar/events{/privacy},https://api.github.com/users/pralabhkumar/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33980,https://github.com/apache/spark/pull/33980,https://github.com/apache/spark/pull/33980.diff,https://github.com/apache/spark/pull/33980.patch,,https://api.github.com/repos/apache/spark/issues/33980/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
211,https://api.github.com/repos/apache/spark/issues/33941,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33941/labels{/name},https://api.github.com/repos/apache/spark/issues/33941/comments,https://api.github.com/repos/apache/spark/issues/33941/events,https://github.com/apache/spark/pull/33941,991698454,MDExOlB1bGxSZXF1ZXN0NzMwMTI2MjI4,33941,[WIP][SPARK-36699][Core] Reuse compatible executors for stage-level scheduling,"[{'id': 1406605079, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDc5', 'url': 'https://api.github.com/repos/apache/spark/labels/WEB%20UI', 'name': 'WEB UI', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,10,2021-09-09T02:43:22Z,2022-01-26T06:14:23Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

https://issues.apache.org/jira/browse/SPARK-36699

We proposed to optionally change behavior of stage-level scheduling by reusing compatible executors. Two executors binding to different resource profiles are **compatible** only when the executorResources (cores in particular if not defining custom resources) are the same, but taskResources can be different. When the executors are compatible, the tasks can be allocated to any of them even when in the different profiles. Users defining profiles should make sure the different taskResources are properly specified against the same executorResources. 
A SparkConf option `spark.dynamicAllocation.reuseExecutors` is defined to change the default behavior which is not reusing executors. When this option is turned on, dynamic allocation will count all compatible executors number to meet init/min/max executor number restrictions. 
The first PR will focus on reusing executors with same cores without custom resources.

### Why are the changes needed?
Current stage-level scheduling allocated separated set of executors for different executor profiles. This approach simplified implementation, however is a waste of executor resources when the existing executors have enough resources to run the following tasks.

The typical user scenario is for different stages, user wants to use different core number for the task with same executor resources. For instance in CPU machine learning scenario, to achieve the best performance, given the same executor resources, when in ETL stage, user will allocate 1 core per task and many tasks, and in the following CPU training stage, user will use more cores per task and less tasks. In the existing implementation, two separated profiles and executors are created. Reusing executors will get better CPU resource utilization and better performance.

### Does this PR introduce _any_ user-facing change?
No

### How was this patch tested?
Unit tests

",https://api.github.com/repos/apache/spark/issues/33941/timeline,,spark,apache,xwu99,23566414,MDQ6VXNlcjIzNTY2NDE0,https://avatars.githubusercontent.com/u/23566414?v=4,,https://api.github.com/users/xwu99,https://github.com/xwu99,https://api.github.com/users/xwu99/followers,https://api.github.com/users/xwu99/following{/other_user},https://api.github.com/users/xwu99/gists{/gist_id},https://api.github.com/users/xwu99/starred{/owner}{/repo},https://api.github.com/users/xwu99/subscriptions,https://api.github.com/users/xwu99/orgs,https://api.github.com/users/xwu99/repos,https://api.github.com/users/xwu99/events{/privacy},https://api.github.com/users/xwu99/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33941,https://github.com/apache/spark/pull/33941,https://github.com/apache/spark/pull/33941.diff,https://github.com/apache/spark/pull/33941.patch,,https://api.github.com/repos/apache/spark/issues/33941/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
212,https://api.github.com/repos/apache/spark/issues/33934,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33934/labels{/name},https://api.github.com/repos/apache/spark/issues/33934/comments,https://api.github.com/repos/apache/spark/issues/33934/events,https://github.com/apache/spark/pull/33934,990760324,MDExOlB1bGxSZXF1ZXN0NzI5MzIyMDg5,33934,[SPARK-36691][PYTHON] PythonRunner failed should pass error message to ApplicationMaster too,"[{'id': 1427970157, 'node_id': 'MDU6TGFiZWwxNDI3OTcwMTU3', 'url': 'https://api.github.com/repos/apache/spark/labels/YARN', 'name': 'YARN', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,34,2021-09-08T06:21:37Z,2022-01-08T10:19:40Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
In current pyspark, stderr and stdout are print together, if python script exit, PythonRunner will only throw a `SparkUserAppsException` with exit code 1. Then pass this error to AM.
In cluster mode, client side only got exception `SparkUserAppsException` and show
```
User application exited with 1.
```
Without correct error message. Then user need to  check ApplicationMaster's stdout log file to find out why their job failed. 

In this pr, make PythonRunner can throw exception message to backend.


### Why are the changes needed?
Make user to know error message more easy.


### Does this PR introduce _any_ user-facing change?
In cluster mode, user can directly see pyspark's error message in client side.

### How was this patch tested?
If we run a sql with wrong table in python script. In ApplicationMaster and client side log will show
```
21/09/08 14:08:42 ERROR Client: Application diagnostics message: User application exited with 1.
Exception in thread ""main"" org.apache.spark.SparkException: Application application_1630930053097_708441 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1150)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1530)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```

Now will show
```
21/09/08 14:08:42 ERROR Client: Application diagnostics message: User application exited with 1 and error message Traceback (most recent call last):
  File ""test.py"", line 68, in <module>
    res = client.sql(exec_sql)
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/pyspark.zip/pyspark/sql/session.py"", line 767, in sql
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__
  File ""/mnt/ssd/0/yarn/nm-local-dir/usercache/yi.zhu/appcache/application_1630930053097_708441/container_e236_1630930053097_708441_02_000002/pyspark.zip/pyspark/sql/utils.py"", line 69, in deco
pyspark.sql.utils.AnalysisException: u""Table or view not found: `xxxx`.`xxxxxx`; line 14 pos 9;\n'InsertIntoTable 'UnresolvedRelation `xxxx`.`xxxxxx`, Map(dt -> None, country -> None), true, false\n+- 'Repartition 50, true\n   +- 'Project [cast('get_json_object('data, $.shopid) as bigint) AS shopid#4, cast('get_json_object('data, $.itemid) as bigint) AS itemid#5, cast('get_json_object('data, $.quantity) as bigint) AS quantity#6, 'userid, 'platform, 'page_type, 'log_timestamp, 'utc_date AS dt#7, 'grass_region AS country#8]\n      +- 'Filter ((('utc_date = cast(2021-01-01 as date)) && ('grass_region = ID)) && ('operation = action_add_to_cart_success))\n         +- 'SubqueryAlias `di`\n            +- 'UnresolvedRelation `xxxxxx`.`xxxxxx`\n""

Exception in thread ""main"" org.apache.spark.SparkException: Application application_1630930053097_708441 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1150)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1530)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```
",https://api.github.com/repos/apache/spark/issues/33934/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33934,https://github.com/apache/spark/pull/33934,https://github.com/apache/spark/pull/33934.diff,https://github.com/apache/spark/pull/33934.patch,,https://api.github.com/repos/apache/spark/issues/33934/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
213,https://api.github.com/repos/apache/spark/issues/33932,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33932/labels{/name},https://api.github.com/repos/apache/spark/issues/33932/comments,https://api.github.com/repos/apache/spark/issues/33932/events,https://github.com/apache/spark/pull/33932,990713129,MDExOlB1bGxSZXF1ZXN0NzI5MjgxOTA3,33932,[SPARK-33781][SHUFFLE] Improve caching of MergeStatus on the executor side to save memory,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-09-08T04:52:23Z,2022-01-03T16:58:31Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This is one of the patches for SPARK-33235: Push-based Shuffle Improvement Tasks.
At high level, in `MapOutputTrackerWorker`, if serialized `MergeStatuse` array size is larger than threshold(`spark.shuffle.push.mergeResult.minSizeForReducedCache`), cache the much more compact serialized bytes instead and only cache the deserialized `MergeStatus` objects that are needed (within `startPartitionId` until `endPartitionId`). Then deserialize the `MergeStatus` array from the cached serialized bytes again.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
For large shuffles with 10s or 100s of thousands of shuffle partitions, caching the entire deserialized and decompressed MergeStatus array on the executor side, while perhaps only 0.1% of them are going to be used by the tasks running in this executor is a huge waste of memory.
This change helps save memory as well as helps with reducing GC pressure on executor side.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes. This PR introduces a client-side config for push-based shuffle(`spark.shuffle.push.mergeResult.minSizeForReducedCache`). If push-based shuffle is turned-off then the users will not see any change.


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test.
Verified effectiveness with jobs that would fail due to GC issue otherwise.

Ran the benchmark using GitHub Actions and did not observe any performance penalties. The results are attached in this PR:
```
core/benchmarks/MapStatusesSerDeserBenchmark-jdk11-results.txt
core/benchmarks/MapStatusesSerDeserBenchmark-results.txt
```

Lead-authored-by: Min Shen mshen@linkedin.com
Co-authored-by: Chandni Singh chsingh@linkedin.com
Co-authored-by: Minchu Yang minyang@linkedin.com",https://api.github.com/repos/apache/spark/issues/33932/timeline,,spark,apache,rmcyang,31781684,MDQ6VXNlcjMxNzgxNjg0,https://avatars.githubusercontent.com/u/31781684?v=4,,https://api.github.com/users/rmcyang,https://github.com/rmcyang,https://api.github.com/users/rmcyang/followers,https://api.github.com/users/rmcyang/following{/other_user},https://api.github.com/users/rmcyang/gists{/gist_id},https://api.github.com/users/rmcyang/starred{/owner}{/repo},https://api.github.com/users/rmcyang/subscriptions,https://api.github.com/users/rmcyang/orgs,https://api.github.com/users/rmcyang/repos,https://api.github.com/users/rmcyang/events{/privacy},https://api.github.com/users/rmcyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33932,https://github.com/apache/spark/pull/33932,https://github.com/apache/spark/pull/33932.diff,https://github.com/apache/spark/pull/33932.patch,,https://api.github.com/repos/apache/spark/issues/33932/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
214,https://api.github.com/repos/apache/spark/issues/33914,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33914/labels{/name},https://api.github.com/repos/apache/spark/issues/33914/comments,https://api.github.com/repos/apache/spark/issues/33914/events,https://github.com/apache/spark/pull/33914,988464013,MDExOlB1bGxSZXF1ZXN0NzI3MzY4MDY0,33914,[SPARK-32268][SQL] Dynamic bloom filter join pruning,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,20,2021-09-05T09:26:44Z,2021-12-10T02:25:31Z,,MEMBER,,True,"### What changes were proposed in this pull request?

Reduce the shuffle data can significantly improve the query performance and increase Spark cluster stability.

This PR implements dynamic bloom filter join pruning to reduce the shuffle data. The main changes:
- Add `BuildBloomFilter` and `InBloomFilter` UDF.
- Enhance `RepartitionByExpression` to support shuffle with `ENSURE_REQUIREMENTS` origin. Dynamic bloom filter join pruning use it to reuse exchange.
- Add a new rule `DynamicBloomFilterPruning`.
- Dynamic bloom filter join pruning supports AQE.
- Support dynamic bloom filter nested with dynamic partition pruning when AQE enabled.
- Add an AQE optimization rule(`OptimizeBloomFilterJoin`) to update the `expectedNumItems` base on stats collected by AQE which is more accurate.



### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:

SQL | Before this PR | After this PR
--- | --- | ---
q12 | 11 | 8
q20 | 6 | 6
q37 | 29 | 13
q50 | 78 | 29
q80 | 14 | 14
q82 | 38 | 16
q93 | 126 | 108
q98 | 6 | 6



</body>

</html>
",https://api.github.com/repos/apache/spark/issues/33914/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33914,https://github.com/apache/spark/pull/33914,https://github.com/apache/spark/pull/33914.diff,https://github.com/apache/spark/pull/33914.patch,,https://api.github.com/repos/apache/spark/issues/33914/reactions,6,5,0,0,0,0,1,0,0,,,,,,,,,,,,,,,,,,
215,https://api.github.com/repos/apache/spark/issues/33893,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33893/labels{/name},https://api.github.com/repos/apache/spark/issues/33893/comments,https://api.github.com/repos/apache/spark/issues/33893/events,https://github.com/apache/spark/pull/33893,985082957,MDExOlB1bGxSZXF1ZXN0NzI0NDM3OTE4,33893,[SPARK-36638][SQL] Generalize OptimizeSkewedJoin,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,48,2021-09-01T12:19:25Z,2022-01-27T14:29:21Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR aims to generalize `OptimizeSkewedJoin` to support all patterns that can be handled by current _split-duplicate_ strategy:

1, find the _splittable_ shuffle query stages by the semantics of internal nodes;

2, for each _splittable_ shuffle query stage, check whether skew partitions exists, if true, split them into specs;

3, handle _Combinatorial Explosion_: for each skew partition, check whether the combination number is too large, if so, re-split the stages to keep a reasonable number of combinations. For example, for partition 0, stage A/B/C are split into 100/100/100 specs, respectively. Then there are 1M combinations, which is too large, and will cause performance regression.

4, attach new specs to shuffle query stages;


### Why are the changes needed?
to Generalize OptimizeSkewedJoin 


### Does this PR introduce _any_ user-facing change?
two additional configs are added


### How was this patch tested?
existing testsuites, added testsuites, some cases on our productive system
",https://api.github.com/repos/apache/spark/issues/33893/timeline,,spark,apache,zhengruifeng,7322292,MDQ6VXNlcjczMjIyOTI=,https://avatars.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33893,https://github.com/apache/spark/pull/33893,https://github.com/apache/spark/pull/33893.diff,https://github.com/apache/spark/pull/33893.patch,,https://api.github.com/repos/apache/spark/issues/33893/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
216,https://api.github.com/repos/apache/spark/issues/33828,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33828/labels{/name},https://api.github.com/repos/apache/spark/issues/33828/comments,https://api.github.com/repos/apache/spark/issues/33828/events,https://github.com/apache/spark/pull/33828,978663528,MDExOlB1bGxSZXF1ZXN0NzE5MjE1ODU3,33828,[SPARK-36579][CORE][SQL] Make spark source stagingDir can be customized,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406587328, 'node_id': 'MDU6TGFiZWwxNDA2NTg3MzI4', 'url': 'https://api.github.com/repos/apache/spark/labels/STRUCTURED%20STREAMING', 'name': 'STRUCTURED STREAMING', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,87,2021-08-25T03:14:48Z,2022-01-11T02:27:04Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Consider such cases:

1. we close a job when it is doing dynamic partition insert, it will remain such staging dir under table's path.  So we make the staging dir customized like hive can avoid remain such staging dir under table path.
2. In hive's API, if we specify a staging dir, not use default staging dir (under table path), it can directly rename to target path and can avoid many hdfs file operations. In spark currently only dynamic partition insert support staging dir, we can do this like https://github.com/apache/spark/pull/33811
3. We can support add a file commit protocol that support staging dir for all types of insert, then when we use that commit protocol, wen can do:
    - Insert into non-partitioned table form it self
    - Insert into partition table's statistic partition and read data from target partition
    - Insert into different partition using statistic partition together

### Why are the changes needed?
Make spark data source insert's  stagingDir can be customized and then we can do more optimize base on this.


### Does this PR introduce _any_ user-facing change?
User can define staging dir by `spark.exec.stagingDir`

### How was this patch tested?
Added UT
",https://api.github.com/repos/apache/spark/issues/33828/timeline,,spark,apache,AngersZhuuuu,46485123,MDQ6VXNlcjQ2NDg1MTIz,https://avatars.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33828,https://github.com/apache/spark/pull/33828,https://github.com/apache/spark/pull/33828.diff,https://github.com/apache/spark/pull/33828.patch,,https://api.github.com/repos/apache/spark/issues/33828/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
217,https://api.github.com/repos/apache/spark/issues/33744,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33744/labels{/name},https://api.github.com/repos/apache/spark/issues/33744/comments,https://api.github.com/repos/apache/spark/issues/33744/events,https://github.com/apache/spark/pull/33744,970956310,MDExOlB1bGxSZXF1ZXN0NzEyODExMjI1,33744,[SPARK-36403][PYTHON] Implement `Index.putmask` ,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,44,2021-08-14T17:04:02Z,2021-11-12T15:12:10Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

Implement `Index.putmask`

This pull request is based on https://github.com/databricks/koalas/pull/1560


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

`putmask` returns a new Index of the values set with the mask.
`putmask` is supported in pandas. PySpark should support that as well.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes. `Index.putmask` can be used.
```python
>>> pidx = pd.Index([""a"", ""b"", ""c"", ""d"", ""e""])
>>> psidx = ps.from_pandas(pidx)
>>> psidx.putmask(psidx < ""c"", ""k"").sort_values()
Index(['c', 'd', 'e', 'k', 'k'], dtype='object')
>>> psidx.putmask(psidx < ""c"", [""g"", ""h"", ""i"", ""j"", ""k""]).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", (""g"", ""h"", ""i"", ""j"", ""k"")).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", ps.Index([""g"", ""h"", ""i"", ""j"", ""k""])).sort_values()
Index(['c', 'd', 'e', 'g', 'h'], dtype='object')
>>> psidx.putmask(psidx < ""c"", ""MASKED"").sort_values()
Index(['MASKED', 'MASKED', 'c', 'd', 'e'], dtype='object')
```


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit tests.
",https://api.github.com/repos/apache/spark/issues/33744/timeline,,spark,apache,beobest2,7010554,MDQ6VXNlcjcwMTA1NTQ=,https://avatars.githubusercontent.com/u/7010554?v=4,,https://api.github.com/users/beobest2,https://github.com/beobest2,https://api.github.com/users/beobest2/followers,https://api.github.com/users/beobest2/following{/other_user},https://api.github.com/users/beobest2/gists{/gist_id},https://api.github.com/users/beobest2/starred{/owner}{/repo},https://api.github.com/users/beobest2/subscriptions,https://api.github.com/users/beobest2/orgs,https://api.github.com/users/beobest2/repos,https://api.github.com/users/beobest2/events{/privacy},https://api.github.com/users/beobest2/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33744,https://github.com/apache/spark/pull/33744,https://github.com/apache/spark/pull/33744.diff,https://github.com/apache/spark/pull/33744.patch,,https://api.github.com/repos/apache/spark/issues/33744/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
218,https://api.github.com/repos/apache/spark/issues/33723,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33723/labels{/name},https://api.github.com/repos/apache/spark/issues/33723/comments,https://api.github.com/repos/apache/spark/issues/33723/events,https://github.com/apache/spark/pull/33723,968682497,MDExOlB1bGxSZXF1ZXN0NzEwNzgyNjMy,33723,[SPARK-36496][SQL] Remove literals from grouping expressions when using the DataFrame withColumn API,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-08-12T12:00:32Z,2022-01-10T11:13:11Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Move the `RemoveLiteralFromGroupExpressions` and `RemoveRepetitionFromGroupExpressions` rules from a separate batch to the `operatorOptimizationBatch`.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
The `RemoveLiteralFromGroupExpressions` does not work in some cases if it is in a separate batch.
The added UT would fail with:
```
[info] - SPARK-36496: Remove literals from grouping expressions *** FAILED *** (2 seconds, 955 milliseconds)
[info]   == FAIL: Plans do not match ===
[info]   !Aggregate [*id#0L, null], [*id#0L, null AS a#0, count(1) AS count#0L]   Aggregate [*id#0L], [*id#0L, null AS a#0, count(1) AS count#0L]
[info]    +- Range (0, 100, step=1, splits=Some(2))                               +- Range (0, 100, step=1, splits=Some(2)) (PlanTest.scala:174)
```

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UT",https://api.github.com/repos/apache/spark/issues/33723/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33723,https://github.com/apache/spark/pull/33723,https://github.com/apache/spark/pull/33723.diff,https://github.com/apache/spark/pull/33723.patch,,https://api.github.com/repos/apache/spark/issues/33723/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
219,https://api.github.com/repos/apache/spark/issues/33675,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33675/labels{/name},https://api.github.com/repos/apache/spark/issues/33675/comments,https://api.github.com/repos/apache/spark/issues/33675/events,https://github.com/apache/spark/pull/33675,963293573,MDExOlB1bGxSZXF1ZXN0NzA1OTM4MDYx,33675,[SPARK-27997][K8S] Add support for kubernetes OAuth Token refresh,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,31,2021-08-07T19:52:11Z,2021-12-14T16:17:11Z,,NONE,,False,"### What changes were proposed in this pull request?

This change allows a spark user to provide a class which implements fabric's OAuthTokenProvider to refresh tokens throughout the life of the spark app.

```
spark.kubernetes.authenticate.submission.oauthTokenProvider=<token>
spark.kubernetes.authenticate.driver.oauthTokenProvider=<token>
spark.kubernetes.authenticate.oauthTokenProvider=<token>
```

https://javadoc.io/doc/io.fabric8/kubernetes-client/5.6.0/io/fabric8/kubernetes/client/OAuthTokenProvider.html


### Why are the changes needed?

Currently, while running spark on kubernetes, one should specify oauth tokens via config before starting an application.
```
spark.kubernetes.authenticate.submission.oauthToken=<token>
spark.kubernetes.authenticate.driver.oauthToken=<token>
spark.kubernetes.authenticate.oauthToken=<token>
```

The token has an expiration time (usually an hour, for GKE) and there is no way to update the token in the runtime. The spark app starts to throw exceptions.
```
io.fabric8.kubernetes.client.KubernetesClientException: Unauthorized
	at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:202)
	at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:571)
	at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:198)
	at okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)
	at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
```


### Does this PR introduce _any_ user-facing change?
Yes, a configuration option `spark.kubernetes.client.oauth.token.provider.class` is added. 


### How was this patch tested?
A class which implements OAuthTokenProvider interface[0] was added into the classpath on driver node with no other spark options for tokens specified 
It was also tested with expired tokens specified, and the token was updated via the user-provided class.
```
--conf spark.kubernetes.authenticate.submission.oauthToken=<expired>
--conf spark.kubernetes.authenticate.driver.oauthToken=<expired> 
--conf spark.kubernetes.authenticate.oauthToken=<expired>
```
There is no need to use any other token-related configuration options if this class is provided.

An example of the user-provided class for GKE
[0] https://gist.github.com/haodemon/5490fefdb258275c1f805d584319090b

```scala
import io.fabric8.kubernetes.client.OAuthTokenProvider

class OAuthGoogleTokenProvider extends OAuthTokenProvider {
  private val binary = ""gcloud""
  private val args = ""config config-helper --format=json""

  override def getToken: String = {
    val response = (binary + "" "" + args).!!
    val token = new ObjectMapper().readTree(response)
      .get(""credential"")
      .get(""access_token"")
    token.getTextValue
  }
}
```
",https://api.github.com/repos/apache/spark/issues/33675/timeline,,spark,apache,haodemon,3372489,MDQ6VXNlcjMzNzI0ODk=,https://avatars.githubusercontent.com/u/3372489?v=4,,https://api.github.com/users/haodemon,https://github.com/haodemon,https://api.github.com/users/haodemon/followers,https://api.github.com/users/haodemon/following{/other_user},https://api.github.com/users/haodemon/gists{/gist_id},https://api.github.com/users/haodemon/starred{/owner}{/repo},https://api.github.com/users/haodemon/subscriptions,https://api.github.com/users/haodemon/orgs,https://api.github.com/users/haodemon/repos,https://api.github.com/users/haodemon/events{/privacy},https://api.github.com/users/haodemon/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33675,https://github.com/apache/spark/pull/33675,https://github.com/apache/spark/pull/33675.diff,https://github.com/apache/spark/pull/33675.patch,,https://api.github.com/repos/apache/spark/issues/33675/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
220,https://api.github.com/repos/apache/spark/issues/33674,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33674/labels{/name},https://api.github.com/repos/apache/spark/issues/33674/comments,https://api.github.com/repos/apache/spark/issues/33674/events,https://github.com/apache/spark/pull/33674,963224719,MDExOlB1bGxSZXF1ZXN0NzA1ODg3MzMz,33674,[Spark-36328][CORE][SQL] Reuse the FileSystem delegation token while querying partitioned hive table.,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-08-07T12:51:00Z,2021-12-09T10:39:47Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

Add the credentials from previous JobConf into the new JobConf to reuse the FileSystem Delegation Token.

### Why are the changes needed?

Spark Job creates a new JobConf (which will have a new Credentials) for every hive table partition, the token is not reused and gets fetched for every partition. This is slowing down the query as each delegation token has to go through KDC and SSL handshake on Secure Clusters.

### Does this PR introduce _any_ user-facing change?

Yes, while user querying partitioned hive table.

### How was this patch tested?

new test added.
",https://api.github.com/repos/apache/spark/issues/33674/timeline,,spark,apache,Shockang,28219857,MDQ6VXNlcjI4MjE5ODU3,https://avatars.githubusercontent.com/u/28219857?v=4,,https://api.github.com/users/Shockang,https://github.com/Shockang,https://api.github.com/users/Shockang/followers,https://api.github.com/users/Shockang/following{/other_user},https://api.github.com/users/Shockang/gists{/gist_id},https://api.github.com/users/Shockang/starred{/owner}{/repo},https://api.github.com/users/Shockang/subscriptions,https://api.github.com/users/Shockang/orgs,https://api.github.com/users/Shockang/repos,https://api.github.com/users/Shockang/events{/privacy},https://api.github.com/users/Shockang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33674,https://github.com/apache/spark/pull/33674,https://github.com/apache/spark/pull/33674.diff,https://github.com/apache/spark/pull/33674.patch,,https://api.github.com/repos/apache/spark/issues/33674/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
221,https://api.github.com/repos/apache/spark/issues/33625,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33625/labels{/name},https://api.github.com/repos/apache/spark/issues/33625/comments,https://api.github.com/repos/apache/spark/issues/33625/events,https://github.com/apache/spark/pull/33625,959587708,MDExOlB1bGxSZXF1ZXN0NzAyNjg4MDkz,33625,[WIP][SPARK-36397][PYTHON] Implement DataFrame.mode,"[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-08-03T22:45:25Z,2021-12-13T17:30:52Z,,CONTRIBUTOR,,True,"### What changes were proposed in this pull request?
Implement DataFrame.mode (along index axis).


### Why are the changes needed?
Get the mode(s) of each element along the selected axis is a common functionality, which is supported in pandas. We should support that.

### Does this PR introduce _any_ user-facing change?
Yes. `DataFrame.mode` can be used now.

```py
>>> psdf = ps.DataFrame(
...     [(""bird"", 2, 2), (""mammal"", 4, np.nan), (""arthropod"", 8, 0), (""bird"", 2, np.nan)],
...     index=(""falcon"", ""horse"", ""spider"", ""ostrich""),
...     columns=(""species"", ""legs"", ""wings""),
... )
>>> psdf
           species  legs  wings                                                 
falcon        bird     2    2.0
horse       mammal     4    NaN
spider   arthropod     8    0.0
ostrich       bird     2    NaN

>>> psdf.mode()
  species  legs  wings
0    bird   2.0    0.0
1    None   NaN    2.0

>>> psdf.mode(dropna=False)
  species  legs  wings
0    bird     2    NaN

>>> psdf.mode(numeric_only=True)
   legs  wings
0   2.0    0.0
1   NaN    2.0
```

### How was this patch tested?
Unit tests.
",https://api.github.com/repos/apache/spark/issues/33625/timeline,,spark,apache,xinrong-databricks,47337188,MDQ6VXNlcjQ3MzM3MTg4,https://avatars.githubusercontent.com/u/47337188?v=4,,https://api.github.com/users/xinrong-databricks,https://github.com/xinrong-databricks,https://api.github.com/users/xinrong-databricks/followers,https://api.github.com/users/xinrong-databricks/following{/other_user},https://api.github.com/users/xinrong-databricks/gists{/gist_id},https://api.github.com/users/xinrong-databricks/starred{/owner}{/repo},https://api.github.com/users/xinrong-databricks/subscriptions,https://api.github.com/users/xinrong-databricks/orgs,https://api.github.com/users/xinrong-databricks/repos,https://api.github.com/users/xinrong-databricks/events{/privacy},https://api.github.com/users/xinrong-databricks/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33625,https://github.com/apache/spark/pull/33625,https://github.com/apache/spark/pull/33625.diff,https://github.com/apache/spark/pull/33625.patch,,https://api.github.com/repos/apache/spark/issues/33625/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
222,https://api.github.com/repos/apache/spark/issues/33572,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33572/labels{/name},https://api.github.com/repos/apache/spark/issues/33572/comments,https://api.github.com/repos/apache/spark/issues/33572/events,https://github.com/apache/spark/pull/33572,955563966,MDExOlB1bGxSZXF1ZXN0Njk5Mjk2MTk3,33572,[SPARK-36180][SQL] Store TIMESTAMP_NTZ into hive catalog as TIMESTAMP,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,9,2021-07-29T07:52:57Z,2021-11-18T03:15:00Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This PR fix a issue that HMS can not recognize timestamp_ntz by mapping timestamp_ntz to `timestamp` of hive

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

The hive 2.3.9 does not have 2 timestamp or a type named timestamp_ntz.
FYI, In hive 3.0, the will be a timestamp with local timezone added.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

no, timestamp_ntz is new and not public yet
### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

new test",https://api.github.com/repos/apache/spark/issues/33572/timeline,,spark,apache,yaooqinn,8326978,MDQ6VXNlcjgzMjY5Nzg=,https://avatars.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33572,https://github.com/apache/spark/pull/33572,https://github.com/apache/spark/pull/33572.diff,https://github.com/apache/spark/pull/33572.patch,,https://api.github.com/repos/apache/spark/issues/33572/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
223,https://api.github.com/repos/apache/spark/issues/33559,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33559/labels{/name},https://api.github.com/repos/apache/spark/issues/33559/comments,https://api.github.com/repos/apache/spark/issues/33559/events,https://github.com/apache/spark/pull/33559,954819646,MDExOlB1bGxSZXF1ZXN0Njk4NjU2MjM2,33559,[SPARK-34265][PYTHON][SQL] Instrument Pandas UDFs using SQL metrics ,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-07-28T12:47:57Z,2022-01-19T08:19:47Z,,CONTRIBUTOR,,False,"### What changes are proposed in this pull request?

This proposes to add SQLMetrics instrumentation for Python UDF execution, for Pandas UDF and related operations such as MapInPandas and MapInArrow.
The proposed metrics are:

- data sent to Python workers
- data returned from Python workers
- number of output rows
  
### Why are the changes needed?
This aims at improving monitoring and performance troubleshooting of Python UDFs.
In particular it is intended as an aid to answer performance-related questions such as:
why is the UDF slow?, how much work has been done so far?, etc.

### Does this PR introduce _any_ user-facing change?
SQL metrics are made available in the WEB UI.  
See the following examples:  

![image1](https://issues.apache.org/jira/secure/attachment/13038693/PandasUDF_ArrowEvalPython_Metrics.png)
  
### How was this patch tested?

Manually tested + a Python unit test has been added.

Example code used for testing:

```
from pyspark.sql.functions import col, pandas_udf
import time

@pandas_udf(""long"")
def test_pandas(col1):
  time.sleep(0.02)
  return col1 * col1

spark.udf.register(""test_pandas"", test_pandas)
spark.sql(""select rand(42)*rand(51)*rand(12) col1 from range(10000000)"").createOrReplaceTempView(""t1"")
spark.sql(""select max(test_pandas(col1)) from t1"").collect()
```

This is used to test with more data pushed to the Python workers

```
from pyspark.sql.functions import col, pandas_udf
import time

@pandas_udf(""long"")
def test_pandas(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12,col13,col14,col15,col16,col17):
  time.sleep(0.02)
  return col1

spark.udf.register(""test_pandas"", test_pandas)
spark.sql(""select rand(42)*rand(51)*rand(12) col1 from range(10000000)"").createOrReplaceTempView(""t1"")
spark.sql(""select max(test_pandas(col1,col1+1,col1+2,col1+3,col1+4,col1+5,col1+6,col1+7,col1+8,col1+9,col1+10,col1+11,col1+12,col1+13,col1+14,col1+15,col1+16)) from t1"").collect()
```

This (from the SPark doc) has been used to test with MapInPandas, where the number of output rows is different from the number of input rows

```
import pandas as pd

from pyspark.sql.functions import pandas_udf, PandasUDFType

df = spark.createDataFrame([(1, 21), (2, 30)], (""id"", ""age""))

def filter_func(iterator):
    for pdf in iterator:
        yield pdf[pdf.id == 1]

df.mapInPandas(filter_func, schema=df.schema).show()
```",https://api.github.com/repos/apache/spark/issues/33559/timeline,,spark,apache,LucaCanali,5243162,MDQ6VXNlcjUyNDMxNjI=,https://avatars.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33559,https://github.com/apache/spark/pull/33559,https://github.com/apache/spark/pull/33559.diff,https://github.com/apache/spark/pull/33559.patch,,https://api.github.com/repos/apache/spark/issues/33559/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
224,https://api.github.com/repos/apache/spark/issues/33522,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33522/labels{/name},https://api.github.com/repos/apache/spark/issues/33522/comments,https://api.github.com/repos/apache/spark/issues/33522/events,https://github.com/apache/spark/pull/33522,953034263,MDExOlB1bGxSZXF1ZXN0Njk3MTM5NDU4,33522,[SPARK-36290][SQL] Pull out join condition,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,51,2021-07-26T15:37:56Z,2021-11-06T16:48:18Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Similar to [`PullOutGroupingExpressions`](https://github.com/wangyum/spark/blob/7fd3f8f9ec55b364525407213ba1c631705686c5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutGroupingExpressions.scala#L48). This pr add a new rule(`PullOutJoinCondition`) to pull out join condition. Otherwise the expression in join condition may be evaluated three times(`ShuffleExchangeExec`, `SortExec` and the join itself). For example:
```sql
CREATE TABLE t1 using parquet AS select id as a, id as b from range(100000000L);
CREATE TABLE t2 using parquet AS select id as a, id as b from range(200000000L);
SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc');
```
Before this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#6L, b#7L]
   +- SortMergeJoin [translate(cast(a#6L as string), 123, abc)], [translate(cast(a#8L as string), 123, abc)], Inner
      :- Sort [translate(cast(a#6L as string), 123, abc) ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(translate(cast(a#6L as string), 123, abc), 5), ENSURE_REQUIREMENTS, [id=#89]
      :     +- Filter isnotnull(a#6L)
      :        +- FileScan parquet default.t1[a#6L,b#7L]
      +- Sort [translate(cast(a#8L as string), 123, abc) ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(translate(cast(a#8L as string), 123, abc), 5), ENSURE_REQUIREMENTS, [id=#90]
            +- Filter isnotnull(a#8L)
               +- FileScan parquet default.t2[a#8L]
```
After this pr:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [a#6L, b#7L]
   +- SortMergeJoin [translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12], [translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13], Inner
      :- Sort [translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12 ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12, 5), ENSURE_REQUIREMENTS, [id=#53]
      :     +- Project [a#6L, b#7L, translate(cast(a#6L as string), 123, abc) AS translate(CAST(spark_catalog.default.t1.a AS STRING), '123', 'abc')#12]
      :        +- Filter isnotnull(translate(cast(a#6L as string), 123, abc))
      :           +- FileScan parquet default.t1[a#6L,b#7L]
      +- Sort [translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13, 5), ENSURE_REQUIREMENTS, [id=#54]
            +- Project [translate(cast(a#8L as string), 123, abc) AS translate(CAST(spark_catalog.default.t2.a AS STRING), '123', 'abc')#13]
               +- Filter isnotnull(translate(cast(a#8L as string), 123, abc))
                  +- FileScan parquet default.t2[a#8L]
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.


### How was this patch tested?

Unit test and benchmark test:
```scala
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 15
spark.sql(s""CREATE TABLE t1 using parquet AS select id as a, id as b from range(${numRows}L)"")
spark.sql(s""CREATE TABLE t2 using parquet AS select id as a, id as b from range(${numRows}L)"")
val benchmark = new Benchmark(""Benchmark pull out join condition"", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s""Pull out join condition ${if (pullOutEnabled) ""(Enabled)"" else ""(Disabled)""}""
  benchmark.addCase(name) { _ =>
    withSQLConf(""spark.sql.pullOutJoinCondition"" -> s""$pullOutEnabled"") {
      spark.sql(""SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc')"").write.format(""noop"").mode(""Overwrite"").save()
    }
  }
}
benchmark.run()
```
Benchmark result:
```
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out join condition:        Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out join condition (Disabled)                30197          34046         690          0.5        1919.9       1.0X
Pull out join condition (Enabled)                 19631          20484         535          0.8        1248.1       1.5X
```
",https://api.github.com/repos/apache/spark/issues/33522/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33522,https://github.com/apache/spark/pull/33522,https://github.com/apache/spark/pull/33522.diff,https://github.com/apache/spark/pull/33522.patch,,https://api.github.com/repos/apache/spark/issues/33522/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
225,https://api.github.com/repos/apache/spark/issues/33446,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33446/labels{/name},https://api.github.com/repos/apache/spark/issues/33446/comments,https://api.github.com/repos/apache/spark/issues/33446/events,https://github.com/apache/spark/pull/33446,948883378,MDExOlB1bGxSZXF1ZXN0NjkzNjU5MzA1,33446,[SPARK-36215][SHUFFLE] Add logging for slow fetches to diagnose external shuffle service issues,"[{'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,16,2021-07-20T17:33:49Z,2022-01-09T05:49:55Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
Add logging to `ShuffleBlockFetcherIterator` to log ""slow"" fetches, where slow is defined by two confs: `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Currently we can see from the metrics that a task or stage has slow fetches, and the logs indicate *all* of the shuffle servers those tasks were fetching from, but often this is a big set (dozens or even hundreds) and narrowing down which one caused issues can be very difficult. This change makes it easier to understand which fetch is ""slow"".
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Adds two configs `spark.reducer.shuffleFetchSlowLogThreshold.time` and `spark.reducer.shuffleFetchSlowLogThreshold.bytesPerSec`


### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Added unit test",https://api.github.com/repos/apache/spark/issues/33446/timeline,,spark,apache,shardulm94,6961317,MDQ6VXNlcjY5NjEzMTc=,https://avatars.githubusercontent.com/u/6961317?v=4,,https://api.github.com/users/shardulm94,https://github.com/shardulm94,https://api.github.com/users/shardulm94/followers,https://api.github.com/users/shardulm94/following{/other_user},https://api.github.com/users/shardulm94/gists{/gist_id},https://api.github.com/users/shardulm94/starred{/owner}{/repo},https://api.github.com/users/shardulm94/subscriptions,https://api.github.com/users/shardulm94/orgs,https://api.github.com/users/shardulm94/repos,https://api.github.com/users/shardulm94/events{/privacy},https://api.github.com/users/shardulm94/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33446,https://github.com/apache/spark/pull/33446,https://github.com/apache/spark/pull/33446.diff,https://github.com/apache/spark/pull/33446.patch,,https://api.github.com/repos/apache/spark/issues/33446/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
226,https://api.github.com/repos/apache/spark/issues/33404,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33404/labels{/name},https://api.github.com/repos/apache/spark/issues/33404/comments,https://api.github.com/repos/apache/spark/issues/33404/events,https://github.com/apache/spark/pull/33404,946850336,MDExOlB1bGxSZXF1ZXN0NjkxOTUyMTk4,33404,[SPARK-36194][SQL] Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,29,2021-07-17T15:59:39Z,2021-11-02T03:40:10Z,,MEMBER,,False,"### What changes were proposed in this pull request?

Remove the aggregation from left semi/anti join if the same aggregation has already been done on left side. For example:
```sql
set spark.sql.autoBroadcastJoinThreshold=-1; -- avoid PushDownLeftSemiAntiJoin
create table t1 using parquet as select id a, id as b from range(10);
create table t2 using parquet as select id as a, id as b from range(8);
select t11.a, t11.b from (select distinct a, b from t1) t11 left semi join t2 on (t11.a = t2.a) group by t11.a, t11.b;
```

Before this PR:
```
== Optimized Logical Plan ==
Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
+- Join LeftSemi, (a#6L = a#8L), Statistics(sizeInBytes=1492.0 B)
   :- Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
   :  +- Filter isnotnull(a#6L), Statistics(sizeInBytes=1492.0 B)
   :     +- Relation default.t1[a#6L,b#7L] parquet, Statistics(sizeInBytes=1492.0 B)
   +- Project [a#8L], Statistics(sizeInBytes=984.0 B)
      +- Filter isnotnull(a#8L), Statistics(sizeInBytes=1476.0 B)
         +- Relation default.t2[a#8L,b#9L] parquet, Statistics(sizeInBytes=1476.0 B)
```

After this PR:
```
== Optimized Logical Plan ==
Join LeftSemi, (a#6L = a#8L), Statistics(sizeInBytes=1492.0 B)
:- Aggregate [a#6L, b#7L], [a#6L, b#7L], Statistics(sizeInBytes=1492.0 B)
:  +- Filter isnotnull(a#6L), Statistics(sizeInBytes=1492.0 B)
:     +- Relation default.t1[a#6L,b#7L] parquet, Statistics(sizeInBytes=1492.0 B)
+- Project [a#8L], Statistics(sizeInBytes=984.0 B)
   +- Filter isnotnull(a#8L), Statistics(sizeInBytes=1476.0 B)
      +- Relation default.t2[a#8L,b#9L] parquet, Statistics(sizeInBytes=1476.0 B)
```

This rule can be disabled by:
```sql
set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAggregatesInLeftSemiAntiJoin;
```

### Why are the changes needed?

Improve query performance.

### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and TPC-DS benchmark test.

SQL | Before this PR(Seconds) | After this PR(Seconds)
-- | -- | --
q14a | 174  | 165
q38 | 26 | 23
q87 | 30 | 26
",https://api.github.com/repos/apache/spark/issues/33404/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33404,https://github.com/apache/spark/pull/33404,https://github.com/apache/spark/pull/33404.diff,https://github.com/apache/spark/pull/33404.patch,,https://api.github.com/repos/apache/spark/issues/33404/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
227,https://api.github.com/repos/apache/spark/issues/33314,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33314/labels{/name},https://api.github.com/repos/apache/spark/issues/33314/comments,https://api.github.com/repos/apache/spark/issues/33314/events,https://github.com/apache/spark/pull/33314,942685825,MDExOlB1bGxSZXF1ZXN0Njg4Mzg1NTc1,33314,[SPARK-36118][SQL] Add bitmap functions for Spark SQL,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982110661, 'node_id': 'MDU6TGFiZWwxOTgyMTEwNjYx', 'url': 'https://api.github.com/repos/apache/spark/labels/INFRA', 'name': 'INFRA', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,12,2021-07-13T03:56:43Z,2021-10-27T03:18:20Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error type or message, please read the guideline first in
     'core/src/main/resources/error/README.md'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
add functions of bitmap building and computing cardinality for Spark SQL, If this is ok, I will update function.scala and FunctionRegistry.scala.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Bitmaps are used more and more widely, and many frameworks have native support, such as Clickhouse

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
CI, it performs well on billions of rows based on our real demand",https://api.github.com/repos/apache/spark/issues/33314/timeline,,spark,apache,ReachInfi,87301083,MDQ6VXNlcjg3MzAxMDgz,https://avatars.githubusercontent.com/u/87301083?v=4,,https://api.github.com/users/ReachInfi,https://github.com/ReachInfi,https://api.github.com/users/ReachInfi/followers,https://api.github.com/users/ReachInfi/following{/other_user},https://api.github.com/users/ReachInfi/gists{/gist_id},https://api.github.com/users/ReachInfi/starred{/owner}{/repo},https://api.github.com/users/ReachInfi/subscriptions,https://api.github.com/users/ReachInfi/orgs,https://api.github.com/users/ReachInfi/repos,https://api.github.com/users/ReachInfi/events{/privacy},https://api.github.com/users/ReachInfi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33314,https://github.com/apache/spark/pull/33314,https://github.com/apache/spark/pull/33314.diff,https://github.com/apache/spark/pull/33314.patch,,https://api.github.com/repos/apache/spark/issues/33314/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
228,https://api.github.com/repos/apache/spark/issues/33257,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33257/labels{/name},https://api.github.com/repos/apache/spark/issues/33257/comments,https://api.github.com/repos/apache/spark/issues/33257/events,https://github.com/apache/spark/pull/33257,939520094,MDExOlB1bGxSZXF1ZXN0Njg1NzI3NzQ3,33257,[SPARK-36039][K8S] Fix executor pod hadoop conf mount,"[{'id': 1406605057, 'node_id': 'MDU6TGFiZWwxNDA2NjA1MDU3', 'url': 'https://api.github.com/repos/apache/spark/labels/KUBERNETES', 'name': 'KUBERNETES', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-07-08T06:33:34Z,2021-12-24T07:43:16Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
Fix executor pod hadoop conf mount.
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->


### Why are the changes needed?
Arg --conf spark.kubernetes.hadoop.configMapName for executor pod not working.
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->


### Does this PR introduce _any_ user-facing change?
No.
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->


### How was this patch tested?
UT.
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->",https://api.github.com/repos/apache/spark/issues/33257/timeline,,spark,apache,cutiechi,27606234,MDQ6VXNlcjI3NjA2MjM0,https://avatars.githubusercontent.com/u/27606234?v=4,,https://api.github.com/users/cutiechi,https://github.com/cutiechi,https://api.github.com/users/cutiechi/followers,https://api.github.com/users/cutiechi/following{/other_user},https://api.github.com/users/cutiechi/gists{/gist_id},https://api.github.com/users/cutiechi/starred{/owner}{/repo},https://api.github.com/users/cutiechi/subscriptions,https://api.github.com/users/cutiechi/orgs,https://api.github.com/users/cutiechi/repos,https://api.github.com/users/cutiechi/events{/privacy},https://api.github.com/users/cutiechi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33257,https://github.com/apache/spark/pull/33257,https://github.com/apache/spark/pull/33257.diff,https://github.com/apache/spark/pull/33257.patch,,https://api.github.com/repos/apache/spark/issues/33257/reactions,1,1,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
229,https://api.github.com/repos/apache/spark/issues/33008,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/33008/labels{/name},https://api.github.com/repos/apache/spark/issues/33008/comments,https://api.github.com/repos/apache/spark/issues/33008/events,https://github.com/apache/spark/pull/33008,926741465,MDExOlB1bGxSZXF1ZXN0Njc1MDA0NTAx,33008,[WIP][SPARK-35801][SQL] Support DELETE operations that require rewriting data,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,23,2021-06-22T01:40:22Z,2021-11-13T04:15:08Z,,CONTRIBUTOR,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This WIP PR shows how we can use the proposed API in SPARK-35801 (per [design doc](https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60)) to support DELETE statements that require rewriting data.

**Note**: This PR must be split into a number of smaller PRs if we decide to adopt this approach. All changes are grouped here only to simplify the review process and support the design doc.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

These changes are required so that Spark can provide support for DELETE, UPDATE, MERGE statements.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

Yes, this PR introduces a set of new APIs for Data Source V2.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

This PR comes with a trivial test. More tests to come.
",https://api.github.com/repos/apache/spark/issues/33008/timeline,,spark,apache,aokolnychyi,6235869,MDQ6VXNlcjYyMzU4Njk=,https://avatars.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/33008,https://github.com/apache/spark/pull/33008,https://github.com/apache/spark/pull/33008.diff,https://github.com/apache/spark/pull/33008.patch,,https://api.github.com/repos/apache/spark/issues/33008/reactions,11,0,0,0,11,0,0,0,0,,,,,,,,,,,,,,,,,,
230,https://api.github.com/repos/apache/spark/issues/32987,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32987/labels{/name},https://api.github.com/repos/apache/spark/issues/32987/comments,https://api.github.com/repos/apache/spark/issues/32987/events,https://github.com/apache/spark/pull/32987,925601145,MDExOlB1bGxSZXF1ZXN0Njc0MDM2MzE5,32987,[SPARK-35564][SQL] Support subexpression elimination for conditionally evaluated expressions,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,37,2021-06-20T13:35:30Z,2022-01-28T20:56:33Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
I am proposing to add support for conditionally evaluated expressions during subexpression elimination. Currently, only expressions that will definitely be always at least twice are candidates for subexpression elimination. This PR updates that logic so that expressions that are always evaluated at least once and conditionally evaluated at least once are also candidates for subexpression elimination. This helps optimize a common case during data normalization and cleaning and want to null out values that don't match a certain pattern, where you have something like:

```
transformed = F.regexp_replace(F.lower(F.trim('my_column')))
df.withColumn('normalized_value', F.when(F.length(transformed) > 0, transformed))
```
or
```
df.withColumn('normalized_value', F.when(transformed.rlike(<some regex>), transformed))
```

In these cases, `transformed` will always be fully calculated twice, because it might only be needed once. I am proposing creating a subexpression for `transformed` in this case.

In practice I've seen a decrease in runtime and codegen size of 10-30% in our production pipelines that heavily make use of this type of logic.

The only potential downside is creating extra subexpressions, and therefore function calls, more than necessary. This should only be an issue for certain edge cases where your conditional overwhelming evaluates to false. And then the only overhead is running your conditional logic potentially in a separate function rather than inlined in the codegen. I added a config to control this behavior if that is actually a real concern to anyone, but I'd be happy to just remove the config.

I also updated some of the existing logic for common expressions in coalesce and when that are actually better handled by the new logic, since you are only guaranteed to have the first value of a Coalesce evaluated, as well as the first conditional of a CaseWhen expression.


### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
To increase the performance of conditional expressions.


### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No, just performance improvements.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New and updated UT.
",https://api.github.com/repos/apache/spark/issues/32987/timeline,,spark,apache,Kimahriman,3536454,MDQ6VXNlcjM1MzY0NTQ=,https://avatars.githubusercontent.com/u/3536454?v=4,,https://api.github.com/users/Kimahriman,https://github.com/Kimahriman,https://api.github.com/users/Kimahriman/followers,https://api.github.com/users/Kimahriman/following{/other_user},https://api.github.com/users/Kimahriman/gists{/gist_id},https://api.github.com/users/Kimahriman/starred{/owner}{/repo},https://api.github.com/users/Kimahriman/subscriptions,https://api.github.com/users/Kimahriman/orgs,https://api.github.com/users/Kimahriman/repos,https://api.github.com/users/Kimahriman/events{/privacy},https://api.github.com/users/Kimahriman/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32987,https://github.com/apache/spark/pull/32987,https://github.com/apache/spark/pull/32987.diff,https://github.com/apache/spark/pull/32987.patch,,https://api.github.com/repos/apache/spark/issues/32987/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
231,https://api.github.com/repos/apache/spark/issues/32769,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32769/labels{/name},https://api.github.com/repos/apache/spark/issues/32769/comments,https://api.github.com/repos/apache/spark/issues/32769/events,https://github.com/apache/spark/pull/32769,910450034,MDExOlB1bGxSZXF1ZXN0NjYwODY4NjAz,32769,[SPARK-35630][SQL] ExpandExec should not introduce unnecessary exchanges,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,18,2021-06-03T12:36:03Z,2021-12-08T12:54:09Z,,CONTRIBUTOR,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
Improved `ExpandExec` so it would retain its child's outputPartitioning, when possible. 

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Currently `ExpandExecs` `outputPartitioning` is always `UnknownPartitioning(0)`. In some cases we do actually know the correct output partitioning. In those cases we could reduce the number of exchanges by using the correct partitioning.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
New UT
",https://api.github.com/repos/apache/spark/issues/32769/timeline,,spark,apache,tanelk,3342974,MDQ6VXNlcjMzNDI5NzQ=,https://avatars.githubusercontent.com/u/3342974?v=4,,https://api.github.com/users/tanelk,https://github.com/tanelk,https://api.github.com/users/tanelk/followers,https://api.github.com/users/tanelk/following{/other_user},https://api.github.com/users/tanelk/gists{/gist_id},https://api.github.com/users/tanelk/starred{/owner}{/repo},https://api.github.com/users/tanelk/subscriptions,https://api.github.com/users/tanelk/orgs,https://api.github.com/users/tanelk/repos,https://api.github.com/users/tanelk/events{/privacy},https://api.github.com/users/tanelk/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32769,https://github.com/apache/spark/pull/32769,https://github.com/apache/spark/pull/32769.diff,https://github.com/apache/spark/pull/32769.patch,,https://api.github.com/repos/apache/spark/issues/32769/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
232,https://api.github.com/repos/apache/spark/issues/32655,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32655/labels{/name},https://api.github.com/repos/apache/spark/issues/32655/comments,https://api.github.com/repos/apache/spark/issues/32655/events,https://github.com/apache/spark/pull/32655,900075089,MDExOlB1bGxSZXF1ZXN0NjUxNjg1MjE0,32655,[SPARK-33743]change TimestampType match to datetime2 instead of datetime for MsSQLServerDialect,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,4,2021-05-24T22:46:36Z,2021-11-10T00:37:20Z,,NONE,,False,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
  8. If you want to add or modify an error message, please read the guideline first:
     https://spark.apache.org/error-message-guidelines.html
-->

SPARK-33743 is to change datetime datatype mapping in JDBC mssqldialect.
### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
case TimestampType => Some(JdbcType(""DATETIME2"", java.sql.Types.TIMESTAMP))

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Spark datetime type is timestamp type. This supports a microsecond resolution.
Sql supports 2 date time types:

datetime can support only milli seconds resolution (0 to 999).
datetime2 is extension of datetime , is compatible with datetime and supports 0 to 9999999 sub second resolution.
datetime2 (Transact-SQL) - SQL Server | Microsoft Docs
datetime (Transact-SQL) - SQL Server | Microsoft Docs

Currently MsSQLServerDialect maps timestamp type to datetime. Datetime only allows 3 digits of microseconds. This implies results in errors when writing timestamp with more than 3 digits of microseconds to sql server table. We want to map timestamp to datetime2, which is compatible with datetime but allows 7 digits of microseconds.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
No.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
Unit tests were updated and passed in JDBCSuit.scala.
E2E test done with SQL Server.",https://api.github.com/repos/apache/spark/issues/32655/timeline,,spark,apache,luxu1-ms,68044595,MDQ6VXNlcjY4MDQ0NTk1,https://avatars.githubusercontent.com/u/68044595?v=4,,https://api.github.com/users/luxu1-ms,https://github.com/luxu1-ms,https://api.github.com/users/luxu1-ms/followers,https://api.github.com/users/luxu1-ms/following{/other_user},https://api.github.com/users/luxu1-ms/gists{/gist_id},https://api.github.com/users/luxu1-ms/starred{/owner}{/repo},https://api.github.com/users/luxu1-ms/subscriptions,https://api.github.com/users/luxu1-ms/orgs,https://api.github.com/users/luxu1-ms/repos,https://api.github.com/users/luxu1-ms/events{/privacy},https://api.github.com/users/luxu1-ms/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32655,https://github.com/apache/spark/pull/32655,https://github.com/apache/spark/pull/32655.diff,https://github.com/apache/spark/pull/32655.patch,,https://api.github.com/repos/apache/spark/issues/32655/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
233,https://api.github.com/repos/apache/spark/issues/32562,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32562/labels{/name},https://api.github.com/repos/apache/spark/issues/32562/comments,https://api.github.com/repos/apache/spark/issues/32562/events,https://github.com/apache/spark/pull/32562,892694415,MDExOlB1bGxSZXF1ZXN0NjQ1MzMyNTY1,32562,[WIP][SPARK-35414][SQL] Submit broadcast collect job first to avoid broadcast timeout in AQE,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,3,2021-05-16T15:26:35Z,2021-11-06T06:41:44Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
1. replace executeCollectIterator() by executeCollectIteratorFuture() in SparkPlan.scala to run collect query in async way and return the future of collect result
2. in BroadcastExchangeExec->relationFuture, call executeCollectIteratorFuture() in current thread and get the collectFuture, wait collectFuture in ""broadcast-exchange"" thread 


### Why are the changes needed?
#31269 gives a partial fix to SPARK-33933, which is not a perfect solution. This changes can make sure the broadcast collect job is submitted before shuffle map jobs. #31269 ensure the calling of materialize() of BroadcastQueryStage is before ShuffleQueryStage. In BroadcastQueryStage's materialize(), doPrepare() will call relationFuture, which will submit collect job before return the future.

### Does this PR introduce _any_ user-facing change?
No


### How was this patch tested?
Add UT
",https://api.github.com/repos/apache/spark/issues/32562/timeline,,spark,apache,zhongyu09,3882710,MDQ6VXNlcjM4ODI3MTA=,https://avatars.githubusercontent.com/u/3882710?v=4,,https://api.github.com/users/zhongyu09,https://github.com/zhongyu09,https://api.github.com/users/zhongyu09/followers,https://api.github.com/users/zhongyu09/following{/other_user},https://api.github.com/users/zhongyu09/gists{/gist_id},https://api.github.com/users/zhongyu09/starred{/owner}{/repo},https://api.github.com/users/zhongyu09/subscriptions,https://api.github.com/users/zhongyu09/orgs,https://api.github.com/users/zhongyu09/repos,https://api.github.com/users/zhongyu09/events{/privacy},https://api.github.com/users/zhongyu09/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32562,https://github.com/apache/spark/pull/32562,https://github.com/apache/spark/pull/32562.diff,https://github.com/apache/spark/pull/32562.patch,,https://api.github.com/repos/apache/spark/issues/32562/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
234,https://api.github.com/repos/apache/spark/issues/32477,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32477/labels{/name},https://api.github.com/repos/apache/spark/issues/32477/comments,https://api.github.com/repos/apache/spark/issues/32477/events,https://github.com/apache/spark/pull/32477,880493723,MDExOlB1bGxSZXF1ZXN0NjM0MTQ4NjEz,32477,[SPARK-35348][SQL] Support the utils for escapse the regex for ANSI SQL: SIMILAR TO … ESCAPE syntax,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,35,2021-05-08T08:21:29Z,2021-12-15T07:13:14Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
`ANSI SQL: SIMILAR TO ... ESCAPE` is very useful.
There are some mainstream database support the syntax.
**PostgreSQL**:
https://www.postgresql.org/docs/current/functions-matching.html#FUNCTIONS-SIMILARTO-REGEXP

**Redshift**:
https://docs.aws.amazon.com/redshift/latest/dg/pattern-matching-conditions-similar-to.html

**Sybase**:
http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.sqlanywhere.12.0.0/dbreference/like-regexp-similarto.html

**Firebird**:
http://firebirdsql.org/file/documentation/html/en/refdocs/fblangref25/firebird-25-language-reference.html#fblangref25-commons-predsiimilarto

This util supports the following pattern-matching metacharacters:

Operator | Description
-- | --
% | Matches any sequence of zero or more characters.
_ | Matches any single character.
\| | Denotes alternation (either of two alternatives).
\* | Repeat the previous item zero or more times.
\+ | Repeat the previous item one or more times.
? | Repeat the previous item zero or one time.
{m} | Repeat the previous item exactly m times.
{m,} | Repeat the previous item m or more times.
{m,n} | Repeat the previous item at least m and not more than n times.
() | Parentheses group items into a single logical item.
[...] | A bracket expression specifies a character class, just as in POSIX regular expressions.

**Note**
`SIMILAR TO` is similar to `RLIKE`, but with the following differences:
       1. The `SIMILAR TO` operator returns true only if its pattern matches the entire string,
          unlike `RLIKE` behavior, where the pattern can match any portion of the string.
       2. The regex string allow use _ and % as wildcard characters denoting any single character
          and any string, respectively (these are comparable to . and .* in POSIX regular
          expressions).
       3. The regex string allow use escape character like `LIKE` behavior.
       4. '.', '^' and '$' is not a meta character for `SIMILAR TO`.

### Why are the changes needed?
`ANSI SQL: SIMILAR TO ... ESCAPE` is very useful.


### Does this PR introduce _any_ user-facing change?
Yes, a new feature.


### How was this patch tested?
New tests
",https://api.github.com/repos/apache/spark/issues/32477/timeline,,spark,apache,beliefer,8486025,MDQ6VXNlcjg0ODYwMjU=,https://avatars.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32477,https://github.com/apache/spark/pull/32477,https://github.com/apache/spark/pull/32477.diff,https://github.com/apache/spark/pull/32477.patch,,https://api.github.com/repos/apache/spark/issues/32477/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
235,https://api.github.com/repos/apache/spark/issues/32397,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32397/labels{/name},https://api.github.com/repos/apache/spark/issues/32397/comments,https://api.github.com/repos/apache/spark/issues/32397/events,https://github.com/apache/spark/pull/32397,870986343,MDExOlB1bGxSZXF1ZXN0NjI2MDgyMDI3,32397,"[SPARK-35084][CORE] Spark 3: supporting ""--packages"" in  k8s cluster mode","[{'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,53,2021-04-29T12:56:44Z,2022-01-03T02:54:00Z,,NONE,,False,"### What changes were proposed in this pull request?
Supporting '--packages' in the k8s cluster mode

### Why are the changes needed?
In spark 3, '--packages' in the k8s cluster mode is not supported. I expected that managing dependencies by using packages like spark 2.

Spark 2.4.5

https://github.com/apache/spark/blob/v2.4.5/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala

```scala
     if (!isMesosCluster && !isStandAloneCluster) {
      // Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files
      // too for packages that include Python code
      val resolvedMavenCoordinates = DependencyUtils.resolveMavenDependencies(
        args.packagesExclusions, args.packages, args.repositories, args.ivyRepoPath,
        args.ivySettingsPath)
      
      if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
        if (args.isPython || isInternal(args.primaryResource)) {
          args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
        }
      } 
      
      // install any R packages that may have been passed through --jars or --packages.
      // Spark Packages may contain R source code inside the jar.
      if (args.isR && !StringUtils.isBlank(args.jars)) {
        RPackageUtils.checkAndBuildRPackage(args.jars, printStream, args.verbose)
      }
    } 
 ```

Spark 3.0.2

https://github.com/apache/spark/blob/v3.0.2/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala

```scala
       if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        // In K8s client mode, when in the driver, add resolved jars early as we might need
        // them at the submit time for artifact downloading.
        // For example we might use the dependencies for downloading
        // files from a Hadoop Compatible fs eg. S3. In this case the user might pass:
        // --packages com.amazonaws:aws-java-sdk:1.7.4:org.apache.hadoop:hadoop-aws:2.7.6
        if (isKubernetesClusterModeDriver) {
          val loader = getSubmitClassLoader(sparkConf)
          for (jar <- resolvedMavenCoordinates.split("","")) {
            addJarToClasspath(jar, loader)
          }
        } else if (isKubernetesCluster) {
          // We need this in K8s cluster mode so that we can upload local deps
          // via the k8s application, like in cluster mode driver
          childClasspath ++= resolvedMavenCoordinates.split("","")
        } else {
          args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
          if (args.isPython || isInternal(args.primaryResource)) {
            args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
          }
        }
      }
```

unlike spark2, in spark 3, jars are not added in any place.

### Does this PR introduce _any_ user-facing change?
Unlike spark 2, resolved jars are added not in cluster mode spark submit but in driver.

It's because in spark 3, the feature is added that is uploading jars with prefix ""file://"" to s3.
So, if resolved jars are added in spark submit, every jars from packages are uploading to s3! When I tested it, it is very bad experience to me.

### How was this patch tested?
In my k8s environment, i tested the code.
",https://api.github.com/repos/apache/spark/issues/32397/timeline,,spark,apache,ocworld,13185662,MDQ6VXNlcjEzMTg1NjYy,https://avatars.githubusercontent.com/u/13185662?v=4,,https://api.github.com/users/ocworld,https://github.com/ocworld,https://api.github.com/users/ocworld/followers,https://api.github.com/users/ocworld/following{/other_user},https://api.github.com/users/ocworld/gists{/gist_id},https://api.github.com/users/ocworld/starred{/owner}{/repo},https://api.github.com/users/ocworld/subscriptions,https://api.github.com/users/ocworld/orgs,https://api.github.com/users/ocworld/repos,https://api.github.com/users/ocworld/events{/privacy},https://api.github.com/users/ocworld/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32397,https://github.com/apache/spark/pull/32397,https://github.com/apache/spark/pull/32397.diff,https://github.com/apache/spark/pull/32397.patch,,https://api.github.com/repos/apache/spark/issues/32397/reactions,9,9,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
236,https://api.github.com/repos/apache/spark/issues/32332,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32332/labels{/name},https://api.github.com/repos/apache/spark/issues/32332/comments,https://api.github.com/repos/apache/spark/issues/32332/events,https://github.com/apache/spark/pull/32332,866947011,MDExOlB1bGxSZXF1ZXN0NjIyNzIwNDQ1,32332,[SPARK-35211][PYTHON] verify inferred schema for _create_dataframe,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981527456, 'node_id': 'MDU6TGFiZWwxOTgxNTI3NDU2', 'url': 'https://api.github.com/repos/apache/spark/labels/CORE', 'name': 'CORE', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,40,2021-04-25T08:14:25Z,2021-11-17T03:35:04Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
Do extra schema verification after it is inferred

This PR do not introduce any semantic changes except for the extra schema verification.

This pr fixes SPARK-35211 when schema verification is turned on. If schema verification is turned off, the bug described in SPARK-35211 still exists. I will create another PR to solve the issue.


### Why are the changes needed?
``` python
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""false"")
from pyspark.testing.sqlutils  import ExamplePoint
import pandas as pd
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
df = spark.createDataFrame(pdf)
df.show()
```
The result is not correct because of incorrect type conversion. Here is the incorrect result:
```
+----------+
|     point|
+----------+
|(0.0, 0.0)|
|(0.0, 0.0)|
+----------+
```

With this PR, type check will be performed:
```
(spark) ➜  spark git:(sadhen/SPARK-35211) ✗ bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/04/24 17:42:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1619257343692).
SparkSession available as 'spark'.
>>> spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""false"")
>>> from pyspark.testing.sqlutils  import ExamplePoint
>>> import pandas as pd
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
>>> df = spark.createDataFrame(pdf)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 653, in createDataFrame
    return super(SparkSession, self).createDataFrame(
  File ""/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py"", line 340, in createDataFrame
    return self._create_dataframe(data, schema, samplingRatio, verifySchema)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 699, in _create_dataframe
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 499, in _createFromLocal
    data = list(data)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/session.py"", line 688, in prepare
    verify_func(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1390, in verify_struct
    verifier(v)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1304, in verify_udf
    verifier(dataType.toInternal(obj))
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1354, in verify_array
    element_verifier(i)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1409, in verify
    verify_value(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1403, in verify_default
    verify_acceptable_types(obj)
  File ""/Users/da/github/apache/spark/python/pyspark/sql/types.py"", line 1291, in verify_acceptable_types
    raise TypeError(new_msg(""%s can not accept object %r in type %s""
TypeError: element in array field point: DoubleType can not accept object 1 in type <class 'int'>
```


### Does this PR introduce _any_ user-facing change?
No



### How was this patch tested?
unit test

```
python/run-tests --testnames pyspark.sql.tests.test_dataframe
```
",https://api.github.com/repos/apache/spark/issues/32332/timeline,,spark,apache,darcy-shen,1267865,MDQ6VXNlcjEyNjc4NjU=,https://avatars.githubusercontent.com/u/1267865?v=4,,https://api.github.com/users/darcy-shen,https://github.com/darcy-shen,https://api.github.com/users/darcy-shen/followers,https://api.github.com/users/darcy-shen/following{/other_user},https://api.github.com/users/darcy-shen/gists{/gist_id},https://api.github.com/users/darcy-shen/starred{/owner}{/repo},https://api.github.com/users/darcy-shen/subscriptions,https://api.github.com/users/darcy-shen/orgs,https://api.github.com/users/darcy-shen/repos,https://api.github.com/users/darcy-shen/events{/privacy},https://api.github.com/users/darcy-shen/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32332,https://github.com/apache/spark/pull/32332,https://github.com/apache/spark/pull/32332.diff,https://github.com/apache/spark/pull/32332.patch,,https://api.github.com/repos/apache/spark/issues/32332/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
237,https://api.github.com/repos/apache/spark/issues/32298,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32298/labels{/name},https://api.github.com/repos/apache/spark/issues/32298/comments,https://api.github.com/repos/apache/spark/issues/32298/events,https://github.com/apache/spark/pull/32298,864996333,MDExOlB1bGxSZXF1ZXN0NjIxMTMxODQ3,32298,[SPARK-34079][SQL] Merge non-correlated scalar subqueries,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,94,2021-04-22T14:08:16Z,2021-12-10T14:01:10Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
This PR adds a new optimizer rule `MergeScalarSubqueries` to merge multiple non-correlated `ScalarSubquery`s to compute multiple scalar values once.

E.g. the following query:
```
SELECT
  (SELECT avg(a) FROM t GROUP BY b),
  (SELECT sum(b) FROM t GROUP BY b)
```
is optimized from:
```
Project [scalar-subquery#231 [] AS scalarsubquery()#241, scalar-subquery#232 [] AS scalarsubquery()#242L]
:  :- Aggregate [b#234], [avg(a#233) AS avg(a)#236]
:  :  +- Relation default.t[a#233,b#234] parquet
:  +- Aggregate [b#240], [sum(b#240) AS sum(b)#238L]
:     +- Project [b#240]
:        +- Relation default.t[a#239,b#240] parquet
+- OneRowRelation
```
to:
```
CommonScalarSubqueries [scalar-subquery#250 []]
:  +- Project [named_struct(avg(a), avg(a)#236, sum(b), sum(b)#238L) AS mergedValue#249]
:     +- Aggregate [b#234], [avg(a#233) AS avg(a)#236, sum(b#234) AS sum(b)#238L]
:        +- Project [a#233, b#234]
:           +- Relation default.t[a#233,b#234] parquet
+- Project [scalarsubqueryreference(0, 0, DoubleType, 231) AS scalarsubquery()#241,
            scalarsubqueryreference(0, 1, LongType, 232) AS scalarsubquery()#242L]
   +- OneRowRelation
```
and in the physical plan subquery references are replaced to the same planned subquery instance and are reused:
```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   *(1) Project [Subquery subquery#250, [id=#104].avg(a) AS scalarsubquery()#241, ReusedSubquery Subquery subquery#250, [id=#104].sum(b) AS scalarsubquery()#242L]
   :  :- Subquery subquery#250, [id=#104]
   :  :  +- AdaptiveSparkPlan isFinalPlan=true
         +- == Final Plan ==
            *(2) HashAggregate(keys=[b#234], functions=[avg(a#233), sum(b#234)], output=[mergedValue#249])
            +- CustomShuffleReader coalesced
               +- ShuffleQueryStage 0
                  +- Exchange hashpartitioning(b#234, 5), ENSURE_REQUIREMENTS, [id=#125]
                     +- *(1) HashAggregate(keys=[b#234], functions=[partial_avg(a#233), partial_sum(b#234)], output=[b#234, sum#252, count#253L, sum#254L])
                        +- *(1) ColumnarToRow
                           +- FileScan parquet default.t[a#233,b#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
         +- == Initial Plan ==
            HashAggregate(keys=[b#234], functions=[avg(a#233), sum(b#234)], output=[mergedValue#249])
            +- Exchange hashpartitioning(b#234, 5), ENSURE_REQUIREMENTS, [id=#102]
               +- HashAggregate(keys=[b#234], functions=[partial_avg(a#233), partial_sum(b#234)], output=[b#234, sum#252, count#253L, sum#254L])
                  +- FileScan parquet default.t[a#233,b#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
   :  +- ReusedSubquery Subquery subquery#250, [id=#104]
   +- *(1) Scan OneRowRelation[]
+- == Initial Plan ==
   ...
```

Please note that the above simple example could be easily optimized into a common select expression without reuse node, but this PR can handle more complex queries as well. 

### Why are the changes needed?
Performance improvement.
```
[info] TPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] q9 - MergeScalarSubqueries off                    51090          54205         NaN          0.0      Infinity       1.0X
[info] q9 - MergeScalarSubqueries on                     17478          18497        1184          0.0      Infinity       2.9X

[info] TPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] q9b - MergeScalarSubqueries off                   14879          15092         164          0.0      Infinity       1.0X
[info] q9b - MergeScalarSubqueries on                     3554           3691         121          0.0      Infinity       4.2X
```
Please find `q9b` in the description of SPARK-34079. It is a variant of [q9.sql](https://github.com/apache/spark/blob/master/sql/core/src/test/resources/tpcds/q9.sql) using CTE.
The performance improvement in case of `q9` comes from merging 15 subqueries into 5 and in case of `q9b` it comes from merging 5 subqueries into 1.

### Does this PR introduce _any_ user-facing change?
No.

### How was this patch tested?
Existing and new UTs.
",https://api.github.com/repos/apache/spark/issues/32298/timeline,,spark,apache,peter-toth,7253827,MDQ6VXNlcjcyNTM4Mjc=,https://avatars.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32298,https://github.com/apache/spark/pull/32298,https://github.com/apache/spark/pull/32298.diff,https://github.com/apache/spark/pull/32298.patch,,https://api.github.com/repos/apache/spark/issues/32298/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
238,https://api.github.com/repos/apache/spark/issues/32031,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/32031/labels{/name},https://api.github.com/repos/apache/spark/issues/32031/comments,https://api.github.com/repos/apache/spark/issues/32031/events,https://github.com/apache/spark/pull/32031,848854905,MDExOlB1bGxSZXF1ZXN0NjA3NzI2MTY3,32031,[WIP] Initial work of Remote Shuffle Service on Kubernetes,"[{'id': 1406627200, 'node_id': 'MDU6TGFiZWwxNDA2NjI3MjAw', 'url': 'https://api.github.com/repos/apache/spark/labels/BUILD', 'name': 'BUILD', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1981471081, 'node_id': 'MDU6TGFiZWwxOTgxNDcxMDgx', 'url': 'https://api.github.com/repos/apache/spark/labels/DOCS', 'name': 'DOCS', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,69,2021-04-01T23:37:24Z,2021-12-08T17:29:53Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?

This PR contains Remote Shuffle Service on Kubernetes. The code is mostly copied from [Uber Remote Shuffle Service](https://github.com/uber/RemoteShuffleService) and modified with some renaming. Also added Kubernetes related support which does not exist in original Uber Remote Shuffle Service.

See [here](https://github.com/hiboyang/spark/tree/remote-shuffle-service/remote-shuffle-service) for how to build and run remote shuffle service in Kubernetes. This is initial work and comments/suggestions are welcome.

### Why are the changes needed?

It is still difficult to use dynamic allocation with Spark on Kubernetes. There are several disaggregated/remote shuffle solutions in different companies. Hopefully we could get a remote shuffle implementation into Spark and enhanced in the future by the Spark community.

### Does this PR introduce _any_ user-facing change?

Yes, user could set Spark config (spark.shuffle.manager=org.apache.spark.shuffle.RssShuffleManager) to run Spark applications with remote shuffle service. It will make Spark use the new RssShuffleManager to write/read shuffle data to/from remote shuffle service.

### How was this patch tested?

Manually tested with Spark application in Kubernetes.
",https://api.github.com/repos/apache/spark/issues/32031/timeline,,spark,apache,hiboyang,14280154,MDQ6VXNlcjE0MjgwMTU0,https://avatars.githubusercontent.com/u/14280154?v=4,,https://api.github.com/users/hiboyang,https://github.com/hiboyang,https://api.github.com/users/hiboyang/followers,https://api.github.com/users/hiboyang/following{/other_user},https://api.github.com/users/hiboyang/gists{/gist_id},https://api.github.com/users/hiboyang/starred{/owner}{/repo},https://api.github.com/users/hiboyang/subscriptions,https://api.github.com/users/hiboyang/orgs,https://api.github.com/users/hiboyang/repos,https://api.github.com/users/hiboyang/events{/privacy},https://api.github.com/users/hiboyang/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/32031,https://github.com/apache/spark/pull/32031,https://github.com/apache/spark/pull/32031.diff,https://github.com/apache/spark/pull/32031.patch,,https://api.github.com/repos/apache/spark/issues/32031/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
239,https://api.github.com/repos/apache/spark/issues/30565,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/30565/labels{/name},https://api.github.com/repos/apache/spark/issues/30565/comments,https://api.github.com/repos/apache/spark/issues/30565/events,https://github.com/apache/spark/pull/30565,754993855,MDExOlB1bGxSZXF1ZXN0NTMwNzkwMjQz,30565,[WIP][SPARK-33625][SQL] Subexpression elimination for whole-stage codegen in Filter,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,21,2020-12-02T06:41:47Z,2021-10-29T00:13:52Z,,MEMBER,,True,"<!--
Thanks for sending a pull request!  Here are some tips for you:
  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html
  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html
  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.
  4. Be sure to keep the PR description updated to reflect all changes.
  5. Please write your PR title to summarize what this PR proposes.
  6. If possible, provide a concise example to reproduce the issue for a faster review.
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->

### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->

This patch proposes to enable whole-stage subexpression elimination for Filter.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->

We made subexpression elimination available for whole-stage codegen in ProjectExec. Another one operator that frequently runs into subexpressions, is Filter. We should also make whole-stage codegen subexpression elimination in FilterExec too.

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->

No

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->

Unit test",https://api.github.com/repos/apache/spark/issues/30565/timeline,,spark,apache,viirya,68855,MDQ6VXNlcjY4ODU1,https://avatars.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/30565,https://github.com/apache/spark/pull/30565,https://github.com/apache/spark/pull/30565.diff,https://github.com/apache/spark/pull/30565.patch,,https://api.github.com/repos/apache/spark/issues/30565/reactions,2,2,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
240,https://api.github.com/repos/apache/spark/issues/29330,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/29330/labels{/name},https://api.github.com/repos/apache/spark/issues/29330/comments,https://api.github.com/repos/apache/spark/issues/29330/events,https://github.com/apache/spark/pull/29330,671807202,MDExOlB1bGxSZXF1ZXN0NDYxOTUzMTc2,29330,[SPARK-32432][SQL] Added support for reading ORC/Parquet files with SymlinkTextInputFormat,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,19,2020-08-03T05:36:20Z,2021-11-11T00:01:45Z,,CONTRIBUTOR,,False,"### What changes were proposed in this pull request?
<!--
Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. 
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.
  2. If you fix some SQL features, you can provide some references of other DBMSes.
  3. If there is design documentation, please add the link.
  4. If there is a discussion in the mailing list, please add the link.
-->
This pull-request is to add support for reading ORC/Parquet files with SymlinkTextInputFormat in Apache Spark.

### Why are the changes needed?
<!--
Please clarify why the changes are needed. For instance,
  1. If you propose a new API, clarify the use case for a new API.
  2. If you fix a bug, you can clarify why it is a bug.
-->
Hive style symlink (SymlinkTextInputFormat) is commonly used in different analytic engines including prestodb and prestosql.
Currently SymlinkTextInputFormat works with JSON/CSV files but does not work with ORC/Parquet files in Apache Spark (and Apache Hive).
On the other hand, prestodb and prestosql support SymlinkTextInputFormat with ORC/Parquet files.
This pull-request is to add support for reading ORC/Parquet files with SymlinkTextInputFormat in Apache Spark.

See details in the JIRA.  SPARK-32432

### Does this PR introduce _any_ user-facing change?
<!--
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
If no, write 'No'.
-->
Yes.
Currently Spark returns exceptions if users try to use SymlinkTextInputFormat with ORC/Parquet files.
With this patch, Spark can handle symlink which indicates locations of ORC/Parquet files.

### How was this patch tested?
<!--
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
I added a new test suite `SymlinkSuite` and confirmed it passed.

```
$ ./build/sbt ""project hive"" ""test-only org.apache.spark.sql.hive.SymlinkSuite""
```",https://api.github.com/repos/apache/spark/issues/29330/timeline,,spark,apache,moomindani,1304020,MDQ6VXNlcjEzMDQwMjA=,https://avatars.githubusercontent.com/u/1304020?v=4,,https://api.github.com/users/moomindani,https://github.com/moomindani,https://api.github.com/users/moomindani/followers,https://api.github.com/users/moomindani/following{/other_user},https://api.github.com/users/moomindani/gists{/gist_id},https://api.github.com/users/moomindani/starred{/owner}{/repo},https://api.github.com/users/moomindani/subscriptions,https://api.github.com/users/moomindani/orgs,https://api.github.com/users/moomindani/repos,https://api.github.com/users/moomindani/events{/privacy},https://api.github.com/users/moomindani/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/29330,https://github.com/apache/spark/pull/29330,https://github.com/apache/spark/pull/29330.diff,https://github.com/apache/spark/pull/29330.patch,,https://api.github.com/repos/apache/spark/issues/29330/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
241,https://api.github.com/repos/apache/spark/issues/28642,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/28642/labels{/name},https://api.github.com/repos/apache/spark/issues/28642/comments,https://api.github.com/repos/apache/spark/issues/28642/events,https://github.com/apache/spark/pull/28642,624755391,MDExOlB1bGxSZXF1ZXN0NDIzMTAyMzI2,28642,[SPARK-31809][SQL] Infer IsNotNull from some special equality join keys,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}, {'id': 1982260031, 'node_id': 'MDU6TGFiZWwxOTgyMjYwMDMx', 'url': 'https://api.github.com/repos/apache/spark/labels/PYTHON', 'name': 'PYTHON', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,32,2020-05-26T09:52:56Z,2021-11-01T00:45:49Z,,MEMBER,,False,"### What changes were proposed in this pull request?

We can infer `IsNotNull` from some special equality join keys. For example:
```sql
CREATE TABLE t1(a string, b string, c string) using parquet;
CREATE TABLE t2(a string, b decimal(38, 18), c string) using parquet;
SELECT t1.* FROM t1 JOIN t2 ON coalesce(t1.a, t1.b)=t2.a; -- case 1
SELECT t1.* FROM t1 JOIN t2 ON CAST(t1.a AS DOUBLE)=CAST(t2.b AS DOUBLE); -- case 2
```
The `coalesce(t1.a, t1.b)` or `CAST(t1.a AS DOUBLE)` may generate a lot of null values, which will lead to skew join.
After this pr:
```
== Physical Plan ==
*(5) Project [a#5, b#6, c#7]
+- *(5) SortMergeJoin [coalesce(a#5, b#6)], [a#8], Inner
   :- *(2) Sort [coalesce(a#5, b#6) ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(coalesce(a#5, b#6), 200), true, [id=#44]
   :     +- *(1) Filter isnotnull(coalesce(a#5, b#6))
   :        +- Scan hive default.t1 [a#5, b#6, c#7], HiveTableRelation `default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#5, b#6, c#7], Statistics(sizeInBytes=8.0 EiB)
   +- *(4) Sort [a#8 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#8, 200), true, [id=#52]
         +- *(3) Filter isnotnull(a#8)
            +- Scan hive default.t2 [a#8], HiveTableRelation `default`.`t2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#8, b#9, c#10], Statistics(sizeInBytes=8.0 EiB)
```

### Why are the changes needed?

1. Avoid skew join in some cases.
2. [Hive support this optimization](https://github.com/apache/hive/blob/rel/release-3.1.2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java).


### Does this PR introduce _any_ user-facing change?

No.

### How was this patch tested?

Unit test and benchmark test:
Case1:
Before this PR | After this PR
-- | --
![image](https://issues.apache.org/jira/secure/attachment/13003914/13003914_default.png) | ![image](https://issues.apache.org/jira/secure/attachment/13003913/13003913_infer.png)

Case2:
Before this PR | After this PR
-- | --
![image](https://user-images.githubusercontent.com/5399861/128879249-f08c0577-caf7-422f-b25c-f47113cc5793.png) | ![image](https://user-images.githubusercontent.com/5399861/128879432-eff937d2-999b-4ac8-a216-25b40e093b67.png)
",https://api.github.com/repos/apache/spark/issues/28642/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/28642,https://github.com/apache/spark/pull/28642,https://github.com/apache/spark/pull/28642.diff,https://github.com/apache/spark/pull/28642.patch,,https://api.github.com/repos/apache/spark/issues/28642/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
242,https://api.github.com/repos/apache/spark/issues/28032,https://api.github.com/repos/apache/spark,https://api.github.com/repos/apache/spark/issues/28032/labels{/name},https://api.github.com/repos/apache/spark/issues/28032/comments,https://api.github.com/repos/apache/spark/issues/28032/events,https://github.com/apache/spark/pull/28032,588181296,MDExOlB1bGxSZXF1ZXN0MzkzOTc3MjM5,28032,[SPARK-31264][SQL] Repartition before writing data source tables/directories,"[{'id': 1405794576, 'node_id': 'MDU6TGFiZWwxNDA1Nzk0NTc2', 'url': 'https://api.github.com/repos/apache/spark/labels/SQL', 'name': 'SQL', 'color': 'ededed', 'default': False, 'description': None}]",open,False,,[],,46,2020-03-26T06:12:14Z,2021-11-14T07:23:08Z,,MEMBER,,True,"### What changes were proposed in this pull request?

This PR adds a new rule `RepartitionWritingDataSource` to support repartitioning before writing data.  It supports three patterns:
- Repartition by none when writing normal tables/directories to reduce small files.
- Repartition by dynamic partition column when writing dynamic partition tables/directories to reduce small files because a single map task may contains many dynamic partition values.
- Repartition by bucket column with bucket number and sort by sort column when writing bucket tables/directories.

We only support data source tables/directories because [it cannot fully support Hive tables](https://github.com/apache/spark/blob/9f7cdb89f7614ef38ba2d8877d4c0f87ad9b6f5f/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala#L182-L188).

Hive has a similar rule: [SortedDynPartitionOptimizer](https://github.com/apache/hive/blob/917221e8378ec48ea05ef6b6c7d9515609b8ec01/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java).


### Why are the changes needed?
1. To reduce generating small files.
2. To ease pressure on the NameNode and improve insert dynamic partition table/directory performance.
   Spark job failed  because too many data blocks were created | HDFS data block monitor
   -- | --
   ![image](https://user-images.githubusercontent.com/5399861/77612149-62020880-6f62-11ea-8b2f-dfd46d0fc5a6.png) | ![image](https://user-images.githubusercontent.com/5399861/77612239-9bd30f00-6f62-11ea-9178-3bcd65aa4034.png)



### Does this PR introduce any user-facing change?
No.


### How was this patch tested?
Unit test and benchmark test:
Query | Before this PR | After this PR
-- | -- | --
CREATE TABLE t1 USING parquet   PARTITIONED BY (p1, p2) AS (SELECT id, id % 1000 AS p1, id % 10000 AS p2 FROM   range(5000000)) | 15 min | 1.1 min

",https://api.github.com/repos/apache/spark/issues/28032/timeline,,spark,apache,wangyum,5399861,MDQ6VXNlcjUzOTk4NjE=,https://avatars.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,https://api.github.com/repos/apache/spark/pulls/28032,https://github.com/apache/spark/pull/28032,https://github.com/apache/spark/pull/28032.diff,https://github.com/apache/spark/pull/28032.patch,,https://api.github.com/repos/apache/spark/issues/28032/reactions,0,0,0,0,0,0,0,0,0,,,,,,,,,,,,,,,,,,
